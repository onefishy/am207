{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2 (Due 09/23/2021, 11:59pm)\n",
    "## Bayesian versus Frequentist Inference\n",
    "\n",
    "**AM 207: Advanced Scientific Computing**<br>\n",
    "**Instructor: Weiwei Pan**<br>\n",
    "**Fall 2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:**\n",
    "\n",
    "**Students collaborators:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "**Submission Format:** Use this notebook as a template to complete your homework. Please intersperse text blocks (using Markdown cells) amongst `python` code and results -- format your submission for maximum readability. Your assignments will be graded for correctness as well as clarity of exposition and presentation -- a “right” answer by itself without an explanation or is presented with a difficult to follow format will receive no credit.\n",
    "\n",
    "**Code Check:** Before submitting, you must do a \"Restart and Run All\" under \"Kernel\" in the Jupyter or colab menu. ***Portions of your submission that contains syntactic or run-time errors will not be graded***.\n",
    "\n",
    "**Libraries and packages:** Unless a problems specifically asks you to implement from scratch, you are welcomed to use any `python` library package in the standard Anaconda distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posterior_samples(prior_var, noise_var, x_matrix, y_matrix, x_test_matrix, samples=100):\n",
    "    '''Function to generate posterior predictive samples for Bayesian linear regression model'''\n",
    "    prior_variance = np.diag(prior_var * np.ones(x_matrix.shape[1]))\n",
    "    prior_precision = np.linalg.inv(prior_variance)\n",
    "\n",
    "    joint_precision = prior_precision + x_matrix.T.dot(x_matrix) / noise_var\n",
    "    joint_variance = np.linalg.inv(joint_precision)\n",
    "    joint_mean = joint_variance.dot(x_matrix.T.dot(y_matrix)) / noise_var\n",
    "\n",
    "    #sampling 100 points from the posterior\n",
    "    posterior_samples = np.random.multivariate_normal(joint_mean.flatten(), joint_variance, size=samples)\n",
    "\n",
    "    #take posterior predictive samples\n",
    "    posterior_predictions = np.dot(posterior_samples, x_test_matrix.T) \n",
    "    posterior_predictive_samples = posterior_predictions[np.newaxis, :, :] + np.random.normal(0, noise_var**0.5, size=(100, posterior_predictions.shape[0], posterior_predictions.shape[1]))\n",
    "    posterior_predictive_samples = posterior_predictive_samples.reshape((100 * posterior_predictions.shape[0], posterior_predictions.shape[1]))\n",
    "    return posterior_predictions, posterior_predictive_samples\n",
    "\n",
    "\n",
    "def generate_data(number_of_points=10, noise_variance=0.3):\n",
    "    '''Function for generating toy regression data'''\n",
    "    #training x\n",
    "    x_train = np.hstack((np.linspace(-1, -0.5, number_of_points), np.linspace(0.5, 1, number_of_points)))\n",
    "    #function relating x and y\n",
    "    f = lambda x: 3 * x**3\n",
    "    #y is equal to f(x) plus gaussian noise\n",
    "    y_train = f(x_train) + np.random.normal(0, noise_variance**0.5, 2 * number_of_points)\n",
    "    x_test = np.array(list(set(list(np.hstack((np.linspace(-1, 1, 200), x_train))))))\n",
    "    x_test = np.sort(x_test)\n",
    "    return x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Maximum Likelihood Estimators for Polynomial Regression\n",
    "\n",
    "In this problem, you are given a function, `generate_data`, to generate toy datasets with a single predictor $X$ representing patient age (normalized) and a single outcome $y$ representing diastolic blood pressure (normalized and rescaled), and your task is to fit polynomial models to the data. That is, assume that the outcome $y$ can be modeled by the following process:\n",
    "\n",
    "\\begin{align}\n",
    "y &= f(x) + \\epsilon = w_0 + w_1x + w_2x^2 + \\ldots + w_Dx^D + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 0.3)\n",
    "\\end{align}\n",
    "\n",
    "where the $w_d$, the *parameters* of the function $f$, are unknown constants, and the degree $D$ is a hyperparameter.\n",
    "\n",
    "\n",
    "You'll notice that in these datasets, the test input is sampled from a different distribution that the training input: the training input has a gap, there are no training input values in [-0.5, 0.5], where as the test input are sampled across [-1, 1]. This change of the distributions over the $x$-values between training and test is called **covariate shift**.\n",
    "\n",
    "These toy datasets simulate a very common problem in machine learning: models are fitted on training data, but during deployment they are given data dissimlar to the training data (i.e. the model encounters covariate shift). As such, you should treat `x_train`, `y_train` as data available during model development and evaluation, and `x_test` as data you encounter during model deployment. \n",
    "\n",
    "The goal in this assignment is to explore how to manage the risk of a deployed model under covariate shift. The ideas developed in this assignment will become a major focus in the latter part of the course and the foundation of an active area of current research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **(Effect of Model Complexity)** Generate a toy dataset with 40 observations (set the parameter `number_of_points=20` for `generate_data`, since twice the number of `number_of_points` will be generated), then  visualize the fit of MLE polynomial models, with degrees $D = [1, 3, 5, 10, 15, 20, 50, 100]$ - you should train on `x_train` and **visualize by predicting on `x_test` provided by the data generating function (`x_test` is a larger set of test points that includes `x_train`)**. You'll need to be thoughtful about your visualization so that these different models can be visually compared in a meaningful way. <br><br>\n",
    "Discuss the effect of the choice of polynomial degree on the fit of the model (concretely describe why certaint choices are unideal in the context of the problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Model Selection)** Later in the course, we will study a number of metrics commonly used for selecting between different MLE models. All of these metric essentially encode Occam's Razor: select the minimal complexity of model that satisfies some pre-determined modeling goal. <br><br>\n",
    "For now, a very simple method for selecting the optimal degree is via cross-validation (by bootstrap):\n",
    "\n",
    "  1. randomly sample two datasets, `x_train`, `x_valid`, from the data generating function: one for training and one for validation. Fit an MLE  polynomial model of degree $d$ on the training data and evaluate its performance on the validation data. Over $S$ number of such randomly sampled pairs of datasets, average the model's validation performance.\n",
    "  2. plot the validation score as a function of model complexity, the polynomial degree $d$. \n",
    "  3. based on the plot, select the the minimal degree that achieves a high average validation performance (i.e. look for the 'elbow' of the plot).\n",
    "\n",
    "  Explain why performing model selection by cross-validation mitigates the risk of choosing an undesirable polynomial (identified in Problem 1)?<br><br>\n",
    "  Implement model selection by cross-validation for the toy dataset generated in Problem 1 using MSE as your performance metric and select an optimal degree from $D=[1,3,5,10,15,20,50,100]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(Uncertainty Estimation)** As you have seen in HW0, we often use the bootstrap predictive uncertainty of MLE models as an indicator of our confidence in the model's output. Increasingly, in practice, decisions making are deferred to human experts when the model's predictive uncertainty is too high. <br><br>\n",
    "Given your understanding of the dataset (`x_train` and `x_test`), describe what the model uncertainty ***should*** ideally look like across the input space (i.e. if you plotted the model uncertainty as a function of $x$, what would it look like)? Justify your answer: consider the context of the problem - the test input has undergone covariate shift and is dissmilar to the training input, what kind of uncertainty would help you mitigate risk under this condition?<br><br>\n",
    "A common practice for estimating predictive uncertainty is to fit a large number of (bootstrap) models on the training data (this collection of models is called an **ensemble**), then, at an input $x$, use the variance of the ensemble predictions to estimate the uncertainty at $x$. Plot the 95% predictive interval of 200 bootstrap MLE polynomial models for each degree $D=[1,3,5,10,15,20,50,100]$, arrange your plots as  subplots in a single figure. For which polynomial degree do you obtain the predictive uncertainty that is most ideal (according to your description above)? Is this the degree you selected in Problem 2? Explain why you would or would not expect the optimal degree in Problem 2 to yield the most ideal uncertainty estimate.<br><br>\n",
    "Make the same plots the 95% predictive intervals for degrees $D=[1,3,5,10,15,20,50,100]$, with models fitted on larger training datasets - set `number_of_points` to 50, 100, 500, 1000 (arrange all these plots in a single figure). What is happening to the predictions of the ensemble in the training data rich region? What is happening to the predictions of the ensemble in the training data poor region? Are these expected behaviours (relate what you see in both cases to the asymptotic properties of MLE)?\n",
    "<br><br>\n",
    "When the training data is abundant (`number_of_points=1000`), are any of the 95% predictive intervals ideal (according to your description above)? What does this imply about the feasibility of using the variance of the ensemble predictions to estimate predictive uncertainty at an input $x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **(Effect of Regularization)** In practice, MLE models are nearly always trained with regularization (since they tend to overfit to the training data). Here, we will explore the effect of adding $\\ell_2$ regularization to our MLE polynomial models (that is, use the `Ridge` regression model from `sklearn` after augmenting your input with polynomial features). <br><br>\n",
    "For a toy dataset with 40 observations (`number_of_points=20`), plot the 95% predictive intervals for degrees $D = [1,3,5,10,15,20,50,100]$ and regularization strengths `alpha = [5e-3, 1e-2, 1e-1, 1e0, 1e1]` (you should organize these plots in a grid).<br><br>\n",
    "Describe the effect of regularization on the bootstrap uncertainties. Looking at these results, are the goals of $\\ell_2$ regularization and obtaining useful predictive uncertatinty estimation neccessarily well-aligned?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Bayesian Polynomial Regression\n",
    "In this problem, your task is to perform Bayesian polynomial regression on the toy datasets in Part I. That is, assume that the outcome $y$ can be modeled by the following process:\n",
    "\n",
    "\\begin{align}\n",
    "y &= f(x) + \\epsilon = w_0 + w_1x + w_2x^2 + \\ldots + w_Dx^D + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 0.3)\\\\\n",
    "w_d &\\sim \\mathcal{N}(0, \\alpha)\n",
    "\\end{align}\n",
    "\n",
    "where $\\alpha$ is a hyperparameter and must be fixed before modeling and inference begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **(Bayesian Kernel Regression)** Just as we can treat a polynomial regression model like a multi-linear regression model after ***transforming*** the input data by adding polynomial features. We can treat Bayesian polynomial regression like Bayesian linear regression on top of the transformed inputs. Formally, the map that takes an input $\\mathbf{x}_n \\in \\mathbb{R}^{D'}$ and transforms it into a new input $\\phi(\\mathbf{x}_n) \\in \\mathbb{R}^{D}$ is called a **feature map**, $\\phi: \\mathbb{R}^{D'} \\to \\mathbb{R}^{D}$, for 1-dimensional input $x \\in \\mathbb{R}$, the polynomial feature map of degree $D$ is defined by\n",
    "\\begin{align}\n",
    "\\\\\\phi: \\mathbb{R} &\\to \\mathbb{R}^D\\\\\n",
    "x &\\mapsto [1, x, x^2, \\ldots, x^D]\\\\\n",
    "\\end{align}\n",
    "<br> Thus, we can write rewrite Bayesian polynomial regression as\n",
    "\\begin{align}\n",
    "\\\\y &= \\mathbf{w}^\\top \\mathbf{x} + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, 0.3)\\\\\n",
    "\\mathbf{w} &\\sim \\mathcal{N}(0, \\alpha I_{D\\times D})\\\\\n",
    "\\end{align}\n",
    "<br>Denote the $N\\times D$ matrix of transformed inputs by $\\Phi$, where the $n$-th row of the matrix is the $n$-th input $\\mathbf{x}_n$ transformed by the feature map, $\\phi(\\mathbf{x}_n)$. Using this notation, write out the closed form for the posterior for Bayesian polynomial regression in terms of $\\Phi$ (you don't need to rederive anything, just make the appropriate substitutions in the formula you derived in HW0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Effect of Model Complexity)** For Bayesian kernel regression, you need to pre-determine the number of features (i.e. $D$) and the hyperparameter $\\alpha$ in the prior. For a toy dataset with 40 observations (set number_of_points=20), visualize the 95% posterior predictive interval for $D = [1,3,5,10,15,20,50,100]$ and $\\alpha = [0.1, 1, 5, 10, 100]$ (arrange these visualizations in a grid), using Bayesian polynomial regression. \n",
    "<br><br>\n",
    "Based on your visualizaion, describe in intuitive terms what is the role of $\\alpha$ and $D$ in determining the shape of the posterior predictive uncertainty. \n",
    "<br><br>\n",
    "***Hint:*** Read Problem 3 before implementing Problem 2, you can implement both at the same time.\n",
    "<br><br>\n",
    "**Extra Credit:** When the feature map $\\phi$ is a general (usually non-linear transformation), applying Bayesian linear regression on the transformed input is called **Bayesian Kernel Regression**. Choose your own non-linear feature map $\\phi: \\mathbb{R} \\to \\mathbb{R}^5$ and visualize the 95% posterior predictive interval of the Bayesian kernel regression for your choice of $\\phi$ and $D = [1,3,5,10,15,20,50,100]$, $\\alpha = [0.1, 1, 5, 10, 100]$. Compare the visualization to the that for Bayesian polynomial regression. Does the posterior predictive of your Bayesian kernel regression capture important properties of the posterior predictive of the Bayesian polynomial regression model?\n",
    "<br><br>\n",
    "**Note:** we highly recommend that you implement the following feature map:\n",
    "<br><br>\n",
    "\\begin{align}\n",
    "\\phi: \\mathbb{R}^{D'} &\\to \\mathbb{R}^D\\\\\n",
    "\\mathbf{x} &\\mapsto \\left[\\sqrt{\\frac{2}{D}} \\cos(w_1^\\top x + b_1), \\ldots, \\sqrt{\\frac{2}{D}} \\cos(w_D^\\top x + b_D)\\right]\n",
    "\\end{align}\n",
    "<br>where $b_d \\sim [0, 2\\pi]$ and $w_d \\sim \\mathcal{N}(0, \\beta I_{D'\\times D'})$ need to be randomly sampled and fixed before modeling and inference. For this exercise, we suggest setting $\\beta=10$. The features generated by $\\phi$ are called **Random Fourier Features**. As the number of features $D$ tends to infinity, the resulting Bayesian kernel regression model tends to an important type of Bayesian (nonparametric) model called Gaussian Process model. We will revisit the connection between Bayesian kernel regression and Gaussian processes in the latter part of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(Model Evaluation and Uncertainty Estimation)** Remember that a direct visual comparision of the 95% predictive interval against the training data is impractical! Rather, to evaluate the fit of the Bayesian model on the observed data, we evaluate the marginal log-likelihood of the data under the posterior. Given a test set $\\{(\\mathbf{x}^*_m, \\mathbf{y}^*_m)\\}$, the log posterior predictive likelihood or, simply, the **log-likelihood** is computed as:\n",
    "\\begin{align}\n",
    "\\\\ \\log \\prod_{m=1}^M p(\\mathbf{y}^*_m | \\mathbf{x}^*_m, \\text{Data}) &= \\sum_{m=1}^M \\log p(\\mathbf{y}^*_m | \\mathbf{x}^*_m, \\text{Data})\\\\\n",
    "&= \\sum_{m=1}^M \\log \\int_\\mathbf{w} p(\\mathbf{y}^*_m | \\mathbf{x}^*_m, \\mathbf{w}) p(\\mathbf{w}| \\text{Data}) d\\mathbf{w}\n",
    "\\end{align}\n",
    "<br>i.e. the log-likelihood at a single observation $(\\mathbf{x}^*_m, \\mathbf{y}^*_m)$ is the log of the likelihood of the observation ***averaged over all models in the posterior***. \n",
    "<br><br>\n",
    "For Bayesian linear regression, with posterior $\\mathcal{N}(\\mu_N, \\Sigma_N)$ we have that\n",
    "$$\n",
    "p(y^*_m | x^*_m, \\text{Data}) = \\mathcal{N}(\\mu^\\top\\mathbf{x}^*_m, \\sigma^2 + (\\mathbf{x}^*_m)^\\top\\Sigma_N\\mathbf{x}^*_m)\n",
    "$$\n",
    "where $\\sigma^2$ is the variance of the observation noise.\n",
    "<br><br>\n",
    "For each choice of $D$ and $\\alpha$ in Problem 2, compute the log-likelihood of the training data. Examine a models with the higher log-likelihoods and a few with lower log-likelihoods, what is the relationship between log-likelihood and predictive uncertainty? In particular, does a higher log-likelihood indicate \"better\" predictive uncertainty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Broader Impact Analysis\n",
    "\n",
    "Starting in 2020, major machine learning conferences are beginning to ask authors as well as reviewers to explicitly consider the broader impact of new machine learning methods. To properly evaluate the potential good or harm that a piece of technology (AI or not) can do to the general public, we need to be aware that no technology is deployed in ideal conditions or in perfectly neutral contexts. In order to assess the potential broader impact of technology, we need to analyze the social systems/institutions of which these technologies will become a part.\n",
    "\n",
    "To help you analyze the broader impact of your technology, begin by considering the following questions:\n",
    "\n",
    "I. Identify the relevant socio-technical systems\n",
    "  - In what social, political, economic system could the tech be deployed?\n",
    "  - How would the tech be used in these systems (what role will it take in the decision making processes)?<br><br>\n",
    "  \n",
    "II. Identify the stakeholders\n",
    "  - Who are the users?\n",
    "  - Who are the affected communities (are these the users)?\n",
    "  \n",
    "    ***Hint:*** users are typically decision makers who will use the technology as decision aids (e.g. doctors), whereas affected communities may be folks who are impacted by these decisions but who are not represented in the decision making process (e.g. patients).<br><br>\n",
    "    \n",
    "III. What types of harm can this tech do?\n",
    "  - What kinds of failures can this tech have?\n",
    "  - What kinds of direct harm can these failures cause?\n",
    "  - What kinds of harm can the socio-technical system cause?\n",
    "  \n",
    "    ***Hint:*** many technical innovations have niche applications, they may sit in a long chain of decision making in a complex system. As such, it may seem, at first glance, that these technologies have no immediate real-life impact. In these cases, it’s helpful to think about the impact of the entire system and then think about how the proposed innovations aid, hamper or change the goals or outcomes of this system.<br><br>\n",
    "    \n",
    "IV. What types of good can this tech do?\n",
    "  - What kinds of needs do these users/communities have?\n",
    "  - What kinds of constraints do these users/communities have?\n",
    "\n",
    "1. **(Impact)**  Analyze the broader impact of these polynomial models. Focus on anticipating ways these models can interact with other components of the decision systems in which they will be deployed, identifying end-users, affected communities as well as anticipating the effects (positive and negative) on affected communities (in particular, does the model have the same effect on all subpopulations in the affected communities?). \n",
    "\n",
    "  Specifically address how predictive uncertainty can impact decision making when the models are deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Bayesian versus Frequentist Uncertainty)** Compare the types of predictive uncertainties that are generated by Bayesian models and ensembles. Characterize the advantages and disadvantages of bootstrap uncertainties from an ensemble. Describe an situation where it would be better to compute bootstrap uncertainties rather than posterior predictive uncertainties from a Bayesian model.\n",
    "\n",
    "  ***Hint:*** For example, consider situations where the data is scarce versus situations where the data is abundant; consider situations where clinicians can provide guidance on model selection using domain expertise versus situations where we would not know how patterns in the data would extrapolate to new populations of patients.\n",
    "\n",
    "  Characterize the advantages and disadvantages of posterior predictive uncertainties from a Bayesian model. Describe an application where it is better to use these uncertainties rather than bootstrap uncertainties from an ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **(Measuring Uncertainty)** From your experiments, are any of the model evaluation metrics consdiered in this assignment (MSE, log-likelihood) appropriate for evaluating the quality of predictive uncertainty far away from the training data, that is, if we are concerned about the performance of models under covariate shift should we use these metrics to perform model selection? \n",
    "\n",
    "  Do our commonly used \"best practices\" of training machine learning models help or hamper our ability to train models with useful predictive uncertainties?\n",
    "\n",
    "  What would be a good metric for measuring uncertainty? How would you define \"good\" uncertainty in the first place?\n",
    "\n",
    "  ***Hint:*** Can you formulate a definition of \"good\" uncertainty without referencing a specific down-stream task?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
