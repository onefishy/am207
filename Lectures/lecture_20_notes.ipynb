{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #20: Variational Autoencoders\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "1. Applications of generative models\n",
    "2. Inference for deep generative models: VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applications of generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review of Latent Variable Models\n",
    "\n",
    "Recall that models that include an observed variable $Y$ and at least one unobserved variable $Z$ are called ***latent variable models***. In general, our model can allow $Y$ and $Z$ to interact in many different ways. Earlier this semester, we studied models with one type of interaction:\n",
    "\n",
    "<img src=\"fig/graphical_model.jpg\" style=\"height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Factor Analysis Models\n",
    "\n",
    "In Lectures #9 and #10, we concentrated on Gaussian Mixutre Models. But in Lecture #9, we introduced ***factor analysis models***, where we posit that the observed data $Y$ with many measurements is generated by a small set of unobserved factors $Z$:\n",
    "\n",
    "\\begin{aligned}\n",
    "Z_n &\\sim \\mathcal{N}(0, I),\\\\\n",
    "Y_n|Z_n &\\sim \\mathcal{N}(f_{\\mu, \\Sigma}(Z_n), \\Phi),\n",
    "\\end{aligned}\n",
    "\n",
    "where $f_{\\mu, \\Sigma}$ is a **linear function** of $Z_n$, in particular, $f_{\\mu, \\Sigma}(Z_n) = \\mu + \\Lambda Z_n$; $n=1, \\ldots, N$, $Z_n\\in \\mathbb{R}^{D'}$ and $Y_n\\in \\mathbb{R}^{D}$. We typically assume that $D'$ is much smaller than $D$.\n",
    "\n",
    "#### Applications\n",
    "Factor analysis models are useful for biomedical data, where we typically measure a large number of characteristics of a patient (e.g. blood pressure, heart rate, etc), but these characteristics are all generated by a small list of health factors (e.g. diabetes, cancer, hypertension etc). Building a good model means we may be able to infer the list of health factors of a patient from their observed measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation for Generative Models\n",
    "\n",
    "The factor analysis model:\n",
    "\n",
    "\\begin{aligned}\n",
    "Z_n &\\sim \\mathcal{N}(0, I),\\\\\n",
    "Y_n|Z_n &\\sim \\mathcal{N}(f_{\\mu, \\Sigma}(Z_n), \\Phi),\n",
    "\\end{aligned}\n",
    "\n",
    "where $f_{\\mu, \\Sigma}$ is a **linear function** of $Z_n$ with parameters $\\mu, \\Sigma$. \n",
    "\n",
    "This is an example of a ***generative model***, that is, after learning the parameters ${\\mu, \\Sigma}$ of $f$, we can **generate** synthetic data by sampling $Z_n \\sim \\mathcal{N}(0, I)$ and then generating a synthetic observation by sampling $Y_n|Z_n \\sim \\mathcal{N}(f_{\\mu, \\Sigma}(Z_n), \\Phi)$. \n",
    "\n",
    "In many applications, like health care, where data collection is an financially and time-wise costly operation, synthetic data from a generative model can be extremely useful!\n",
    "\n",
    "Generative models can also be used to perform imputation of missing data (i.e. fill-in missing covariates of observed data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation for Deep Generative Models \n",
    "\n",
    "The problem with the factor analysis model is that marginal distribution of the data $Y$ is a Gaussian distribution, since $Y$ is the result of a Gaussian distribution $\\mathcal{N}(0, I)$ transformed linearly by $f_{\\mu, \\Sigma}$. But in practice, the distribution of real data is complex! That is, we need **nonlinear transformations**.\n",
    "\n",
    "A deep generative model can be defined as follows:\n",
    "\n",
    "\\begin{aligned}\n",
    "Z_n &\\sim \\mathcal{N}(0, I),\\\\\n",
    "Y_n|Z_n &\\sim \\mathcal{N}(f_{\\mathbf{W}}(Z_n), \\Phi),\n",
    "\\end{aligned}\n",
    "\n",
    "where $f$ is a **nonlinear function** of $Z_n$, parametrized by a **neural network** with weights $\\mathbf{W}$. The deep generative model we will study today, ***variational autoencoders***, will have exactly this generative model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating Data with Variational Autoencoders\n",
    "\n",
    "For example, if we train a deep generative model (a VAE) to capture the distribution of a set of image data consisting of celebrity faces, we can use this generative model to generate realistic looking synthetic faces:\n",
    "\n",
    "<img src=\"fig/vae.gif\" style=\"height:450px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inference for deep generative models: VAEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Expectation Maximization: Estimating the MLE for Latent Variable Models\n",
    "\n",
    "Given a deep latent variable model $p(Y, Z| \\mathbf{W}) = p(Y | f_\\mathbf{W}(Z)) p(Z)$, we are interested computing the MLE of parameters $\\mathbf{W}$, i.e. the parameters that maximizes the likelihood of the observed data:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbf{W}_{\\text{MLE}} &= \\underset{\\mathbf{W}}{\\mathrm{argmax}}\\; \\ell(\\mathbf{W})\\\\\n",
    "&= \\underset{\\mathbf{W}}{\\mathrm{argmax}}\\; \\log \\prod_{n=1}^N \\int_{\\Omega_Z}  p(y_n, z_n | \\mathbf{W}) dz\\\\\n",
    "&= \\underset{\\mathbf{W}}{\\mathrm{argmax}}\\; \\log \\prod_{n=1}^N \\int_{\\Omega_Z}  p(y_n| f_\\mathbf{W}(z_n))p(z_n) dz\n",
    "\\end{aligned}\n",
    "where $\\Omega_Z$ is the domain of $Z$.\n",
    "\n",
    "In the case of factor analysis models and mixture of Gaussian models, we used Expectation Maximization in order to maximize a lower bound of the observed data log-likelihood, the ELBO. How will this work for deep latent variable models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/lecture_20_insert.png\" style=\"height:650px;\">"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
