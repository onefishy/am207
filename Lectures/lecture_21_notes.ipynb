{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #21: Implementation of Variational Autoencoders\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "1. Overall Structure of an VAE Implementation\n",
    "2. Implementing the ELBO\n",
    "3. Implementing the Log-Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overall Structure of VAE Implementation\n",
    "\n",
    "A VAE implementation requires us to keep track of two models: the generative model and the inference model. We need to update the parameters of these models during training, we will eventually need to evaluate the model by generating synthetic data and by computing the log-likelihood. \n",
    "\n",
    "Passing the two models back and forth between functions might get complicated. So we choose to implement a VAE class.\n",
    "\n",
    "```python \n",
    "class VAE:\n",
    "    # Initialization \n",
    "    def __init__(self, generative_architecture, inference_architecture):\n",
    "        \n",
    "        # We use the Feedforward neural network class to make the generative and inference models\n",
    "        # 1. Initialize weights for the generative and inference models\n",
    "        # 2. Instantiate generative model\n",
    "        self.generative = Feedforward(decoder_architecture, weights=generative_weights)\n",
    "        # 3. Instantiate inference model\n",
    "        self.inference = Feedforward(encoder_architecture, weights=inference_weights)\n",
    "        # 4. Do other initializations...\n",
    "        \n",
    "    # Define the variational objective: the negative ELBO, using S number of samples for the Monte Carlo estimte in the ELBO\n",
    "    def make_objective(self, y, S):\n",
    "        # ...\n",
    "        return variational_objective, grad(variational_objective)\n",
    "    \n",
    "    # Generate synthetic data from the generative model\n",
    "    def generate(self, N=100):\n",
    "        # ...\n",
    "        return synthetic_samples\n",
    "    \n",
    "    # Infer the variational parameters of the gaussian approximation of p(z|y) given y\n",
    "    def infer(self, y):\n",
    "        # ...\n",
    "        return variational_means, variational_variances\n",
    "    \n",
    "    # Compute the log-likelihood of the data y using S number of samples for the Monte Carlo estimte in the log-likelihood\n",
    "    def log_likelihood(self, y, S=100):\n",
    "        # ...\n",
    "        return lkhd\n",
    "    \n",
    "    # Fit the model given training data y, using S number of samples for the Monte Carlo estimte in the ELBO\n",
    "    def fit(self, y, S):\n",
    "        # ...\n",
    "        return None\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining `.make_objective`\n",
    "The `.make_objective` function of the VAE class takes in parameters: `y`, the training data, and `S`, the number of samples used for the Monte Carlo estimte in the ELBO. This function returns the negative ELBO and the gradient of the ELBO function.\n",
    "\n",
    "Recall that the ELBO for an VAE, whose generative model has parameters $w$ and whose inference model has parameters $v$, is written as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{ELBO}(w, q_v) &= \\sum_{n=1}^N \\left[\\mathbb{E}_{q_v}\\left[\\log p_w(y_n|z_n) \\right] - \\mathrm{D}_{\\mathrm{KL}}\\left[q_v(z_n)|p(z_n) \\right]\\right]\\\\\n",
    "&= \\sum_{n=1}^N \\left[\\mathbb{E}_{q_v}\\left[\\log p_w(y_n|z_n) \\right] - \\mathbb{E}_{q_v}\\left[\\log \\frac{q_v(z_n)}{p(z_n)} \\right]\\right]\\\\\n",
    "&= \\sum_{n=1}^N \\left[\\mathbb{E}_{q_v}\\left[\\log p_w(y_n|z_n) \\right] - \\mathbb{E}_{q_v}\\left[\\log q_v(z_n)\\right] + \\mathbb{E}_{q_v} \\left[{p(z_n)} \\right]\\right]\\\\\n",
    "&\\approx \\sum_{n=1}^N \\left[\\frac{1}{S}\\sum_{s=1}^S \\log p_w(y_n|z^s_n)  - \\frac{1}{S}\\sum_{s=1}^S\\log q_v(z^s_n) + \\frac{1}{S}\\sum_{s=1}^S {p(z^s_n)}\\right], \\quad z^s_n \\sim q_v(z).\n",
    "\\end{align}\n",
    "\n",
    "We see that we need to implement three terms: \n",
    "\n",
    "1. $\\mathbb{E}_{q_v}\\left[\\log p_w(y_n|z_n) \\right]$\n",
    "2. $\\mathbb{E}_{q_v}\\left[\\log q_v(z_n)\\right]$\n",
    "3. $\\mathbb{E}_{q_v} \\left[{p(z_n)} \\right]$\n",
    "\n",
    "When we implement $\\mathbb{E}_{q_v}\\left[\\log p_w(y_n|z_n) \\right]$, we need to first sample $z_n$ from the inference model given the current inference parameters, $q_v$. Recall that $q_v$ is the Gaussian distribution $\\mathcal{N}(\\mu_v(y_n), \\sigma_v(y_n))$, where $\\mu_v(y_n)$ and $\\sigma_v(y_n)$ are the variational mean and variance for the variational posterior for $y_n$.\n",
    "\n",
    "Recall that $p_w(y_n|z_n)$ is the Gaussian distribution $\\mathcal{N}(y_n; f_w(z_n), \\sigma_y^2I)$, where $f_w$ is the generative model given the current generative parameters. So we will need to pass the samples $z_n$ through the inference model and obtain $f_w(z_n)$, then we evaluate the Gaussian pdf, with mean $f_w(z_n)$ and variance $\\sigma_y^2$, at $y_n$.\n",
    "\n",
    "When we implement $\\mathbb{E}_{q_v}\\left[\\log q_v(z_n)\\right]$, we evaluate the Gaussian $\\mathcal{N}(\\mu_v(x_n), \\sigma_v(x_n))$ at the samples $z_n$, drawn from the variational posterior $q_z$ for $x_n$.\n",
    "\n",
    "Finally, when we implement $\\mathbb{E}_{q_v} \\left[{p(z_n)} \\right]$, we evaluate the standard Gaussian $\\mathcal{N}(0, I)$ at the samples $z_n$, drawn from the variational posterior $q_z$ for $x_n$.\n",
    "\n",
    "```python\n",
    "def make_objective(self, y, y_var, S):\n",
    "           \n",
    "        def variational_objective(params, t):\n",
    "            '''\n",
    "            Definition of the ELBO\n",
    "            params: the parameters of the generative and inference models concatenated into a single vecor\n",
    "            t: the current iteration number, required by autograd\n",
    "            '''\n",
    "            \n",
    "            #unpack the generative and inference model parameters\n",
    "            inference_weights, generative_weights = self.unpack_weights(params)\n",
    "\n",
    "            #infer z's: this returns the variational means and variances concatenated\n",
    "            z_params = self.inference.forward(inference_weights, y)\n",
    "            \n",
    "            #unpack the variational means and variances\n",
    "            mean, std = self.unpack_params(z_params)\n",
    "            \n",
    "            #sample z's\n",
    "            z_samples = np.random.normal(0, 1, size=(self.S, self.z_dim, N)) * std + mean\n",
    "            \n",
    "            #predict y's\n",
    "            y_pred = self.generative.forward(decoder_weights, z_samples)\n",
    "            \n",
    "            #evaluate term 1: log-likelihood\n",
    "            log_likelihood = np.sum(norm.logpdf(y, y_pred, y_var ** 0.5), axis=-2)\n",
    "            \n",
    "            #evaluate term 2: evaluate sampled z's under variational distribution\n",
    "            log_qz_given_y = np.sum(norm.logpdf(z_samples, mean, std), axis=-2)\n",
    "                        \n",
    "            #evaluate term 3: evaluate sampled z's under prior\n",
    "            log_pz = np.sum(norm.logpdf(z_samples, 0.0, 1.0), axis=-2)\n",
    "            \n",
    "            #compute the elbo\n",
    "            elbo = np.mean(log_likelihood - log_qz_given_y + log_pz)\n",
    "            \n",
    "            #return the negative elbo to be minimized\n",
    "            return -elbo\n",
    "            \n",
    "            \n",
    "        return variational_objective, grad(variational_objective)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Defining `.log_likelihood`\n",
    "\n",
    "The `.log_likelihood` function computes the log-likelihood of the observed data given a learned generative model $f_w$. Recall that the log-likelihood can be written as:\n",
    "\\begin{align}\n",
    "\\ell_v(y) &= \\frac{1}{N}\\sum_{n=1}^N \\log \\left[\\mathbb{E}_{z_n \\sim p(z)} \\mathcal{N}(y_n; f_w(z_n), \\sigma_y^2I) \\right]\\\\\n",
    "&\\approx \\frac{1}{N}\\sum_{n=1}^N \\log \\left[\\frac{1}{S}\\sum_{s=1}^S \\mathcal{N}(y_n; f_w(z^s_n), \\sigma_y^2I) \\right], \\quad z^s_n \\sim p(z)\n",
    "\\end{align}\n",
    "\n",
    "Since the Gaussian pdf will tend to be small, in higher dimensions, the log-likelihood will be numerically unstable to compute. We instead write the log-likelihood in an equivalent but more computationally stable way:\n",
    "\n",
    "\\begin{align}\n",
    "\\ell_v(y) &\\approx \\sum_{n=1}^N \\log \\frac{1}{S}\\sum_{s=1}^S \\mathcal{N}(y_n; f_w(z^s_n), \\sigma_y^2I) , \\quad z^s_n \\sim p(z)\\\\\n",
    "&= \\sum_{n=1}^N \\log\\frac{1}{S} + \\sum_{n=1}^N \\underbrace{\\log \\sum_{s=1}^S \\mathrm{exp}}_{\\texttt{log-sum-exp function}}\\left\\{ \\log \\mathcal{N}(y_n; f_w(z^s_n), \\sigma_y^2I)\\right\\}, \\quad z^s_n \\sim p(z).\n",
    "\\end{align}\n",
    "where the $\\log \\sum_{s=1}^S \\mathrm{exp}$ portion of the expression is computed using `scipy`'s `logsumexp` function. \n",
    "\n",
    "```python\n",
    "def log_likelihood(self, y, S=100):\n",
    "        # Define the covariance matrix for the Gaussian likelihood\n",
    "        cov = y_var * np.ones((1, y.shape[2], 1))\n",
    "        # We reshape the observations into a 3D array\n",
    "        y_tile = y.reshape((1, y.shape[2], N)).T\n",
    "        # We sample from the prior\n",
    "        z_samples = np.random.normal(0, 1, size=(N, self.z_dim, S))\n",
    "        # We feed the samples through the generative model \n",
    "        y_synthetic = self.generative.forward(self.generative.weights, z_samples)\n",
    "\n",
    "        # Compute the constant term of the log Gaussian pdf\n",
    "        const = -0.5 * (self.y_dim * np.log(2 * np.pi) + np.sum(np.log(cov)))\n",
    "        # Compute the exponential term of the log Gaussian pdf\n",
    "        exponential = np.sum(-0.5 * ((y_synthetic - y_tile)**2 / cov), axis=1)\n",
    "        # Add the constant and the exponential terms\n",
    "        llkhd = const + exponential\n",
    "        # Compute the log-likelihood using the logsumexp trick for numeric stability\n",
    "        lkhd = -np.log(S) + logsumexp(llkhd, axis=1)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
