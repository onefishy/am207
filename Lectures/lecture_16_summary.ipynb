{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #16: Neural Network Models for Regression\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture #16 Summary\n",
    "\n",
    "1. **(Why is NN a Big Deal?)** Neural networks are universal function approximators that are easy to compute with and easy to train. \n",
    "\n",
    "  - Specifically, we can build a very very complex function using very **simple** and **numerically stable** functions and **fast** operations (like **vectorized** addition, multiplication and simple non-linear function evaluation). In fact, we can approximate any \"nice\" non-linear function arbitrarily well with a neural network.\n",
    "  - It's easy to find the MLE of NN parameters by gradient descent, since computing the gradient is just recursively applying the chain rule of differentiation.\n",
    "\n",
    "  **Lesson:** If you need a \"generic\" non-linear model for data, use a neural network.\n",
    "\n",
    "  <img src=\"./fig/backprop.jpg\" style='height:600px;'>\n",
    "  \n",
    "  \n",
    "  \n",
    "2. **(Why is NN So Good at Modeling Data?)** Instead of thinking about NNs as representing very complex regression functions or very complex decision boundaries, we can think of NNs as:\n",
    "  - first transforming the data into a form that is simple to capture with a linear model, we call these ***representations*** or ***features*** of the data\n",
    "  - fitting a linear model on the transformed data\n",
    "  \n",
    "  <table>\n",
    "    <tr><td><font size=\"3\">A Complex Decision Boundary $g\\quad\\quad\\quad\\quad\\quad$</font></td>\n",
    "        <td><font size=\"3\">A Transformation $g_0$ and a linear model $g_1\\quad\\quad\\quad\\quad$</font></td>\n",
    "    <tr><td><img src=\"fig/decision.png\" style=\"height:350px;\"></td>\n",
    "        <td><img src=\"fig/architecture2.png\" style=\"height:400px;\"></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network Regression vs Linear Regression\n",
    "\n",
    "Linear models are easy to interpret. Once we've found the MLE of the model parameters, we can formulate scientific hypotheses about the relationship between the outcome $Y$ and the covariates $\\mathbf{X}$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\widehat{\\text{income}} = 2 * \\text{education (yr)} + 3.1 * \\text{married} - 1.5 * \\text{gaps in work history}\n",
    "\\end{align}\n",
    "\n",
    "What do the weights of a neural network tell you about the relationship between the covariates and the outcome?\n",
    "<img src=\"./fig/fig5.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretable Deep Learning\n",
    "We might be tempted to conclude that neural networks are uninterpretable due to their complexity. But just because we can't understand neural networks by inspecting the value of the individual weights, it does not mean that we can't understand them.\n",
    "\n",
    "In [The Mythos of Model Interpretability](https://arxiv.org/abs/1606.03490), the authors survey a large number of methods for interpreting deep models. \n",
    "\n",
    "<img src=\"fig/cnnviz.jpg\" style=\"height:400px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can Machine Learning Models Make Use of Human Concepts?\n",
    "***(with Anita Mahinpei, Justin Clark, Ike Lage, Finale Doshi-Velez)***\n",
    "\n",
    "What if instead building complex non-linear models based on raw inputs, we instead build simple linear models based on human interpretable **concepts**? We use a neural network to predict concepts from inputs and then use a linear model to predict the outcome from the concepts. We interpret the relationship between the outcome and the concepts via the linear model. These models are called **concept bottleneck models**.\n",
    "\n",
    "In [The Promises and Pitfalls of Black-box Concept Learning Models](https://arxiv.org/abs/2106.13314), we examine the advantages and drawbacks of these models.\n",
    "\n",
    "<img src=\"fig/slide15.png\" style=\"height:300px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can Machine Learning Models Learn to Explore Hypothetical Scenarios?\n",
    "***(with Michael Downs, Jonathan Chu, Wisoo Song, Yaniv Yacoby, Finale Doshi-Velez)***\n",
    "\n",
    "Rather than explaining why the model made a decision, it's often more helpful to explain how to change the data in order to change the model's decision. This modified input is a **counter-factual**. In [CRUDS: Counterfactual Recourse Using Disentangled Subspaces](https://finale.seas.harvard.edu/files/finale/files/cruds-_counterfactual_recourse_using_disentangled_subspaces.pdf), we study how to automatically generate counter-factual explanations that can help users achieve a favorable outcome from a decision system.\n",
    "\n",
    "<img src=\"fig/slide16.png\" style=\"height:350px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Right for the Right Reasons?\n",
    "\n",
    "In [*An explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets*](), the authors build a neural network model to detect acute intracranial haemorrhage (ICH) and classifies five ICH subtypes. \n",
    "\n",
    "Model classifications are explained by highlighting the pixels that contributed the most to the decision. The highligthed regions tends to overlapped with ‘bleeding points’ annotated by neuroradiologists on the images.\n",
    "\n",
    "<img src=\"./fig/shap.png\" style=\"height: 350px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Perils of Explanations\n",
    "In [*How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection*](), the authors found that clinicians interacting with incorrect recommendations paired with simple explanations experienced significant reduction in treatment selection accuracy.\n",
    "\n",
    "<img src=\"./fig/reliance.png\" style=\"height: 350px;\" align=\"center\"/>\n",
    "\n",
    "\n",
    "**Take-away:** Incorrect ML recommendations may adversely impact clinician treatment selections and that explanations are insufficient for addressing overreliance on imperfect ML algorithms."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
