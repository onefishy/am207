{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #15: Parallel Tempering and Stochastic HMC\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture #14 Summary: \n",
    "\n",
    "### Commonly Asked Questions About HMC\n",
    "\n",
    "1. **Question:** HMC seems complicated, do I really need to use it?<br>\n",
    "  **Answer:** Yes. \n",
    "  \n",
    "  For complex models of interest (non-conjugate, hierarchical, latent variable) HMC is the least complicated sampler you can use to perform reliable inference.<br><br>\n",
    "\n",
    "2. **Question:** Ok, but can I treat the theory as a black-box, i.e. can I just press some `model.sample()` button?<br>\n",
    "  **Answer:** No. \n",
    "  \n",
    "  HMC (like all samplers) must be tuned. That is, for many models and datasets, `model.sample()` will not have good performance. You need the theory to tell you which design choices need to be adjusted and in what way.\n",
    "  \n",
    "  In the In-Class Exercise 10.14, everyone struggled with finding `pymc3` documentation on how to compare models using WAIC and **no one caught the `pymc3` error about the WAIC computation**:\n",
    "  > /root/venv/lib/python3.7/site-packages/arviz/stats/stats.py:1460: UserWarning: For one or more samples the posterior variance of the log predictive densities exceeds 0.4. This could be indication of WAIC starting to fail. <br>\n",
    "See http://arxiv.org/abs/1507.04544 for details\n",
    "\n",
    "  Practitioner's rely on black-box machine learning tools to analyze data and rely on complex ML metrics (like the WAIC) to assess the validity of their analysis. The insights resulting from this process gets translated into policies that potentially effect millions. \n",
    "  \n",
    "  What should/can a practitioner do when not only their models fail but their metrics ALSO fail?<br><br>\n",
    "  \n",
    "3. **Question:** But I don't need to implement it since `pymc3` has already done so, right?<br>\n",
    "  **Answer:** Wrong.\n",
    "  \n",
    "  You need to implement HMC. Because `pymc3` is not going to scale well for Bayesian inference for models with neural network likelihoods and large datasets. <br><br>\n",
    "  \n",
    "4. **Question:** Why don't you just give me the solution to the In-Class Exercise 10.19, i.e. GIVE ME YOUR IMPLEMENTATION OF HMC!!!!!!<br>\n",
    "  **Answer:** It won't help.\n",
    "  \n",
    "  The main problem with HMC isn't with implementing the leap-frog steps, it's the way you define your energy functions. \n",
    "  \n",
    "  Last year, many groups chose to copy a version of HMC from somewhere random online, this group of people had the MOST trouble with debugging when trying to RUN HMC on different models: debugging other people's code can be way harder than your own (I swear I'm not lying)!<br><br>\n",
    "  \n",
    "5. **Question:** We worked so hard to derive HMC, it had better be the bestest sampler and can be applied to any Bayesian model for any dataset, right?<br>\n",
    "  **Answer:** No. \n",
    "  \n",
    "  HMC may still be inefficient for complex posteriors and improving HMC is an active area of research (see this weeks readings). Futhermore, HMC **does not scale well to large datasets**.\n",
    "  \n",
    "  - HMC can still fail to mix!\n",
    "    - You still need to check for convergence and run multiple chains etc!\n",
    "  \n",
    "  - HMC still will struggle to find multiple modes in the posterior!\n",
    "    - Possible solution: parallel tempering\n",
    "  - Each leap-frog step in HMC requires computing the gradient over the ENTIRE dataset!\n",
    "    - Possibile solution: Stochastic Gradient HMC\n",
    "      **Note:** we lose asymptotic correctness!<br><br>\n",
    "      \n",
    "6. **Question:** What is the broader impact of HMC?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
