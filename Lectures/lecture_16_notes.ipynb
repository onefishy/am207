{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #16: Neural Network Models for Regression\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "from autograd import numpy as np\n",
    "from autograd import grad\n",
    "import numpy\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "from IPython.display import HTML\n",
    "from IPython.display import YouTubeVideo\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Regression as Generalized Linear Models\n",
    "2. Neural Networks\n",
    "3. Automatic Differentiation and BackPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression as Generalized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression Models\n",
    "Here is a generalized linear model (GLM) you've known since day one!\n",
    "\\begin{align}\n",
    "\\mu &= \\mathbf{w}^\\top \\mathbf{X}^{(n)}\\\\\n",
    "Y^{(n)}&\\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "\\end{align}\n",
    "Alternatively, we can write this model as\n",
    "\n",
    "\\begin{align}\n",
    "    Y^{(n)} = \\mathbf{w}^\\top \\mathbf{X}^{(n)} + \\epsilon; \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "\\end{align}\n",
    "\n",
    "That is, injecting covariates into a normal likelihood is precisely linear regression! Just like in the case of logistic regression, we can form scientific hypotheses by examining the parameters of a linear regression model:\n",
    "\n",
    "\\begin{align}\n",
    "    \\widehat{\\text{income}} = 2 * \\text{education (yr)} + 3.1 * \\text{married} - 1.5 * \\text{gaps in work history}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Would You Parameterize a Non-linear Trend?\n",
    "<img src=\"./fig/fig12.png\" style='height:400px;'>\n",
    "It's not easy to think of a function $g(x)$ can capture the trend in the data, e.g. what degree of polynomial should we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review of the Geometry of Logistic Regression \n",
    "In **logistic regression**, we model the probability of an input $\\mathbf{x}$ being labeled '1' as a function of its distance from the hyperplane parametrized by $\\mathbf{w}$\n",
    "<img src=\"./fig/fig0.png\" style='height:300px;'>\n",
    "That is, we model $p(y=1 | \\mathbf{w}, \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x})$. Where $\\mathbf{w}^\\top \\mathbf{x}=0$ is the equation of the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize a ellipitical decision boundary?\n",
    "\n",
    "<img src=\"./fig/fig1.png\" style='height:300px;'>\n",
    "\n",
    "We can say that the decision boundary is given by a ***quadratic function*** of the input:\n",
    "$$\n",
    "w_1x^2_1 + w_2x^2_2 + w_3 = 0\n",
    "$$\n",
    "We say that we can fit such a decision boundary using logistic regression with degree 2 polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize an arbitrary complex decision boundary?\n",
    "\n",
    "<img src=\"./fig/fig2.png\" style='height:300px;'>\n",
    "\n",
    "It's not easy to think of a function $g(x)$ can capture this decision boundary.\n",
    "\n",
    "**GOAL:** Find models that can capture *arbitrarily complex* functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximating Arbitrarily Complex Decision Boundaries\n",
    "\n",
    "Given an exact parametrization, we could learn the functional form, $g$, of the decision boundary directly. \n",
    "\n",
    "However, assuming an exact form for $g$ is restrictive. \n",
    "\n",
    "Rather, we can build increasingly good approximations, $\\widehat{g}$, of $g$ by composing simple functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Neural Network?\n",
    "\n",
    "**Goal:** build a good approximation $\\widehat{g}$ of a complex function $g$ by composing simple functions.\n",
    "\n",
    "For example, let the following picture represents $a = f\\left(\\sum_{i}w_ix_i\\right)$, where $f$ is a non-linear transform, and we denote the intermediate value $\\sum_{i}w_ix_i$ by $s$.\n",
    "\n",
    "<img src=\"./fig/fig4.png\" style='height:300px;'>\n",
    "\n",
    "**Note:** we always assume that $x_0=1$ and hence $w_0$ is the intercept or ***bias*** of the linear expression $\\sum_i w_i x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks as Function Approximators\n",
    "\n",
    "Then we can define the approximation $\\widehat{g}$ with a graphical schema representing a complex series of compositions and sums of the form, $f\\left(\\sum_{i}w_ix_i\\right)$\n",
    "\n",
    "<img src=\"./fig/fig5.png\" style='height:300px;'>\n",
    "\n",
    "This is a ***neural network***. We denote the weights of the neural network collectively by $\\mathbf{W}$.\n",
    "The non-linear function $f$ is called the ***activation function***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Note:** \n",
    "Typically, at each node, we want to take a weighted sum of the values of the previous nodes with a additional ***bias term***. That is, we want as input $\\sum_i w^l_{ij} \\text{node}^l_i + \\text{bias}_j$ for the $j$-th node in the $l$-th hidden layer. This is often done by adding an extra node per layer with the value of 1:\n",
    "<img src=\"./fig/bias.png\" style='height:250px;'>\n",
    "The bias terms are considered to be part of the network parameters and when we jointly denote the network parameters by $\\mathbf{W}$, bias terms are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Flexible Framework for Function Approximation\n",
    "\n",
    "\n",
    "<img src=\"./fig/fig6.png\" style='height:500px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common Choices for the Activation Function\n",
    "\n",
    "<img src=\"./fig/fig8.png\" style='height:500px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks are Universal Function Approximators\n",
    "\n",
    "So what kind of functions can be approximated by neural networks?\n",
    "\n",
    "**Theorem: (Hornik, Stinchombe, White, 1989)** Fix a \"nice\" activation function $f$. For any continuous function $g$ on a compact set $K$, there exists a feedforward neural network with activation $f$, having only a single hidden layer, which approximates $g$ to within an arbitrary degree of precision on $K$.\n",
    "\n",
    "For this reason, we call neural networks ***universal function approximators***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks Regression\n",
    "\n",
    "**Model for Regression:** $Y^{(n)}\\sim \\mathcal{N}(\\mu, \\sigma^2)$, $\\mu = g_\\mathbf{W}(\\mathbf{X}^{(n)})$, where $g_\\mathbf{W}$ is a neural network with parameters $\\mathbf{W}$.\n",
    "\n",
    "**Training Objective:** find $\\mathbf{W}$ to maximize the likelihood of our data. This is equivalent to minimizing the Mean Square Error,\n",
    "$$\n",
    "\\max_{\\mathbf{W}}\\, \\mathrm{MSE}(\\mathbf{W}) = \\frac{1}{N}\\sum^N_{n=1} \\left(y_n - g_\\mathbf{W}(x_n)\\right)^2\n",
    "$$\n",
    "\n",
    "**Optimizing the Training Objective:** For linear regression (when $g_\\mathbf{W}$ is a linear function), we computed the gradient of the MSE with respective to the model parameters $\\mathbf{W}$, set it equal to zero and solved for the optimal $\\mathbf{W}$ analytically (see Homework #0). For logistic regression, we computed the gradient and used (stochastic) gradient descent to \"solve for where the gradient is zero\".\n",
    "\n",
    "Can we do the same when $g_\\mathbf{W}$ is a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automatic Differentiation and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Computation for Neural Networks\n",
    "\n",
    "Computing the gradient for any parameter $w^l_{ij}$ in the following network requires us to use the ***chain rule***:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial t} g(h(t)) = g'(h(t))h'(t),\\quad& \\text{or}\\quad\\frac{\\partial g}{\\partial t} = \\frac{\\partial g}{\\partial h} \\frac{\\partial h}{\\partial t}\n",
    "\\end{align}\n",
    "\n",
    "This is because a neural network is just a big composition of functions.\n",
    "\n",
    "<img src=\"./fig/fig7.png\" style='height:150px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Computing Neural Network Gradients\n",
    "\n",
    "<img src=\"./fig/backprop.jpg\" style='height:600px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation: Gradient Descent for Neural Networks\n",
    "\n",
    "The ***backpropagation*** algorithm consists of three phases:\n",
    "0. (**Initialize**) intialize the network parameters $\\mathbf{W}$\n",
    "1. Repeat:\n",
    "  1. (**Forward Pass**) compute all intermediate values $s_{ij}^l$ and $a_{ij}^l$ for the given covariates $\\mathbf{X}$\n",
    "  2. (**Backward Pass**) compute all the gradients $\\frac{\\partial \\mathcal{L}}{\\partial w^l_{ij}}$\n",
    "  3. (**Update Parameters**) update each parameter by $-\\eta \\frac{\\partial \\mathcal{L}}{\\partial w^l_{ij}}$\n",
    "  \n",
    "<img src=\"./fig/graph_structure.png\" style='height:200px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Computation with Automatic Differentiation\n",
    "\n",
    "The forwards-backwards way of computing the gradient lends itself to an algorithm that automates gradient computation for any neural network. \n",
    "\n",
    "This is a special instance of ***reverse mode automatic differentiation*** -- a method of algorithmically computing exact gradients for functions defined by combinations of simple functions, by drawing graphical models of the composition of functions and then taking gradients by going forwards-backwards.\n",
    "<img src=\"./fig/function.png\" style='height:50px;'>\n",
    "\n",
    "<img src=\"./fig/computation_graph.png\" style='height:150px;'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What Does a Neural Network Learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why is a Neural Network Classifier So Effective?\n",
    "\n",
    "Visualizing the decision boundary:\n",
    "\n",
    "<img src=\"./fig/boundary.png\" style='height:350px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why is a Neural Network Classifier So Effective?\n",
    "\n",
    "Visualizing the output of the last hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Before neural network models became wildly popular in machine learning, a common method for building non-linear classifiers is to first map the data, in the input space $\\mathbb{R}^{\\text{input}}$, into a 'feature' space $\\mathbb{R}^{\\text{feature}}$, such that the classes are well-separated in the feature space. Then, a linear classifier can be fitted to the transformed data.\n",
    "\n",
    "If we ignore the output node of our neural network classifier, we are left with a function, $\\mathbb{R}^{2} \\to \\mathbb{R}^2$, mapping the data from the input space to a 2-dimensional feature space. The transformed data (and in general, the output from a hidden layer in a neural network) is called a ***representation*** of the data. \n",
    "\n",
    "<img src=\"fig/architecture.jpeg\" style=\"height:350px;\">\n",
    "\n",
    "Visualizing these representations can often shed light on how and what neural network models learns from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"./fig/latent.png\" style='height:350px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two Interpretations of a Neural Network Classifier: \n",
    "<table>\n",
    "    <tr><td><font size=\"3\">A Complex Decision Boundary $g\\quad\\quad\\quad\\quad\\quad$</font></td>\n",
    "        <td><font size=\"3\">A Transformation $g_0$ and a linear model $g_1\\quad\\quad\\quad\\quad$</font></td>\n",
    "    <tr><td><img src=\"fig/decision.png\" style=\"height:350px;\"></td>\n",
    "        <td><img src=\"fig/architecture2.png\" style=\"height:400px;\"></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# With Great Flexibility Comes with Great Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network Regression vs Linear Regression\n",
    "\n",
    "Linear models are easy to interpret. Once we've found the MLE of the model parameters, we can formulate scientific hypotheses about the relationship between the outcome $Y$ and the covariates $\\mathbf{X}$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\widehat{\\text{income}} = 2 * \\text{education (yr)} + 3.1 * \\text{married} - 1.5 * \\text{gaps in work history}\n",
    "\\end{align}\n",
    "\n",
    "What do the weights of a neural network tell you about the relationship between the covariates and the outcome?\n",
    "<img src=\"./fig/fig5.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretable Deep Learning\n",
    "We might be tempted to conclude that neural networks are uninterpretable due to their complexity. But just because we can't understand neural networks by inspecting the value of the individual weights, it does not mean that we can't understand them.\n",
    "\n",
    "In [The Mythos of Model Interpretability](https://arxiv.org/abs/1606.03490), the authors survey a large number of methods for interpreting deep models. \n",
    "\n",
    "<img src=\"fig/cnnviz.jpg\" style=\"height:400px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can Machine Learning Models Make Use of Human Concepts?\n",
    "***(with Anita Mahinpei, Justin Clark, Ike Lage, Finale Doshi-Velez)***\n",
    "\n",
    "What if instead building complex non-linear models based on raw inputs, we instead build simple linear models based on human interpretable **concepts**? We use a neural network to predict concepts from inputs and then use a linear model to predict the outcome from the concepts. We interpret the relationship between the outcome and the concepts via the linear model. These models are called **concept bottleneck models**.\n",
    "\n",
    "In [The Promises and Pitfalls of Black-box Concept Learning Models](https://arxiv.org/abs/2106.13314), we examine the advantages and drawbacks of these models.\n",
    "\n",
    "<img src=\"fig/slide15.png\" style=\"height:300px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can Machine Learning Models Learn to Explore Hypothetical Scenarios?\n",
    "***(with Michael Downs, Jonathan Chu, Wisoo Song, Yaniv Yacoby, Finale Doshi-Velez)***\n",
    "\n",
    "Rather than explaining why the model made a decision, it's often more helpful to explain how to change the data in order to change the model's decision. This modified input is a **counter-factual**. In [CRUDS: Counterfactual Recourse Using Disentangled Subspaces](https://finale.seas.harvard.edu/files/finale/files/cruds-_counterfactual_recourse_using_disentangled_subspaces.pdf), we study how to automatically generate counter-factual explanations that can help users achieve a favorable outcome from a decision system.\n",
    "\n",
    "<img src=\"fig/slide16.png\" style=\"height:350px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Right for the Right Reasons?\n",
    "\n",
    "In [*An explainable deep-learning algorithm for the detection of acute intracranial haemorrhage from small datasets*](), the authors build a neural network model to detect acute intracranial haemorrhage (ICH) and classifies five ICH subtypes. \n",
    "\n",
    "Model classifications are explained by highlighting the pixels that contributed the most to the decision. The highligthed regions tends to overlapped with ‘bleeding points’ annotated by neuroradiologists on the images.\n",
    "\n",
    "<img src=\"./fig/shap.png\" style=\"height: 350px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Perils of Explanations\n",
    "In [*How machine-learning recommendations influence clinician treatment selections: the example of antidepressant selection*](), the authors found that clinicians interacting with incorrect recommendations paired with simple explanations experienced significant reduction in treatment selection accuracy.\n",
    "\n",
    "<img src=\"./fig/reliance.png\" style=\"height: 350px;\" align=\"center\"/>\n",
    "\n",
    "\n",
    "**Take-away:** Incorrect ML recommendations may adversely impact clinician treatment selections and that explanations are insufficient for addressing overreliance on imperfect ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization Error and Bias/Variance\n",
    "Complex models have ***low bias*** -- they can model a wide range of functions, given enough samples.\n",
    "\n",
    "But complex models like neural networks can use their 'extra' capacity to explain non-meaningful features of the training data that are unlikely to appear in the test data (i.e. noise). These models have ***high variance*** -- they are very sensitive to small changes in the data distribution, leading to drastic performance decrease from train to test settings.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"./fig/fig11.png\" style=\"width: 380px;\" align=\"center\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./fig/fig12.png\" style=\"width: 380px;\" align=\"center\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization of Deep Models\n",
    "\n",
    "Just as in the case of linear and polynomial models, we can prevent nerual networks from overfitting (i.e. poor generalization due to high variance) by regularization or by ensembling a large number of models.\n",
    "\n",
    "However, a new body of work like [Deep Double Descent: Where Bigger Models and More Data Hurt](https://mltheory.org/deep.pdf) show that very wide neural networks (with far more parameters than there are data observations) actually ceases to overfit as the width surpasses a certain threshold. In fact, as the width of a neural network approaches infinity, training the neural network becomes kernel regression (this kernel is called the ***neural tangent kernel***)!\n",
    "\n",
    "<img src=\"./fig/dd.jpg\" style=\"width: 500px;\" align=\"center\"/>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
