{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #10: Bayesian Latent Variable Models and Variational Inference\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Bayesian Latent Variable Models\n",
    "2. Coordinate Ascent Variational Inference\n",
    "3. Bayesian Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Latent Variable Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Latent Variable Models\n",
    "Overfitting is an always a concern when using MLE model parameters. We can mitigate the effect of outliers in the data on the model we learn by treating the parameters as random variables and placing priors on them. \n",
    "\n",
    "In a latent variable model, maximum liklihood inference treats parameters $\\theta$, $\\phi$ as unknown constants and produces point-estimates for them. In a Bayesian latent variable model, $\\theta$, $\\phi$ are random variables and we derive the posterior distribution over them.\n",
    "\n",
    "<img src=\"fig/bayesian_model.jpg\" style=\"height:220px;\">\n",
    "\n",
    "\n",
    "That is, we want to infer \n",
    "$$p(\\theta, \\phi, Z_1, \\ldots, Z_N|Y_1, \\ldots, Y_N, a, b) = \\frac{p(\\theta | a)p(\\phi|b)\\prod_{n}p(Y_n|Z_n, \\phi)p(Z_n|\\theta)}{\\prod_{n} p(Y_n)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges in Bayesian Inference\n",
    "Unfortunately, most Bayesian models with multiple types of random variables (like Bayesian latent variable models) have complex posteriors that do not match known distributions. ***Exact inference*** is not possible. \n",
    "\n",
    "Sampling from the posterior may not always be the best option because:\n",
    "\n",
    "1. Convergence of samplers may be slow (due to high dimensionality of the distribution or multimodality)<br><br>\n",
    "\n",
    "2. Samplers like Metropolis-Hastings requires evaluating the liklihood $\\prod_n p(Y_n | Z_n, \\phi)$ in each iteration, if the observed data is large ($N$ is in the millions), this computation is expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Idea of Variational Inference\n",
    "\n",
    "**Idea: (Approximate Inferenec)** Approximate the hard posterior $p(\\theta, \\phi, Z_1, \\ldots, Z_N|Y_1, \\ldots, Y_N)$ with a distribution $q$ that is easy to sample from (like a Gaussian). Any computation involving the posterior can now be done with $q$.\n",
    "\n",
    "This approximation of $p(\\theta, \\phi, Z_1, \\ldots, Z_N|Y_1, \\ldots, Y_N)$ with a distribution $q$ is called ***variational inference***.\n",
    "<img src=\"fig/variational.jpg\" style=\"height:220px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Design of the Variational Objective\n",
    "\n",
    "**Goal:** given a target posterior distribution $p(\\psi | Y_1, \\ldots, Y_N)$, $\\psi \\in \\mathbb{R}^I$ we want to find a distribution $q(\\psi |\\lambda^*)$ in a family of distributions $Q = \\{q(\\psi |\\lambda) | \\lambda \\in \\Lambda \\}$ such that $q(\\psi |\\lambda^*)$ best approximates $p$. \n",
    "\n",
    "**Design Choices:** we need to choose:\n",
    "\n",
    "A. ***(Variational family)*** a family $Q$ of candidate distributions for approximating $p$. The members of $Q$ are called the ***variational distributions***.\n",
    "\n",
    "  **Our Choice:**  we assume that the joint $q(\\psi)$ factorizes completely over each dimension of $\\psi$, i.e. $q(\\psi)= \\prod_{i=1}^I q(\\psi_i | \\lambda_i)$. This is called the ***mean field assumption***. What can go wrong with this design choice?\n",
    "  \n",
    "B. ***(Divergence measure)*** a divergence measure to quantify the difference between $p$ and $q$.\n",
    "\n",
    "  **Our Choice:** \n",
    "  $$D_{\\text{KL}}(q(\\psi | \\lambda) \\| p(\\psi | Y_1, \\ldots, Y_N)) = \\mathbb{E}_{\\psi \\sim q(\\psi|\\lambda)}\\left[\\log\\left( \\frac{q(\\psi | \\lambda)}{p(\\psi | Y_1, \\ldots, Y_N)} \\right) \\right]$$\n",
    "  What can go wrong with this design choice?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variational Inference as Optimization\n",
    "\n",
    "We now formalize variational inference for a target $p(\\psi)$: find $q(\\psi|\\lambda^*)$ where \n",
    "\n",
    "\\begin{aligned}\n",
    "\\lambda^* &= \\underset{\\lambda}{\\text{argmin}}\\; D_{\\text{KL}}(q(\\psi|\\lambda) \\| p(\\psi|Y_1, \\ldots, Y_N))) \\\\\n",
    "&= \\underset{\\lambda}{\\text{argmin}}\\; \\mathbb{E}_{\\psi \\sim q(\\psi|\\lambda)}\\left[\\log\\left(\\frac{q(\\psi | \\lambda)}{p(\\psi|Y_1, \\ldots, Y_N))}\\right) \\right]\n",
    "\\end{aligned}\n",
    "\n",
    "Recall that for EM, we had proved that minimizing the KL is equivalent to maximizing the ELBO (for which it is easier to compute the gradient). We will do the same here:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\underset{\\lambda}{\\min}D_{\\text{KL}}(q(\\psi|\\lambda) \\| p(\\psi|Y_1, \\ldots, Y_N))) \\overset{\\text{equiv}}{\\equiv}& \\underset{\\lambda}{\\max} -D_{\\text{KL}}(q(\\psi|\\lambda) \\| p(\\psi|Y_1, \\ldots, Y_N))) \\\\\n",
    "=& \\underset{\\lambda}{\\max} -\\mathbb{E}_{\\psi \\sim q(\\psi|\\lambda)}\\left[\\log\\left(\\frac{q(\\psi | \\lambda)}{p(\\psi|Y_1, \\ldots, Y_N))} \\right)\\right] \\\\\n",
    "=& \\underset{\\lambda}{\\max}\\underbrace{\\mathbb{E}_{\\psi \\sim q(\\psi|\\lambda)}\\left[\\log\\left(\\frac{p(\\psi, Y_1, \\ldots, Y_N))}{q(\\psi | \\lambda)} \\right)\\right]}_{ELBO(\\lambda)} \\\\\n",
    "&- \\log p(Y_1, \\ldots, Y_N).\n",
    "\\end{aligned}\n",
    "\n",
    "Thus, the variational objective can be rephrased as maximizing the $ELBO$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradients of the ELBO\n",
    "\n",
    "Unfortunately, the ELBO for variational inference of the posterior does not have easy gradients,\n",
    "\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\lambda}\\,\\underbrace{\\mathbb{E}_{\\psi \\sim q(\\psi|\\lambda)}\\left[\\log\\left(\\frac{p(\\psi, Y_1, \\ldots, Y_N)}{q(\\psi | \\lambda)} \\right)\\right]}_{ELBO(\\lambda)}.\n",
    "\\end{aligned}\n",
    "\n",
    "In particular, the issue is that the gradient taken is with respect to the parameter $\\psi$ of the distribution over which we are taking the expectation - i.e. we cannot push the gradient into the expectation.\n",
    "\n",
    "Today we will maximize the $ELBO$ using coordinate ascent (just as in the case of EM). But you'll see that ***coordinate ascent variational inference*** requires that we perform model specific computations (often in closed form). This restrict the class of Bayesian models for which we can perform variational inference.\n",
    "\n",
    "Two of the major development we will cover later in the semester address how to estimate this gradient **efficiently and without bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coordinate Ascent Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<img src=\"fig/Lecture_10_derivation.pdf\" style=\"height:900px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximizing the ELBO via Coordinate Ascent\n",
    "\n",
    "The coordinate ascent algorithm maximizes an objective function $ELBO(\\lambda)$ by iteratively maximizing over $\\lambda_i$, holding constant $\\lambda_{-i} = [\\lambda_1\\; \\ldots\\; \\lambda_{i-1}\\; \\lambda_{i+1}\\; \\ldots\\; \\lambda_{I}]$.\n",
    "\n",
    "The ***coordinate ascent variational inference algorithm***:\n",
    "0. **Initialization:** pick an intial value $\\lambda^{(0)}$\n",
    "1. **Coordinate-wise maximization:** \n",
    "\n",
    "   Repeat for $j=1, \\ldots, J$ iterations:\n",
    "\n",
    "   $\\quad\\quad$ Cycle thru $i=1, \\ldots, I$ coordinates:\n",
    "  \n",
    "$$q(\\psi_i | \\lambda^{\\text{new}}_i) \\propto \\exp\\left\\{ \\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i} | \\lambda^{\\text{new}}_{1}, \\ldots, \\lambda^{\\text{new}}_{i-1}, \\lambda^{\\text{old}}_{i+1}, \\ldots, \\lambda^{\\text{old}}_{I})}\\left[\\log p(Y_1, \\ldots, Y_N, \\psi)\\right]\\right\\}.$$\n",
    "       \n",
    "where $\\psi_{-i} = [\\psi_1\\; \\ldots\\; \\psi_{i-1}\\; \\psi_{i+1}\\; \\ldots\\; \\psi_{I}]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Proof of the Update Rule for $q(\\psi_i | \\lambda^{\\text{new}}_i)$\n",
    "\n",
    "We want to show that $q(\\psi_i | \\lambda^{*}_i) \\propto \\exp\\left\\{ \\mathbb{E}_{\\psi_{-i} \\sim q(\\phi_{-i} | \\lambda_{-i})}\\left[\\log p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i})\\right]\\right\\}$, where\n",
    "\n",
    "$$\n",
    "\\lambda^{*}_i = \\underset{\\lambda_i}{\\max}\\underbrace{\\mathbb{E}_{\\psi \\sim q(\\psi|\\lambda)}\\left[\\log\\left(\\frac{p(\\psi, Y_1, \\ldots, Y_N))}{q(\\psi | \\lambda)} \\right)\\right]}_{ELBO(\\lambda)}.\n",
    "$$\n",
    "\n",
    "To maximize the $ELBO$, we will \n",
    "\n",
    "1. use the mean-field assumption to break up the expectation $\\mathbb{E}_{\\psi \\sim q(\\psi|\\lambda)}$ into an interated expectation $\\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})}$\n",
    "2. we will rewrite the outer expectation $\\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}$ as a negative KL-divergence\n",
    "3. we will maximize the negative KL-divergence by setting the two arguments of the divergence equal to each other\n",
    "\n",
    "For the following, we will use the following notation:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\lambda_{-i} &= [\\lambda_1\\; \\ldots\\; \\lambda_{i-1}\\; \\lambda_{i+1}\\; \\ldots\\; \\lambda_{I}]\\\\\n",
    "\\psi_{-i} &= [\\psi_1\\; \\ldots\\; \\psi_{i-1}\\; \\psi_{i+1}\\; \\ldots\\; \\psi_{I}]\\\\\n",
    "q(\\psi_{-i}|\\lambda_{-i}) &= \\prod_{j\\neq i}q(\\psi_{j}|\\lambda_{j})\\\\\n",
    "\\Psi_{-i} &= \\bigcup_{j\\neq i} \\Psi_j\\\\\n",
    "d\\psi_{-1} &= d(\\psi_1, \\ldots, \\psi_{i-1}, \\psi_{i+1}, \\ldots, \\psi_{I})\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "### Step 1: Show that $\\mathbb{E}_{\\psi \\sim q(\\psi|\\lambda)}[\\ldots] = \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})}[\\ldots] \\right]$\n",
    "\n",
    "For step 1, we rewrite the ELBO:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\underbrace{\\mathbb{E}_{\\psi \\sim q(\\psi|\\lambda)}\\left[\\log\\left(\\frac{p(\\psi, Y_1, \\ldots, Y_N))}{q(\\psi | \\lambda)} \\right)\\right]}_{ELBO(\\lambda)} &= \\int_{\\Psi} \\left[ \\log \\left(\\frac{p(Y_1, \\ldots, Y_N, \\psi)}{q(\\psi|\\lambda)} \\right)\\right]q(\\psi|\\lambda) d\\psi\\\\\n",
    "&= \\int_{\\Psi_i} \\int_{\\Psi_{-i}} \\left[ \\log \\left(\\frac{p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i})p(\\psi_{-i})}{q(\\psi_{i}|\\lambda_{i})q(\\psi_{-i}|\\lambda_{-i})} \\right)\\right] q(\\psi_{i}|\\lambda_{i})q(\\psi_{-i}|\\lambda_{-i}) d\\psi_{-i}d\\psi_i\\quad (\\text{Fubini's Theorem})\\\\\n",
    "&= \\int_{\\Psi_i} \\left[\\int_{\\Psi_{-i}} \\left[ \\log \\left(\\frac{p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i})p(\\psi_{-i})}{q(\\psi_{i}|\\lambda_{i})q(\\psi_{-i}|\\lambda_{-i})} \\right)\\right] q(\\psi_{-i}|\\lambda_{-i}) d\\psi_{-i} \\right] q(\\psi_{i}|\\lambda_{i})d\\psi_i\\\\\n",
    "&= \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(\\frac{p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i})p(\\psi_{-i})}{q(\\psi_{i}|\\lambda_{i})q(\\psi_{-i}|\\lambda_{-i})} \\right)\\right]\\right]\n",
    "\\end{aligned}\n",
    "\n",
    "Now we decompose the ELBO into terms containing $\\lambda_i$, over which we are going to optimize, and terms not containing $\\lambda_i$:\n",
    "\n",
    "\\begin{aligned}\n",
    "ELBO(\\lambda)&=\\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(\\frac{p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i})p(\\psi_{-i})}{q(\\psi_{i}|\\lambda_{i})q(\\psi_{-i}|\\lambda_{-i})} \\right)\\right]\\right]\\\\\n",
    "&= \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(\\frac{p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i})}{q(\\psi_{i}|\\lambda_{i})} \\right) + \\log\\left(\\frac{p(\\psi_{-i})}{q(\\psi_{-i}|\\lambda_{-i})} \\right)\\right]\\right]\\\\\n",
    "&= \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(\\frac{p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i})}{q(\\psi_{i}|\\lambda_{i})} \\right)\\right]\\right] + \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log\\left(\\frac{p(\\psi_{-i})}{q(\\psi_{-i}|\\lambda_{-i})} \\right)\\right]\\right]\\\\\n",
    "&= \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(\\frac{p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i})}{q(\\psi_{i}|\\lambda_{i})} \\right)\\right]\\right] + \\underbrace{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log\\left(\\frac{p(\\psi_{-i})}{q(\\psi_{-i}|\\lambda_{-i})} \\right)\\right]}_{\\text{constant with respect to }\\lambda_i}\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Note that we don't need to consider the constant term when optimizing with respect to $\\lambda_i$.\n",
    "\n",
    "### Step 2: Show that $\\underset{\\lambda_i}{\\max}\\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}[\\ldots] \\equiv \\underset{\\lambda_i}{\\min} D_{\\text{KL}}[\\ldots]$\n",
    "\n",
    "We show that maximizing the $ELBO$ with respect to $\\lambda_{i}$ is equivalent to minimizing a KL-divergence:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\underset{\\lambda_i}{\\max} ELBO(\\lambda) &= \\underset{\\lambda_i}{\\max} \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(\\frac{p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i})}{q(\\psi_{i}|\\lambda_{i})} \\right)\\right]\\right]\\\\\n",
    "&= \\underset{\\lambda_i}{\\max} \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right] - \\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log\\left(q(\\psi_{i}|\\lambda_{i})\\right)\\right]\\right]\\\\\n",
    "&= \\underset{\\lambda_i}{\\max} \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right] - \\log\\left(q(\\psi_{i}|\\lambda_{i})\\right)\\right]\\\\\n",
    "&= \\underset{\\lambda_i}{\\max} \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[ \\log \\left(\\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right] - \\log\\left(q(\\psi_{i}|\\lambda_{i})\\right)\\right\\}\\right)\\right]\\quad (\\text{adding both a log and an exp})\\\\\n",
    "&= \\underset{\\lambda_i}{\\max} \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[ \\log \\left(\\frac{\\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right]\\right\\} } {\\exp \\left\\{\\log\\left(q(\\psi_{i}|\\lambda_{i})\\right)\\right\\}} \\right)\\right]\\\\\n",
    "&= \\underset{\\lambda_i}{\\max} \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[ \\log \\left(\\frac{\\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right]\\right\\} } {q(\\psi_{i}|\\lambda_{i})} \\right)\\right]\\\\\n",
    "&\\equiv \\underset{\\lambda_i}{\\min} -\\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[ \\log \\left(\\frac{\\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right]\\right\\} } {q(\\psi_{i}|\\lambda_{i})} \\right)\\right]\\\\\n",
    "&= \\underset{\\lambda_i}{\\min} \\underbrace{\\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[ \\log \\left(\\frac{ q(\\psi_{i}|\\lambda_{i}) } {\\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right]\\right\\} } \\right)\\right]}_{\\text{looks like a KL except the denominator is not a distribution}}\\quad (\\text{using the properties of log})\\\\\n",
    "&= \\underset{\\lambda_i}{\\min} \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[ \\log \\left(\\frac{ \\mathcal{Z} q(\\psi_{i}|\\lambda_{i}) } {\\mathcal{Z}\\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right]\\right\\} } \\right)\\right],\\quad (\\mathcal{Z} \\text{ normalizes the denominator})\\\\\n",
    "&= \\underset{\\lambda_i}{\\min} \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[ \\log \\left(\\frac{  q(\\psi_{i}|\\lambda_{i}) } {\\mathcal{Z}\\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right]\\right\\} } \\right)\\right] +\\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[ \\log(\\mathcal{Z})\\right] \\\\\n",
    "&= \\underset{\\lambda_i}{\\min} \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[ \\log \\left(\\frac{  q(\\psi_{i}|\\lambda_{i}) } {\\mathcal{Z}\\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right]\\right\\} } \\right)\\right] +\\log(\\mathcal{Z})\\\\\n",
    "&= \\underset{\\lambda_i}{\\min} \\mathbb{E}_{\\psi_i \\sim q(\\psi_i|\\lambda_i)}\\left[ \\log \\left(\\frac{  q(\\psi_{i}|\\lambda_{i}) } {\\mathcal{Z}\\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right]\\right\\} } \\right)\\right]\\\\\n",
    "&= \\underset{\\lambda_i}{\\min}D_{\\text{KL}} \\left[ q(\\psi_{i}|\\lambda_{i})\\| \\mathcal{Z}\\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right]\\right\\}\\right].\n",
    "\\end{aligned}\n",
    "\n",
    "### Step 3: Minimize the KL-divergence\n",
    "\n",
    "We see that \n",
    "\n",
    "$$\n",
    "D_{\\text{KL}} \\left[ q(\\psi_{i}|\\lambda_{i})\\| \\mathcal{Z}\\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right]\\right\\}\\right]\n",
    "$$\n",
    "\n",
    "is minimized when \n",
    "\n",
    "$$\n",
    " q(\\psi_{i}|\\lambda_{i})\\propto \\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right]\\right\\}.\n",
    "$$\n",
    "\n",
    "This is also exactly where the ELBO is maximized.\n",
    "\n",
    "We can further rewrite the update rule for $ q(\\psi_{i}|\\lambda_{i})$ as:\n",
    "\n",
    "\\begin{aligned}\n",
    " q(\\psi_{i}|\\lambda_{i})&\\propto \\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)\\right]\\right\\}\\\\\n",
    " &= \\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)p(\\psi_{-i})\\right] - \\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log p(\\psi_{-i})\\right]\\right\\}\\\\\n",
    " &= \\frac{\\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)p(\\psi_{-i})\\right]\\right\\}}{\\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})}  \\left[\\log p(\\psi_{-i})\\right]\\right\\}}\\\\\n",
    " &\\propto \\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi_i | \\psi_{-i}) \\right)p(\\psi_{-i})\\right]\\right\\}\\\\\n",
    " &= \\exp\\left\\{\\mathbb{E}_{\\psi_{-i} \\sim q(\\psi_{-i}|\\lambda_{-i})} \\left[\\log \\left(p(Y_1, \\ldots, Y_N, \\psi) \\right)\\right]\\right\\}\n",
    "\\end{aligned}\n",
    "\n",
    "The rational for rewriting the update for $q(\\psi_{i}|\\lambda_{i})$ in terms of the joint $p(Y_1, \\ldots, Y_N, \\psi)$ is that the joint is easy to compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variational Inference for Bayesian Gaussian Mixture Models\n",
    "We consider a Bayesian model for a mixture of $K$ number of univariate Gaussians:\n",
    "\n",
    "<img src=\"fig/bayesian_gmm.jpg\" style=\"height:250px;\">\n",
    "\n",
    "The ***hyperparameters*** of the models are $\\pi, \\sigma^2, m_0, s^2_0$, which are constants that must be selected prior to inference. For example, to simplify our computations we selected $\\pi = [1/K, \\ldots, 1/K]$, $m_0 = 0$, $\\sigma=1$.\n",
    "\n",
    "We make the mean field assumption -- that our variational posterior factorizes completely:\n",
    "\n",
    "$$q(Z, \\mu| m, s^2, \\phi) = \\prod_{k=1}^K q(\\mu_k|m_k, s_k^2) \\prod_{n=1}^N q(Z_n|\\phi_n).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Coordinate Ascent Variational Inference Updates for Bayesian GMM\n",
    "\n",
    "In coordinate ascent variational inference, we update the variational distribution for each variable in turn. For the Bayesian GMM this means:\n",
    "\n",
    "\\begin{aligned}\n",
    "q(Z_n | \\phi_n) &\\propto \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2), Z_{-n} \\sim q(Z_{-n} | \\phi_{-n})}\\left[\\log p(Y_1, \\ldots, Y_N, Z_1, \\ldots, Z_N, \\mu)\\right]\\right\\}\\\\\n",
    "q(\\mu_k | m_k, s_k^2) &\\propto \\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi), q(\\mu_{-k}|m_{-k}, s_{-k})}\\left[\\log p(Y_1, \\ldots, Y_N, Z_1, \\ldots, Z_N, \\mu)\\right]\\right\\}\n",
    "\\end{aligned}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{aligned}\n",
    "q(\\mu | m, s^2) &= \\prod_{k=1}^K q(\\mu_k|m_k, s_k^2),\\\\\n",
    "q(\\mu_{-k} | m_{-k}, s_{-k}^2) &= \\prod_{j\\neq k} q(\\mu_j|m_j, s_j^2),\\\\\n",
    "Z &= [Z_1\\;\\;\\ldots\\;\\; Z_N],\\\\\n",
    "q(Z |\\phi) &= \\prod_{n=1}^N q(Z_n|\\phi_n),\\\\\n",
    "Z_{-n} &= [Z_1\\;\\;\\ldots\\;\\;Z_{n-1}\\;\\; Z_{n+1}\\;\\;\\ldots\\;\\; Z_N],\\\\\n",
    "q(Z_{-n} |\\phi_{-n}) &= \\prod_{m\\neq n} q(Z_m|\\phi_m).\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "### Update rule for $q(Z_n | \\phi_n)$\n",
    "\n",
    "To update $q(Z_n | \\phi_n)$, we rewrite $exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2), Z_{-n} \\sim q(Z_{-n} | \\phi_{-n})}\\left[\\log p(Y_1, \\ldots, Y_N, Z_1, \\ldots, Z_N, \\mu)\\right]\\right\\}$, dropping all terms that do not involve $Z_n$ and $\\mu$:\n",
    "\n",
    "\\begin{aligned}\n",
    "q(Z_n | \\phi_n) &\\propto \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2), Z_{-n} \\sim q(Z_{-n} | \\phi_{-n})}\\left[\\log p(Y_1, \\ldots, Y_N, Z_1, \\ldots, Z_N, \\mu)\\right]\\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2), Z_{-n} \\sim q(Z_{-n} | \\phi_{-n})}\\left[\\log \\left(\\prod_{m=1}^N p(Y_m|\\mu, Z_m) p(Z_m)\\right)p(\\mu)\\right]\\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2), Z_{-n} \\sim q(Z_{-n} | \\phi_{-n})}\\left[\\log \\left(\\prod_{m=1}^N p(Y_m|\\mu, Z_m) p(Z_m)\\right)\\right] + \\log[p(\\mu)]\\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2), Z_{-n} \\sim q(Z_{-n} | \\phi_{-n})}\\left[\\log \\left(\\prod_{m=1}^N p(Y_m|\\mu, Z_m) p(Z_m)\\right)\\right]\\right\\} \\exp\\left\\{ \\log[p(\\mu)]\\right\\}\\\\\n",
    "&\\propto \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2), Z_{-n} \\sim q(Z_{-n} | \\phi_{-n})}\\left[\\log \\left(\\prod_{m=1}^N p(Y_m|\\mu, Z_m) p(Z_m)\\right)\\right]\\right\\}\\quad\\quad (\\text{dropped priors on $\\mu$})\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2), Z_{-n} \\sim q(Z_{-n} | \\phi_{-n})}\\left[\\log \\left(p(Y_n | \\mu, Z_n) p(Z_n) \\prod_{m\\neq n} p(Y_m|\\mu, Z_m) p(Z_m)\\right)\\right]\\right\\}\\\\\n",
    "&=  \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2), Z_{-n} \\sim q(Z_{-n} | \\phi_{-n})}\\left[ \\log p(Y_n | \\mu, Z_n) + \\log p(Z_n) + \\sum_{m\\neq n} \\log \\left( p(Y_m|\\mu, Z_m) p(Z_m)\\right) \\right]\\right\\}\\\\\n",
    "&=  \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[ \\log p(Y_n | \\mu, Z_n) + \\log p(Z_n)\\right] +  \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2), Z_{-n} \\sim q(Z_{-n} | \\phi_{-n})}\\left[\\sum_{m\\neq n} \\log \\left( p(Y_m|\\mu, Z_m) p(Z_m)\\right) \\right]\\right\\}\\\\\n",
    "&=  \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[ \\log p(Y_n | \\mu, Z_n) + \\log p(Z_n)\\right]\\right\\} \\underbrace{\\exp\\left\\{\\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2), Z_{-n} \\sim q(Z_{-n} | \\phi_{-n})}\\left[\\sum_{m\\neq n} \\log \\left( p(Y_m|\\mu, Z_m) p(Z_m)\\right) \\right]\\right\\}}_{\\text{constant with respect to $Z_n$}}\\\\\n",
    "&\\propto  \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[ \\log p(Y_n | \\mu, Z_n) + \\log p(Z_n)\\right]\\right\\} \\quad\\quad (\\text{dropped all terms except for the $n$-th})\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[ \\log p(Y_n | \\mu, Z_n) \\right]+ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)} \\left[\\log p(Z_n)\\right]\\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[ \\log p(Y_n | \\mu, Z_n) \\right]+ \\log p(Z_n)\\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[ \\log p(Y_n | \\mu, Z_n) \\right]+ \\log \\frac{1}{K}\\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[ \\log p(Y_n | \\mu, Z_n) \\right]\\right\\} \\exp\\left\\{\\log \\frac{1}{K}\\right\\}\\\\\n",
    "&\\propto \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[ \\log p(Y_n | \\mu, Z_n) \\right]\\right\\}\\quad\\quad (\\text{dropped the prior on $Z_n$})\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "The likelihood $p(Y_n | \\mu, Z_n)$ is a normal distribution given the value of $Z_n$, we can express this by treating each $Z_n$ as an indicator variable -- a $K$-dimensional binary vector with $Z_{nk}=1$ if $Y_n$ is in class $k$ and 0 otherwise. Thus, we can write\n",
    "\\begin{aligned}\n",
    "\\log p(Y_n | \\mu, Z_n) &= \\log \\prod_{k=1}^K \\mathcal{N}(Y_n; \\mu_k, 1)^{Z_{nk}}\\\\\n",
    "&= \\sum_{k=1}^K Z_{nk} \\log \\mathcal{N}(Y_n; \\mu_k, 1)\\\\\n",
    "&= -\\frac{1}{2}\\sum_{k=1}^K Z_{nk} (Y_n - \\mu_k)^2 + const\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "Finally, we can write the update for $q(Z_n | \\phi_n)$ as follows, again dropping all terms not involving $Z_n$ and $\\mu$:\n",
    "\n",
    "\\begin{aligned}\n",
    "q(Z_n | \\phi_n) &\\propto  \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[ -\\frac{1}{2}\\sum_{k=1}^K Z_{nk} (Y_n - \\mu_k)^2 + const\\right]\\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[ -\\frac{1}{2}\\sum_{k=1}^K Z_{nk} (Y_n - \\mu_k)^2\\right]\\right\\} \\exp\\left\\{\\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)} \\left[const\\right]\\right\\}\\\\\n",
    "&\\propto \\exp\\left\\{ \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[ -\\frac{1}{2}\\sum_{k=1}^K Z_{nk} (Y_n - \\mu_k)^2\\right]\\right\\}\\\\\n",
    "&= \\exp\\left\\{  -\\frac{1}{2}\\sum_{k=1}^K Z_{nk} \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[Y^2_n - 2Y_n\\mu_k + \\mu_k^2\\right]\\right\\}\\\\\n",
    "&= \\exp\\left\\{  -\\frac{1}{2}\\sum_{k=1}^K Z_{nk} \\left(\\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[Y^2_n\\right] - 2Y_n\\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[\\mu_k\\right] + \\mathbb{E}_{\\mu \\sim q(\\mu | m, s^2)}\\left[\\mu_k^2\\right]\\right)\\right\\}\\\\\n",
    "&= \\exp\\left\\{  -\\frac{1}{2}\\sum_{k=1}^K Z_{nk} \\left(Y^2_n - 2Y_nm_k + (m_k + s_k^2)\\right)\\right\\}\\\\\n",
    "&= \\exp\\left\\{  -\\sum_{k=1}^K Z_{nk} Y^2_n  +\\sum_{k=1}^KZ_{nk}\\left( Y_nm_k - \\frac{(m_k + s_k^2)}{2}\\right) \\right\\}\\\\\n",
    "&= \\exp\\left\\{  -\\sum_{k=1}^K Z_{nk} Y^2_n\\right\\} \\exp\\left\\{ \\sum_{k=1}^KZ_{nk}\\left( Y_nm_k - \\frac{(m_k + s_k^2)}{2}\\right) \\right\\}\\\\\n",
    "&\\propto \\exp\\left\\{\\sum_{k=1}^KZ_{nk}\\left( Y_nm_k - \\frac{(m_k + s_k^2)}{2}\\right) \\right\\}\n",
    "\\end{aligned}\n",
    "\n",
    "Since $q(Z_n | \\phi_n)$ is a categorical distribution, we can use the above to compute the $k$-th component as:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\phi_{nk}\\propto \\exp\\left\\{ Y_n m_k - \\frac{(m_k + s_k^2)}{2} \\right\\}\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "### Update rule for $q(\\mu_k | m_k, s_k^2)$\n",
    "\n",
    "To update $q(\\mu_k | m_k, s_k^2)$, we compute:\n",
    "\\begin{aligned}\n",
    "q(\\mu_k | m_k, s_k^2) &\\propto \\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi), q(\\mu_{-k}|m_{-k}, s_{-k})}\\left[\\log p(Y_1, \\ldots, Y_N, \\psi)\\right]\\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi), q(\\mu_{-k}|m_{-k}, s_{-k})}\\left[\\log \\left(\\prod_{n=1}^N p(Y_n|\\mu, Z_n) p(Z_n)\\right) p(\\mu)\\right] \\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi), q(\\mu_{-k}|m_{-k}, s_{-k})}\\left[\\log \\left(\\prod_{n=1}^N p(Y_n|\\mu, Z_n) \\right) p(\\mu)\\right]  +  \\log \\prod_{n=1}^N p(Z_n) \\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi), q(\\mu_{-k}|m_{-k}, s_{-k})}\\left[\\log \\left(\\left(\\prod_{n=1}^N p(Y_n|\\mu, Z_n) \\right)p(\\mu)\\right) \\right] \\right\\}  \\exp\\left\\{\\log \\prod_{m=1}^Mp(Z_m) \\right\\}\\\\\n",
    "&\\propto \\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi), q(\\mu_{-k}|m_{-k}, s_{-k})}\\left[\\log \\left(\\prod_{k=1}^K \\prod_{n=1}^N p(Y_n|\\mu_k, Z_n) \\prod_{k=1}^K p(\\mu_k)\\right) \\right] \\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi), q(\\mu_{-k}|m_{-k}, s_{-k})}\\left[\\log \\left(\\prod_{n=1}^N p(Y_n|\\mu_k, Z_n) p(\\mu_k) \\left(\\prod_{j\\neq k} \\prod_{n=1}^N p(Y_n|\\mu_j, Z_n) \\prod_{j\\neq k} p(\\mu_j)\\right) \\right)\\right] \\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi), q(\\mu_{-k}|m_{-k}, s_{-k})}\\left[\\log \\left(\\prod_{n=1}^N p(Y_n|\\mu_k, Z_n) p(\\mu_k)\\right) + \\log \\left(\\prod_{j\\neq k} \\prod_{n=1}^N p(Y_n|\\mu_j, Z_n)  \\prod_{j\\neq k} p(\\mu_j)\\right)\\right] \\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi), q(\\mu_{-k}|m_{-k}, s_{-k})}\\left[\\log \\left(\\prod_{n=1}^N p(Y_n|\\mu_k, Z_n) p(\\mu_k)\\right)\\right] + \\mathbb{E}_{Z \\sim q(Z | \\phi), q(\\mu_{-k}|m_{-k}, s_{-k})}\\left[\\log \\left(\\prod_{j\\neq k} \\prod_{n=1}^N p(Y_n|\\mu_j, Z_n)  \\prod_{j\\neq k} p(\\mu_j)\\right)\\right] \\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi)}\\left[\\log \\left(\\prod_{n=1}^N p(Y_n|\\mu_k, Z_n) p(\\mu_k)\\right)\\right] \\right\\} \\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi), q(\\mu_{-k}|m_{-k}, s_{-k})}\\left[\\log \\left(\\prod_{j\\neq k} \\prod_{n=1}^N p(Y_n|\\mu_j, Z_n)  \\prod_{j\\neq k} p(\\mu_j)\\right)\\right] \\right\\}\\\\\n",
    "&\\propto \\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi)}\\left[\\log \\left(\\prod_{n=1}^N p(Y_n|\\mu_k, Z_n) p(\\mu_k)\\right)\\right] \\right\\}\n",
    "&=\\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi)}\\left[\\sum_{n=1}^N \\log p(Y_n|\\mu_k, Z_n) + \\log p(\\mu_k)\\right] \\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\mathbb{E}_{Z \\sim q(Z | \\phi)}\\left[\\sum_{n=1}^N \\sum_{j=1}^K Z_{nj}\\log \\mathcal{N}(Y_n; \\mu_j, 1) + \\log \\mathcal{N}(\\mu_k; 0, s^2_0)\\right] \\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\sum_{n=1}^N \\sum_{j=1}^K \\mathbb{E}_{Z \\sim q(Z | \\phi)}\\left[Z_{nj}\\right]\\log \\mathcal{N}(Y_n; \\mu_j, 1) + \\log \\mathcal{N}(\\mu_k; 0, s^2_0) \\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\sum_{n=1}^N \\sum_{j=1}^K \\phi_{nj}\\log \\mathcal{N}(Y_n; \\mu_j, 1) + \\log \\mathcal{N}(\\mu_k; 0, s^2_0) \\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\sum_{n=1}^N  \\phi_{nk}\\log \\mathcal{N}(Y_n; \\mu_k, 1) + \\log \\mathcal{N}(\\mu_k; 0, s^2_0)  +  \\sum_{n=1}^N \\sum_{j\\neq k} \\phi_{nj}\\log \\mathcal{N}(Y_n; \\mu_j, 1) \\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\sum_{n=1}^N  \\phi_{nk}\\log \\mathcal{N}(Y_n; \\mu_k, 1) + \\log \\mathcal{N}(\\mu_k; 0, s^2_0)\\right\\}  \\exp\\left\\{\\sum_{n=1}^N \\sum_{j\\neq k} \\phi_{nj}\\log \\mathcal{N}(Y_n; \\mu_j, 1) \\right\\}\\\\\n",
    "&\\propto \\exp\\left\\{ \\sum_{n=1}^N  \\phi_{nk}\\log \\mathcal{N}(Y_n; \\mu_k, 1) + \\log \\mathcal{N}(\\mu_k; 0, s^2_0)\\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\sum_{n=1}^N  \\phi_{nk} \\frac{(y_n-\\mu_k)^2}{2} - \\frac{\\mu_k^2}{2\\sigma_0^2} + const\\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\sum_{n=1}^N  \\phi_{nk} \\frac{(y_n-\\mu_k)^2}{2} - \\frac{\\mu_k^2}{2\\sigma_0^2}\\right\\}\\exp\\left\\{const\\right\\}\\\\\n",
    "&\\propto \\exp\\left\\{ \\sum_{n=1}^N  \\phi_{nk} \\frac{(y_n-\\mu_k)^2}{2} - \\frac{\\mu_k^2}{2\\sigma_0^2}\\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\left(\\sum_{n=1}^N  \\phi_{nk} y_n\\right)\\mu_k - \\frac{1}{2}\\left(\\sum_{n=1}^N  \\phi_{nk} + \\frac{1}{\\sigma_0^2}\\right)\\mu^2_k + const\\right\\}\\\\\n",
    "&= \\exp\\left\\{ \\left(\\sum_{n=1}^N  \\phi_{nk} y_n\\right)\\mu_k - \\frac{1}{2}\\left(\\sum_{n=1}^N  \\phi_{nk} + \\frac{1}{\\sigma_0^2}\\right)\\mu^2_k\\right\\} \\exp\\left\\{ const\\right\\}\\\\\n",
    "&\\propto \\exp\\left\\{ \\underbrace{\\left(\\sum_{n=1}^N  \\phi_{nk} y_n\\right)}_{A}\\mu_k - \\underbrace{\\frac{1}{2}\\left(\\sum_{n=1}^N  \\phi_{nk} + \\frac{1}{\\sigma_0^2}\\right)}_{B}\\mu^2_k\\right\\} \\\\\n",
    "&= \\exp\\left\\{ - (B\\mu_k^2 - A\\mu_k)\\right\\}\\\\\n",
    "&= \\exp\\left\\{ - B\\left(\\mu_k^2 - \\frac{A}{B}\\mu_k + \\left( \\frac{A}{2B}\\right)^2 - \\left( \\frac{A}{2B}\\right)^2 \\right)\\right\\}\\\\\n",
    "&= \\exp\\left\\{ - B\\left(\\mu_k^2 - \\frac{A}{2B}\\right)^2  - B\\left( \\frac{A}{2B}\\right)^2 \\right\\}\\\\\n",
    "&= \\exp\\left\\{ - B\\left(\\mu_k^2 - \\frac{A}{2B}\\right)^2\\right\\} \\exp\\left\\{- B\\left( \\frac{A}{2B}\\right)^2 \\right\\}\\\\\n",
    "&\\propto \\exp\\left\\{ - B\\left(\\mu_k^2 - \\frac{A}{2B}\\right)^2\\right\\} \n",
    "\\end{aligned}\n",
    "\n",
    "Thus, we see that $q(\\mu_k | m_k, s_k^2) \\propto \\exp\\left\\{ - B\\left(\\mu_k^2 - \\frac{A}{2B}\\right)^2\\right\\} $ is a Gaussian with mean \n",
    "\n",
    "$$m_k = \\frac{A}{2B} = \\frac{\\sum_{n=1}^N \\phi_{nk} y_n}{\\sum_{n=1}^N \\phi_{nk} + \\frac{1}{\\sigma_0^2}},$$\n",
    "\n",
    "and variance\n",
    "\n",
    "$$s^2_k = \\frac{1}{2B} = \\frac{1}{\\sum_{n=1}^N \\phi_{nk} + \\frac{1}{\\sigma_0^2}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implemenation of CAVI for Bayesian GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Generate data\n",
    "N = 2000\n",
    "pis = [1./3, 1./3, 1./3]\n",
    "mus = [6, 10, 14]\n",
    "sigmas = [1., 1., 1.]\n",
    "K = 3\n",
    "zs = np.random.choice(np.arange(K), size=N, p=pis)\n",
    "y = np.array([np.random.normal(mus[z], sigmas[z]**0.5, 1)[0] for z in zs])\n",
    "\n",
    "#defining the bayesian model\n",
    "sigma_sq = 1.**2\n",
    "s_sq_0 = 1.\n",
    "\n",
    "#initialization for CAVI\n",
    "m_init = np.array([2, 4, 5])\n",
    "s_sq_init = np.array([1., 1., 1.])\n",
    "\n",
    "#implement coordinate ascent VI\n",
    "m_current = m_init\n",
    "s_sq_current = s_sq_init\n",
    "\n",
    "total_iter = 10000\n",
    "threshold = 1e-10\n",
    "delta_m = 1.\n",
    "delta_s = 1.\n",
    "i = 0\n",
    "\n",
    "#implementation of the ELBO\n",
    "def elbo(m, s_sq, phi):\n",
    "    summand_1 = (np.log(s_sq) - m / s_sq_0).sum()\n",
    "    summand_2 = ((-0.5 * np.add.outer(y**2, s_sq + m**2) + np.outer(y, m) - np.log(phi)) * phi).sum()\n",
    "    return summand_1 + summand_2\n",
    "\n",
    "ELBOs = []\n",
    "while i < total_iter and delta_m > threshold and delta_s > threshold:\n",
    "    #update q(Z_n)\n",
    "    exponent = np.outer(y, m_current) + -(0.5 * m_current**2 + 0.5 * s_sq_current)[np.newaxis, :]\n",
    "    phi_unnormalized = np.exp(exponent)\n",
    "    phi = phi_unnormalized / phi_unnormalized.sum(1)[:, np.newaxis]\n",
    "    #update mu\n",
    "    m_new = (phi * y[:, np.newaxis]).sum(0) * (1. / s_sq_0 + phi.sum(0))**(-1)\n",
    "    #update s squared \n",
    "    s_sq_new = (1. / s_sq_0 + phi.sum(0))**(-1)\n",
    "    \n",
    "    #compute ELBO\n",
    "    ELBOs.append(elbo(m_new, s_sq_new, phi))\n",
    "    \n",
    "    #compute variational parameter change\n",
    "    delta_m = np.linalg.norm(m_new - m_current)\n",
    "    delta_s = np.linalg.norm(s_sq_new - s_sq_current)\n",
    "    \n",
    "    m_current = m_new\n",
    "    s_sq_current = s_sq_new\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "#sample from posterior predictive    \n",
    "def posterior_predictive_sampling(means, variances, S):\n",
    "    posterior_predictive_samples = []\n",
    "    for mean, variance in zip(means, variances):\n",
    "        posterior_samples = np.random.normal(mean, variance**2, size=S)\n",
    "        posterior_predictive_samples += [np.random.normal(sample, sigma_sq**2, 1)[0] for sample in posterior_samples]\n",
    "    return np.array(posterior_predictive_samples)\n",
    "    \n",
    "x = np.linspace(y.min(), y.max(), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.hist(y, bins=60, density=True, color='gray', alpha=0.7, label='histogram of birth weights')\n",
    "posterior_predictive_samples = posterior_predictive_sampling(m_current, s_sq_current, 100)\n",
    "ax.hist(posterior_predictive_samples, bins=60, density=True, color='red', alpha=0.3, label='variational posterior predictive')\n",
    "ax.set_title('GMM for Birth Weights')\n",
    "ax.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sanity Check: ELBO During Training\n",
    "\n",
    "Remember that ploting the posterior predictive against actual data is not always an option (e.g. high-dimensional data).\n",
    "\n",
    "A sanity check for that your CAVI algorithm has been implemented correctly is to plot the ELBO (or alternatively, the observed data log-likelihood) over the iterations of the algorithm:\n",
    "\n",
    "$$\n",
    "ELBO(\\phi, m, s^2) = \\mathbb{E}_{Z, \\mu\\sim q(Z, \\mu | \\phi, m, s^2)} \\left[\\log \\left( \\frac{p(Y_1, \\ldots, Y_N, Z_1, \\ldots, Z_N, \\mu)}{q(Z, \\mu | \\phi, m, s^2)}\\right) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 2))\n",
    "ax.plot(range(len(ELBOs)), ELBOs, color='red', alpha=0.5)\n",
    "ax.set_title('ELBO over iterations of CAVI')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
