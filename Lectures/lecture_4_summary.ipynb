{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #4: Bayesian versus Frequentist Inference\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview of Lecture #4\n",
    "\n",
    "1. What is the Bayesian modeling processing?\n",
    "  - **(Model Definition)** Define the joint model over the data and the model parameters $p(y, \\theta)$. For this, you need to ***choose***:\n",
    "    - the ***likelihood*** $p(y|\\theta)$, e.g. $y \\sim \\mathcal{N}(\\theta, 2)$\n",
    "    - then ***prior*** $p(\\theta)$, e.g. $\\theta \\sim \\mathcal{N}(0, 1)$\n",
    "  - **(Model Inference)** Learn the best ***distribution*** over models that fits your data, $p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)}$.\n",
    "  - **(Model Evaluation)** Why is model evaluation hard? How do you evaluate a ***distribution*** over models, especially if we're not doing regression (i.e. we have no covariates)?\n",
    "    - Use the models in the posterior to generate synethic data, in order to compare with real data, i.e. ***sample from the posterior predictive***.\n",
    "    - Compute the marginal ***log-likelihood*** of the data (train or test) over the posterior: $\\sum_{m=1}^M \\log \\int_\\Theta p(\\mathbf{y}^*_m | \\theta) p(\\theta| \\text{Data}) d\\theta$<br><br>\n",
    " \n",
    "2. Why is any of this hard?\n",
    " - How do we choose a prior?\n",
    " - How do we compute the posterior distribution? From Piazza: \"is there a standard procedure?\"\n",
    " - How do we sample from the posterior predictive?\n",
    " - How do we compute the log-likelihood?<br><br>\n",
    " \n",
    "3. Remember we said that we can get uncertainties from frequentist approaches to modeling too: by bootstrapping! There are even more connections between frequentist and Bayesian approaches:\n",
    " - Regularization of MLEs can be interpreted as posterior modes of Bayesian models! I.e. regularization terms correspond to (sometimes complex) priors!\n",
    " - If we want to mimic the frequentist approach of giving one best model we can reduce the posterior to a ***point estimate***, what is wrong with this approach?\n",
    " - **(Asymptotic Behavior)** For sufficiently nice likelihoods, as the number of observations approach infinity, the posterior distribution approaches a Gaussian distribution with mean (and mode) centered at the MLE!\n",
    " - **Wait, why do I care about the connections between frequentist and Bayesian approaches again?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
