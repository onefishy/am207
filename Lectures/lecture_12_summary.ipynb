{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #12: Logistic Regression and Gradient Descent\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture #12 Summary\n",
    "\n",
    "For a set of observations $y$, we can collect additional data $x$ and model the ***conditional distribution*** $p(Y|X)$. This is exactly regression and classification models do, they use ***covariates*** $x$ to predict the ***outcome*** $y$.\n",
    "\n",
    "1. **(Conditional Models)** Why do we need additional data $x$? Why isn't it sufficient to just model $y$ as a binomial, Guassian or a mixture of Gaussians?\n",
    "  - to predict\n",
    "  - to explain \n",
    "  - to model finer grain variations in y<br><br>\n",
    "\n",
    "2. **(Classification)** For binary data (like whether or not an individual has kidney cancer), we can model $p(Y|X)$ as a Bernoulli\n",
    "$$\n",
    "Y^{(n)} \\sim Ber\\left(\\mathrm{sigm}\\left(f\\left(\\mathbf{X}^{(n)}; \\mathbf{w}\\right)\\right)\\right).\n",
    "$$\n",
    "Geomtrically, $f\\left(\\mathbf{X}^{(n)}; \\mathbf{w}\\right)$ gives the distance from a decision boundary that separates $y=1$ from $y=0$, and we convert this distance into a probability using the sigmoid function $\\mathrm{sigm}$.\n",
    "\n",
    "<img src=\"fig/fig0.png\" alt=\"\" style=\"height: 200px;\"/>\n",
    "\n",
    "\n",
    "3. **(Maximizing the Likelihood)** We want to maximize the log Bernoulli likelihood of $y$ (conditioned on $x$) over model parameters $\\mathbf{w}$, but the zeros of the gradient of the log-likelihood cannot be analytically solved for!\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\ell(\\mathbf{w}) = -\\sum_{n=1}^N \\left(y^{(n)} - \\frac{1}{1 + e^{-\\mathbf{w}^\\top\\mathbf{x}^{(n)}}} \\right) \\mathbf{x}^{(n)} =\\mathbf{0}\n",
    "$$\n",
    "  But just because we can't analytically find the zeros of the gradient, it doesn't mean that the gradient is useless! The gradient of a loss function points to the direction of the greatest rate of increase, hence ***the negative gradient points to the direction of the fastest way of decreasing the loss function***. \n",
    "\n",
    "  When we iteratively decrease the loss function by changing $\\mathbf{w}$ in the direction of the negative gradient, this is called ***gradient descent***.<br><br>\n",
    "  \n",
    "4. **(Properties of Gradient Descent)** Gradient descent sounds like we can optimize any function without work, is it as good as it sounds?\n",
    "  - What do I need to know in order to use gradient descent?\n",
    "  - Is gradient descent guaranteed to converge?\n",
    "  - Is gradient descent efficient?\n",
    "    - EM was also an algorithm to maximize a log pdf. What is the difference between EM and gradient descent? What was the point of learning EM if we could have just learned gradient descent?<br><br>\n",
    "    \n",
    "5. **(The Challenges of Conditional Models)** Conditional models have so much potential to generate scientific understanding of our data but they are also the most frequently misused! \n",
    "\n",
    "  Suppose that you fit a logistic regression model to predict whether a loan application should be approved. Suppose that you have three covariates: \n",
    "\n",
    "  `x_1` representing gender: 0 for male, 1 for female<br>\n",
    "  `x_2` for the income<br>\n",
    "  `x_3` for the loan amount\n",
    "\n",
    "  Suppose that the parameters you found are:\n",
    "$$\n",
    "p(y=1 | x_1, x_2, x_3) = \\mathrm{sigm}(-1 + 3 x_1 + 1.5 x_2 + 1.75 x_3).\n",
    "$$\n",
    "  What can I conclude about the loan decisions data that this model was fitted on? Can I deploy this model for usage?\n",
    "  - **Bias in your conclusions is exponentially exacerbated by bias in the data collection process.**<br><br>\n",
    "    - **What information are you asking for?** If you ask the wrong question, the covariates you collect are at best unpredictive, and ***at worse they can mask the signal in the data***.\n",
    "    \n",
    "      In many data collection processes, one finds that the questionaire might be using \"sex\" and \"gender\" interchangeably, or \"race\" and \"ethnicity\" interchageably. But the former (depending on field and context) frequently refers to biological invariants (like chromosomes and genetic lineage), whereas the latter frequently refers to lived experiences (related to identity, culture etc). **Caveat:** although we collect data on race and sex as if delineation in these categories are more \"immutable\" or \"factual\", many have/do argue that these concepts are also social constructs (see discussion on false binaries and false aggregtaion)!\n",
    "    \n",
    "      When you ask for \"sex\" are you asking about a person's chromosomal information, becaues you're studying chromosomal related diseases, or are you asking about their experiences with the US medical system and how their health outcome is affected by their gendery identity OR the identity that people tend to assume of them? **For example:** women, especially women of color, tend to be under-diagnosed and mis-diagnosed due to gendered treatments of patients.\n",
    "      \n",
    "      To complicate matters, the terms \"gender\" and \"sex\" can have very specific legal definitions -- that may not align with defintions in medical or sociological contexts! For example, the language of the Equal Credit Opportunity Act is written in terms of \"sex\", however, this term is not defined in the ECOA. The Consumer Financial Protection Bureau interprets (in their operating philosophy) this term to include both sexual orientation and gender identity.  <br><br>\n",
    "      \n",
    "    - **Did you already bias the answer?** If you gave confusing, misleading or incorrect options for answers (think about when you were frustrated with the concept quizzes!), then the covariates you collect are at best unpredictive, and ***at worse they can mask the signal in the data***.\n",
    "    \n",
    "      In many data collection processes, one finds that the respondent is forced into a category that is unrepresentative of their experience, like having \"Asian\" as a broad racial category that includes South Asian, East Asian, South East Asian, like providing binary choices for gender or sex.\n",
    "      \n",
    "      If you are looking for disparaties in health outcomes, you might miss disparaties amongst important smaller subgroups within broad group of \"Asian\", since health outcomes are often affected by income and  Asian-Americans have the largest in-group income disparity. <br><br>\n",
    "      \n",
    "    - **Should you be asking for this information?** The complexity of asking a seemingly simple question can seem overwhelming, often we resort to over-collecting (collecting every piece of information that we can think of) or under-collecting (not collecting any sensitive attribute). <br><br>\n",
    "    \n",
    "      **Why you might not want to** But respondents (e.g. patients, applicants for benefits) can be suspicious that their data can be used precisely to discriminate against them on the basis of protected attributes. Note that data collection have often historically been used to oppress, not to up-lift, vunerable communities. So collecting this data without clear purpose can compromise user trust and constitute a privacy threat if this data is not well protected.<br><br>\n",
    "    \n",
    "      **Why you should** However, in order to make sure our model is not disproportionately negatively impacting vunerable subgroups and that we are in compliancenb with laws like the EOCA, we need to check for disparate impact on protected groups. But in order to do that, we need to have access to sensitive attributes.<br><br> \n",
    "\n",
    "  - **Is correlation causation?**<br><br>\n",
    "    - It appears that the sex of the applicant the logistic regression model has the greatest influence on the prediction of the model, does this mean that the human decision makers were biased? (**Hint:** not necssarily, but it might be a hypothesis to explore to retroactively detect potential regulatory violation in this dataset.)<br><br>\n",
    "    \n",
    "    - If the interpretations of the coefficients of the logistic regression model does not imply human bias in the data, then can we use this model for real life loan decisions? (**Hint:** the answer is NO. It would be an violation!)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
