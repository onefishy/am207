{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #18: Automatic Differentiation\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "import numpy\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "import autograd.scipy.stats.multivariate_normal as mvn\n",
    "import autograd.scipy.stats.norm as norm\n",
    "from autograd import grad\n",
    "from autograd.misc.optimizers import adam\n",
    "import numpy\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "from IPython.display import HTML\n",
    "from IPython.display import YouTubeVideo\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Review of BBVI\n",
    "2. Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Review of Black Box Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Developments in Computationally Efficient Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. **Black-box Variational Inference (2013)**\n",
    "  Uses the log-derivative trick to rewrite the gradient of the ELBO as:\n",
    "  \n",
    "  $$\\nabla_{\\mu, \\Sigma} \\, ELBO(\\mu, \\Sigma) = \\mathbb{E}_{\\mathbf{W} \\sim q(\\mathbf{W} | \\mu, \\Sigma)}\\left[ \\nabla_{\\mu, \\Sigma}\\, q(\\mathbf{W} | \\mu, \\Sigma) * \\log \\left( \\frac{p(\\mathbf{W}) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)})}{q(\\mathbf{W} | \\mu, \\Sigma)} \\right) \\right]$$\n",
    "  \n",
    "  This requires **only** the computation of the gradient of $q(\\mathbf{W})$, which is generally much simpler than $p(\\mathbf{W}) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)})$. \n",
    "  \n",
    "  Implementation of BBVI means hard-coding a large library of different kinds of variational distributions $q(\\mathbf{W} | \\lambda)$ and their gradients. User inputs the joint distribution of their Bayesian model and chooses a variational family $q(\\mathbf{W} | \\lambda)$ - then you can optimize the variational parameters $\\lambda$ to best approximate the target posterior by gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Developments in Computationally Efficient Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. **Weight Uncertainty in Neural Networks (2015)**\n",
    "  Assuming the variational family is mean-field Gaussian, uses the reparametrization trick to rewrite the gradient of the ELBO as:\n",
    "  \\begin{align}\n",
    "  \\nabla_{\\mu, \\Sigma} \\, ELBO(\\mu, \\Sigma)=& \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})}\\left[\\nabla_{\\mu, \\Sigma} \\log \\left[p(\\epsilon^\\top \\Sigma^{1/2} + \\mu) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\epsilon^\\top \\Sigma^{1/2} + \\mu)\\right]\\right]\\\\\n",
    "  &+ \\nabla_{\\mu, \\Sigma}\\underbrace{\\mathbb{E}_{\\mathbf{W} \\sim \\mathcal{N}(\\mu, \\Sigma )}\\left[\\log \\mathcal{N}(\\mathbf{W};\\mu, \\Sigma ) \\right]}_{\\text{Guassian entropy: has closed form}}\n",
    "  \\end{align}\n",
    "  \n",
    "  For Bayesian Neural Networks, the gradient in the above can be computed by backpropagation - then you can optimize the variational parameters $\\mu, \\Sigma$ to best approximate the target posterior by gradient descent. This algorithm is called **Bayes by Backprop**.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Developments in Computationally Efficient Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. **Automatic Differentiaition Variational Inference (2016)**\n",
    "  Anytime the gradient of the ELBO can be written as the expectation of a gradient (of an expression without integrals), the gradient can be computed by any automatic differentiation package - then you can optimize the variational parameters $\\lambda$ to best approximate the target posterior by gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of Computational Differentiation\n",
    "\n",
    "1. Manually computing closed form expressions of derivatives and then coding them up (say using `numpy` functions).<br><br>\n",
    "\n",
    "2. **numeric differentiation:** approximate a derivative $\\frac{df(x)}{dx}$ with a rate of change $\\frac{f(x+h) - f(x)}{h}$ at $x=a$.<br><br>\n",
    "\n",
    "3. **symbolic differentiation:** manipulate expressions of functions using pre-programmed rules (of differentiation).<br><br>\n",
    "\n",
    "4. **automatic differentiation:** algorithmic computation of exact numeric derivatives. `autograd` is a `python` implementation of automatic differentiation. `pytorch` implements one particular mode of automatic differentiation, also called `autograd`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numeric Differentiation\n",
    "\n",
    "Given $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, approximate each gradient $\\nabla f_j = \\left(\\frac{\\partial f_j}{\\partial x_i}\\ldots \\frac{\\partial f_j}{\\partial x_n}\\right)$ for $i = 1, \\ldots, n$ and $j = 1, \\ldots, m$ with\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{f}}{\\partial x_i} \\approx \\frac{f(x + he_i) - f(x)}{h}\n",
    "$$\n",
    "where $e_i$ is the $i$-th standard basis vector of $\\mathbb{R}^n$.\n",
    "\n",
    "This is numerically unstable when $h\\approx 0$ and biased when $h$ is large. For each gradient $\\nabla f_j$, it requires $O(n)$ computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Symbolic Differentiation\n",
    "\n",
    "Given an expression for a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, we represent the expression as a tree and automatically manipulate the expression tree by applying transformations representing differentiation:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}[h(x) + g(x)] \\rightarrow \\frac{d}{dx}h(x) + \\frac{d}{dx}g(x)\n",
    "$$\n",
    "\n",
    "This can be computationally inefficient since expressions of derivatives can be exponentially longer than the original function expression:\n",
    "\n",
    "\\begin{align}\n",
    "f(x) &= h(x)^2 g(x) + \\ln(h(x)) + g(x)\\\\\n",
    "f'(x) &= 2h(x)h'(x)g(x) + h(x)^2g'(x) + \\frac{h'(x)}{h(x)} + g'(x)\n",
    "\\end{align}\n",
    "\n",
    "Numerical evaluation of the derivative can be inefficient due to the redudant evaluation of the components of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic Differentiation: The Idea\n",
    "\n",
    "We decompose a function $f(a, b) = (a + b)(b + 1)$ into elementary operations:\n",
    "\n",
    "<img src=\"fig/computation_graph.png\" style=\"height:250px;\">\n",
    "\n",
    "We apply symbolic differentiation to elementary operations, like: arithmetic operations, elementary functions (exponential, logarithmic, trignometric, power).\n",
    "\n",
    "We keep intermediate values of the components of $f$ so that they can be reused.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation Trace and Computational Graph\n",
    "\n",
    "Given a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, we represent $f$ as the composition of elementary functions through elemtary operations by a sequence of intermediate values $v_k$ that is involved with the evaluation of $f$, this is the ***evaluation trace***. We can also represent the trace graphically, resulting in the ***computational graph***.\n",
    "\n",
    "**Example:** Given $y = f(x_1, x_2) = \\ln(x_1) + x_1x_2 - \\sin(x_2)$, its evaluation trace and computational graph are:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"fig/trace.jpg\" style=\"height:250px;\" align=\"center\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./fig/graph.jpg\" style=\"height: 200px;\" align=\"center\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic Differentiation: Forward Mode\n",
    "\n",
    "In ***forward mode automatic differentiation***, we start with the input and work towards the output: evaluating the value of each intermediate value $v_k$ as well as the derivative of $v_k$ with repect to a fixed $x_i$ using the chain rule: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial v_k}{\\partial x_i} = \\sum_{v\\in \\mathrm{parent}(v_k)}\\frac{\\partial v_k}{\\partial v}\\frac{\\partial v}{\\partial x_i}\n",
    "$$\n",
    "We denote $\\frac{\\partial v_k}{x_i}$ by $\\dot{v}_k$.\n",
    "\n",
    "<img src=\"fig/forward.jpg\" style=\"height:300px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic Differentiation: Reverse Mode\n",
    "\n",
    "In ***reverse mode automatic differentiation***, we first do a forward pass to compute all intermediate values. Then we start with in the output and work towards the input: evaluating the derivative of $f$ with repect to an intermediate value $v_k$ using the chain rule: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial v_k} = \\sum_{v\\in \\mathrm{child}(v_k)}\\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial v_k}\n",
    "$$\n",
    "We denote $\\frac{\\partial f}{v_k}$ by $\\overline{v}_k$. \n",
    "\n",
    "<img src=\"fig/reverse.jpg\" style=\"height:300px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing Reverse Mode AutoDiff\n",
    "\n",
    "We see that each intermediate gradient computation $\\frac{\\partial f}{\\partial v_k}$ in reverse mode autodiff is local, it only requires:\n",
    "1. the current value of $v_k$\n",
    "2. the derivative of $f$ with respect to every child of $v_k$: $\\frac{\\partial f}{\\partial v},\\, v\\in \\mathrm{child}(v_k)$.\n",
    "3. the derivative of the elementary function $h_v$ describing the way $v$ depends on $v_k$.\n",
    "\n",
    "We implement the computation graph of a function $f$ as a directed graph, where each node keeps tracks of the above three pieces of information and uses them to compute its own gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An Example of Reverse Mode AutoDiff in `python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "'''small example of reverse mode autodiff as implemented in https://github.com/Rufflewind/revad/'''\n",
    "class Var:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.children = []\n",
    "        self.grad_value = None\n",
    "\n",
    "    def grad(self):\n",
    "        if self.grad_value is None:\n",
    "            self.grad_value = sum(weight * var.grad()\n",
    "                                  for weight, var in self.children)\n",
    "        return self.grad_value\n",
    "\n",
    "    #overloading the '+' operator\n",
    "    def __add__(self, other):\n",
    "        z = Var(self.value + other.value)\n",
    "        self.children.append((1.0, z))\n",
    "        other.children.append((1.0, z))\n",
    "        return z\n",
    "\n",
    "def sin(x):\n",
    "    z = Var(math.sin(x.value))\n",
    "    x.children.append((math.cos(x.value), z))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of y + sin(x) evaluated at x=0.5, y=4.2: 4.679425538604203\n",
      "forward pass of our implementation: 4.679425538604203\n"
     ]
    }
   ],
   "source": [
    "x = Var(0.5)\n",
    "y = Var(4.2)\n",
    "z = y + sin(x)\n",
    "z.grad_value = 1.0\n",
    "\n",
    "print('value of y + sin(x) evaluated at x=0.5, y=4.2: {}\\nforward pass of our implementation: {}'.format(4.2 + np.sin(0.5), z.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8775825618903728"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of dz/dx = cos(x) evaluated at x=0.5, y=4.2: 0.8775825618903728\n",
      "reverse pass of our implementation: 0.8775825618903728\n"
     ]
    }
   ],
   "source": [
    "print('value of dz/dx = cos(x) evaluated at x=0.5, y=4.2: {}\\nreverse pass of our implementation: {}'.format(np.cos(0.5), x.grad_value))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
