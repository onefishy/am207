{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #17: Black-box Variational Inference\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary of Lecture #17\n",
    "\n",
    "1. **(Width vs Depth)** In a NN architecture, we control the width and the depth of the network. What is the effect of increasing each (independently) on the function that we learn?\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"./fig/wide.jpg\" style=\"height: 350px;\" align=\"center\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./fig/deep.jpg\" style=\"height: 350px;\" align=\"center\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "  - Extremely wide NNs have fundamentally different behaviors than wide NNs\n",
    "    \n",
    "    [Reconciling modern machine learning practice and the bias-variance trade-off](https://arxiv.org/pdf/1812.11118.pdf)<br><br>\n",
    "\n",
    "  - Deep networks behave fundamentally differently than shallow but wide NNs \n",
    "    \n",
    "    [Exponential expressivity in deep neural networks through transient chaos](https://proceedings.neurips.cc/paper/2016/file/148510031349642de5ca0c544f31b2ef-Paper.pdf)<br><br>\n",
    "\n",
    "  - Deep networks can have unexpected pathologies \n",
    "    \n",
    "    [COLLAPSE OF DEEP AND NARROW NEURAL NETS](https://openreview.net/pdf?id=r1MSBjA9Ym)\n",
    "    \n",
    "    [DEEP DOUBLE DESCENT: WHERE BIGGER MODELS AND MORE DATA HURT](https://arxiv.org/pdf/1912.02292.pdf)\n",
    "<br><br>\n",
    "\n",
    "2. **(Linear vs Neural Network Models)** While both simple and complex models can be interpreted and successfully trained, there are trade-offs between using a complex vs a simple model!\n",
    "\n",
    "  - Easily interpreted vs interpretations are harder to come by\n",
    "  - Interpretations are more faithful vs interpretations can be more misleading\n",
    "  - Training is easier (because objective function is convex) vs training is much harder (because objective function is extremely non-convex)<br><br>\n",
    "  \n",
    "3. **(Bayesian Neural Networks)** Deep Bayes refers to the subfield of ML wherein we try to combine deep learning with Bayesian learning. A Bayesian Neural Network is one example of a deep Bayesian model.\n",
    "\n",
    "  - Why do we want to make our NNs Bayesian? \n",
    "  - How do we make NNs Bayesian?\n",
    "  - How do we sample from the posterior?\n",
    "    - Will sampling from the posterior using HMC going to work?\n",
    "    - Can we do variational inference? Now we can - with 4 pieces of technology:\n",
    "      - algebra tricks to get the gradient past the expectation in the ELBO\n",
    "      - Monte Carlo estimate of the expectation in the ELBO\n",
    "      - automatic differentiation\n",
    "      - gradient descent\n",
    "      \n",
    "      \\begin{aligned}\n",
    "\\nabla_{\\mu, \\Sigma}\\,\\underbrace{\\mathbb{E}_{\\mathbf{W} \\sim q(\\mathbf{W} | \\mu, \\Sigma)} \\left[ \\log \\left( \\frac{p(\\mathbf{W}) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\mathbf{W})}{q(\\mathbf{W} | \\mu, \\Sigma)} \\right) \\right]}_{ELBO(\\mu, \\Sigma)}.\n",
    "\\end{aligned}\n",
    "      \n",
    "---\n",
    "### Variational Inference for BNNs\n",
    "The ***Black-box Variational Inference (BBVI)*** algorithm for BNN's:\n",
    "0. **Initialization:** pick an intial value $\\mu^{(0)}, \\Sigma^{(0)}$\n",
    "1. **Gradient Ascent:** repeat:\n",
    "\n",
    "   1. Approximate the gradient \n",
    "   \\begin{align}\n",
    "   \\nabla_{\\mu, \\Sigma} \\, ELBO(\\mu, \\Sigma) &= \\nabla_{\\mu, \\Sigma}\\,\\underbrace{\\mathbb{E}_{\\mathbf{W} \\sim q(\\mathbf{W} | \\mu, \\Sigma)} \\left[ \\log \\left( \\frac{p(\\mathbf{W}) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\mathbf{W})}{q(\\mathbf{W} | \\mu, \\Sigma)} \\right) \\right]}_{ELBO(\\mu, \\Sigma)}\\\\\n",
    "   &= \\underbrace{\\mathbb{E}_{\\mathbf{W} \\sim q(\\mathbf{W} | \\mu, \\Sigma)}\\left[ \\nabla_{\\mu, \\Sigma}\\, q(\\mathbf{W} | \\mu, \\Sigma) * \\log \\left( \\frac{p(\\mathbf{W}) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\mathbf{W})}{q(\\mathbf{W} | \\mu, \\Sigma)} \\right) \\right]}_{\\text{score function gradient}}\\\\\n",
    "   &\\approx\\underbrace{\\frac{1}{S}\\sum_{s=1}^S \\nabla_{\\mu, \\Sigma}\\, \\log q(\\mathbf{W}^s | \\mu, \\Sigma) * \\log \\left( \\frac{p(\\mathbf{W}^s) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\mathbf{W}^s)}{q(\\mathbf{W}^s | \\mu, \\Sigma)} \\right)}_{{\\text{MC approx. of score function gradient}}},\n",
    "   \\end{align}\n",
    "   where $\\mathbf{W}^s\\sim q(\\mathbf{W} | \\mu^{\\text{current}}, \\Sigma^{\\text{current}})$.<br><br>\n",
    "   2. Update parameters $(\\mu^{\\text{current}}, \\Sigma^{\\text{current}}) \\leftarrow (\\mu^{\\text{current}}, \\Sigma^{\\text{current}}) + \\eta * {\\text{(MC approx. of score function gradient)}}$<br><br>\n",
    "\n",
    "The ***reparametrization gradient*** is another way to push the gradient past the expectation in the gradient of the ELBO.\n",
    "\n",
    "We note that since $q(\\mathbf{W} | \\mu, \\Sigma) = \\mathcal{N}(\\mathbf{W};\\mu, \\Sigma )$, sampling $W\\sim q(\\mathbf{W} | \\mu, \\Sigma)$ is equivalent to sampling $\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})$ and then transforming the sample $\\mathbf{W} = \\epsilon^\\top \\Sigma^{1/2} + \\mu$, where $\\mathbf{I}$ and $\\Sigma$ have the same dimensions.\n",
    "\n",
    "Thus, we can rewrite the ELBO:\n",
    "<img src=\"./fig/reparametrized_grad.png\" style='height:300px;'>\n",
    "\n",
    "The ***Black-box Variational Inference (BBVI) with the reparametrization trick*** algorithm for BNN's:\n",
    "0. **Initialization:** pick an intial value $\\mu^{(0)}, \\Sigma^{(0)}$\n",
    "1. **Gradient Ascent:** repeat:\n",
    "\n",
    "   1. Approximate the gradient \n",
    "   \\begin{align}\n",
    "   \\nabla_{\\mu, \\Sigma} \\, ELBO(\\mu, \\Sigma) \\approx& \\small\\frac{1}{S} \\sum_{s=1}^S \\nabla_{\\mu, \\Sigma} \\log \\left[p(\\epsilon_s^\\top \\Sigma^{1/2} + \\mu) \\prod_{n=1}^N p(Y^{(n)} | \\mathbf{X}^{(n)}, \\epsilon_s^\\top \\Sigma^{1/2} + \\mu)\\right] \\\\\n",
    "   &+ \\nabla_{\\mu, \\Sigma}\\underbrace{-\\mathbb{E}_{\\mathbf{W} \\sim \\mathcal{N}(\\mu, \\Sigma )}\\left[\\log \\mathcal{N}(\\mathbf{W};\\mu, \\Sigma ) \\right]}_{\\text{Guassian entropy: has closed form}},\n",
    "   \\end{align}\n",
    "   where $\\epsilon_s \\sim \\mathcal{N}(0, \\mathbf{I})$.\n",
    "   2. Update parameters $(\\mu^{\\text{current}}, \\Sigma^{\\text{current}}) \\leftarrow (\\mu^{\\text{current}}, \\Sigma^{\\text{current}}) + \\eta * {\\text{reparametrization gradient}}$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
