{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #2: Maximimum Likelihood Estimation\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "1. A Motivating Example\n",
    "2. A Statistical Model for a Coin Flip\n",
    "3. Maximum Likelihood Estimation\n",
    "4. Convex Optimization: Constrained and Unconstrained\n",
    "5. Properties of MLE\n",
    "6. Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Motivating Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Simple Betting Game\n",
    "\n",
    "I propose to you that we play a betting game: I toss a coin, if the coin lands heads up then you will pay me $\\$20$, otherwise I will pay you $\\$20$.\n",
    "<img src=\"fig/quarter.jpg\" style=\"height:250px;\">\n",
    "\n",
    "**Question:** What information do you need to determine if this will be a profitable game for you to play?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimating the \"Bias\" of a Coin\n",
    "\n",
    "You might want to determine if my coin is a \"trick\" or \"biased\" coin before betting your money. A common way to test a coin for bias is to toss this coin $N$ number of times and count the number of heads, $H$. The fraction\n",
    "\n",
    "$$\n",
    "\\frac{\\text{Number of Heads}}{\\text{Total Number of Tosses}} = \\frac{H}{N}\n",
    "$$\n",
    "\n",
    "is one way to quantify the probability of the coin to land heads up on any given toss. \n",
    "\n",
    "Alternatively, we can interpret this fraction to represent the fraction of heads that would appear in a large (infinite) number of such experiments. \n",
    "\n",
    "**Question 1:** Is this estimate of the bias valid? I.e. does $\\frac{H}{N}$ acurately capture the property of interest?\n",
    "\n",
    "**Question 2:** Is this the \"best\" way to estimate the bias? For example, is the quantity\n",
    "\n",
    "$$\n",
    "\\frac{\\text{Number of Heads} + 1}{\\text{Total Number of Tosses} + 2} = \\frac{H + 1}{N + 2}\n",
    "$$\n",
    "\n",
    "an equally valid or better estimate of the bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Statistical Model for a Coin Toss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Likelihood for a Coin Toss\n",
    "\n",
    "We can formally model the outcome of the single toss of a coin by a Bernoulli distribution\n",
    "$$\n",
    "Y \\sim Ber(\\theta)\n",
    "$$\n",
    "where $\\theta$ is the probability that the outcome $Y$ will be heads.\n",
    "\n",
    "**Question:** what assumptions does this statistical model expose?\n",
    "\n",
    "After $N$ number of ***independent*** tosses of an ***identical*** coin, the probability (or likelihood) of observing $Y=H$ number of heads is\n",
    "\n",
    "$$\n",
    "{N \\choose H} \\theta^{H} (1 - \\theta)^{N-H}\n",
    "$$\n",
    "\n",
    "That is, $Y$ is a random variable with a ***binomial*** distribution $Y \\sim Bin(N, \\theta)$.\n",
    "\n",
    "We see that the fraction $\\frac{H}{N}$ from our empirical experiment is an estimate of the parameter $\\theta$ of the binomal distribution $Bin(N, \\theta)$. Now that we have a statistical model, we can give formal justification for why our estimate is desirable (or undesirable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameter Estimation: Maximum Likelihood\n",
    "Let $Y_1, \\ldots, Y_N$ be independently and identically distributed with $Y_n \\sim p(Y|\\theta)$, where $p(Y|\\theta)$ is a distribution parameterized by $\\theta$ ($\\theta$ can be a scalar, a vector, a matrix, or a n-tuple of such quantities). The ***joint likelihood*** of $N$ observations, $y_1, \\ldots, y_N$, is \n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\theta) = \\prod_{n=1}^N p(y_n | \\theta)\n",
    "\\end{aligned}\n",
    "*Note that we use upper-case letters $Y_n$ to represent random variables and lower-case $y_n$ to represent specific observed values of those variables.*\n",
    "\n",
    "The joint likelihood quantifies how likely (or probable, if $Y$ is discrete) we are to observed the data assuming the model $\\theta$. When we consider the joint likelihood as a function of $\\theta$ (that is, treat the observed data as fixed), the $\\mathcal{L}(\\theta)$ is called the ***likelihood function***.\n",
    "\n",
    "The ***maximium likelihood estimate*** of $\\theta$ is defined as\n",
    "\\begin{aligned}\n",
    "\\theta_{\\text{MLE}} = \\underset{\\theta}{\\mathrm{argmax}}\\; \\mathcal{L}(\\theta) = \\underset{\\theta}{\\mathrm{argmax}}\\; \\prod_{n=1}^N p(y_n | \\theta)\n",
    "\\end{aligned}\n",
    "Recall that in Lecture #1 we gave some intuitive justification for the validity of the MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximizing Likelihood is Equivalent to Maximizing Log-Likelihood\n",
    "\n",
    "Frequently, the likelihood function is complex and so it's often preferable to work with the log of the likelihood function. Luckily, ***maximizing the likelihood is equivalent to maximizing the log likelihood*** due to the following fact. \n",
    "\n",
    "**Theorem:**  For any $f: \\mathbb{R}^D \\to \\mathbb{R}$, we have that $x^* = \\underset{\\theta}{\\mathrm{argmax}}\\; f(x)$ if and only if $x^* = \\underset{\\theta}{\\mathrm{argmax}}\\; \\log (f(x))$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "***Proof:*** Recall that the monotone property of the $\\log: \\mathbb{R} \\to \\mathbb{R}$ function:\n",
    "\n",
    "$$\n",
    "z_1 < z_2 \\text{ if and only if } \\log(z_1) < \\log(z_2).\n",
    "$$\n",
    "\n",
    "Suppose that $x^* = \\underset{\\theta}{\\mathrm{argmax}}\\; f(x)$, then for all $x\\in \\mathbb{R}^D$ we must have that $f(x) \\leq f(x^*)$. Hence, it follow from the monotonicity of $\\log$ that $\\log(f(x)) \\leq \\log(f(x^*))$, for all $x\\in \\mathbb{R}^D$. So, by definition, we have that $x^* = \\underset{\\theta}{\\mathrm{argmax}}\\; \\log (f(x))$. <br><br> Now suppose that $x^* = \\underset{\\theta}{\\mathrm{argmax}}\\; \\log (f(x))$, for all $x\\in \\mathbb{R}^D$. That is, for any $x\\in \\mathbb{R}^D$, we have that $\\log (f(x)) \\leq \\log (f(x^*))$. It then follows from the monotonicity of $\\log$ that $ f(x) \\leq f(x^*)$ for all $x\\in \\mathbb{R}^D$. By definition, we conclude that $x^* = \\underset{\\theta}{\\mathrm{argmax}}\\; f(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convex Optimization: Constrained and Unconstrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to Optimization: Types of Optima\n",
    "\n",
    "<img src=\"fig/optima.jpg\" style=\"height:450px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stationary Points\n",
    "\n",
    "The instaneous rate of change, at $x=x_0$, of a differentiable function $f: \\mathbb{R} \\to \\mathbb{R}$ is given by it's first derivative at $x=x^*$, $\\left.\\frac{df}{dx}\\right\\vert_{x^*}$.\n",
    "\n",
    "For a multivariate differentiable function $f: \\mathbb{R}^D \\to \\mathbb{R}$, the ***gradient*** of $f$ at a point $x^*$ is a vector consisting of the partial derivatives of $f$ evaluated at $x^*$:\n",
    "$$\n",
    "\\left.\\nabla_x f \\right\\vert_{x^*}= \\left[\\left.\\frac{\\partial}{\\partial x^{(1)}}\\right\\vert_{x^*}, \\ldots, \\left.\\frac{\\partial}{\\partial x^{(D)}}\\right\\vert_{x^*}\\right]\n",
    "$$\n",
    "\n",
    "Each $\\left.\\frac{\\partial}{\\partial x^{(1)}}\\right\\vert_{x^*}$ compute the instantaneous change of $f$ at $x=x^*$ with respect to $x^{(1)}$.\n",
    "\n",
    "The gradient is orthogonal to the level curve of $f$ at $x^*$ and hence, *when it is not zero*, points in the direction of the greatest instantaneous increase in $f$.\n",
    "<img src=\"fig/levelcurves.jpg\" style=\"height:300px;\">\n",
    "\n",
    "A point $x=x^*$ at which the first derivative or gradient is zero is called a ***stationary point***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Characterization of Local Optima\n",
    "\n",
    "A local optima must be a stationary point, but *a stationary point need not be a local optima*!\n",
    "<img src=\"fig/stationary.jpg\" style=\"height:200px;\">\n",
    "\n",
    "To check that a stationary point is a local max (or local min), we must check that the function is ***concave*** (or ***convex***) at the point.\n",
    "\n",
    "Recall, that for a twice differentiable function  $f: \\mathbb{R} \\to \\mathbb{R}$, $f$ is concave at $x=x^*$ if the second derivative of $f$ is negative; $f$ is convex at $x=x^*$ if the second derivative of $f$ is positive. For a multivariate twice differentiable function $f: \\mathbb{R}^D \\to \\mathbb{R}$, $f$ is concave at $x=x^*$ if the Hessian matrix is negative semi-definite; $f$ is convex at $x=x^*$ if the Hessian is positive semi-definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Characterization of Global Optima\n",
    "\n",
    "For an arbitrary function, we cannot generally determine if a local optimal is a global one! In certain very restricted cases, we can deduce if a local optimal is global:\n",
    "\n",
    "\n",
    "**Theorem:** If a continuous function $f$ is convex (or resp. concave) on its domain then every local min (or resp. max) is a global min (or resp. max).\n",
    "\n",
    "<img src=\"fig/optima.jpg\" style=\"height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unconstrained Optimization\n",
    "\n",
    "Analytically solving an optimization problem without constraints on the domain of the function,\n",
    "$$\n",
    "x_{\\max} = \\underset{x}{\\mathrm{argmax}}\\; f(x)\n",
    "$$\n",
    "involves:\n",
    "1. find the expression for $\\nabla_x f(x)$.\n",
    "2. find the stationary points for $\\nabla_x f(x)$. That is, solve the equation $\\nabla_x f(x)=0$ for $x$.\n",
    "3. determine local optima. That is, check the concavity of $f$ at the stationary points.\n",
    "4. determine global optima. That is, check if local optima can be characterized as global optima (e.g. check that $f$ is convex everywhere on its domain)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Example: Poisson Distribution\n",
    "\n",
    "Suppose that $Y_n \\overset{\\text{iid}}{\\sim} Poi(\\lambda)$, where $\\lambda > 0$. The likelihood for $N$ observations $y_1, \\ldots, y_N$ is\n",
    "$$\n",
    "\\mathcal{L}(\\lambda) = \\prod_{n=1}^N \\frac{e^{-\\lambda}\\lambda^{y_n}}{y_n!} = e^{-N\\lambda}\\lambda^{\\sum_{n=1}^N y_n} \\frac{1}{y_n!}.\n",
    "$$\n",
    "The log likelihood is \n",
    "$$\n",
    "\\ell(\\lambda) = -N\\lambda + \\log(\\lambda) \\sum_{n=1}^N y_n - \\sum_{n=1}^N \\log(y_n!).\n",
    "$$\n",
    "\n",
    "The first derivative with respect to $\\lambda$ is\n",
    "$$\n",
    "\\frac{d\\ell}{d\\lambda} = -N + \\lambda^{-1}\\sum_{n=1}^N y_n.\n",
    "$$\n",
    "\n",
    "Next, we find the stationary points of the first derivative by setting it equal to zero and solving for $\\lambda$:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{d\\ell}{d\\lambda} &= -N + \\lambda^{-1}\\sum_{n=1}^N y_n = 0\\\\\n",
    "\\lambda &= \\frac{1}{N}\\sum_n y_n\n",
    "\\end{aligned}\n",
    "\n",
    "We see that the first derivative has a unique stationary point at $\\lambda = \\frac{1}{N}\\sum_n y_n$. Taking the second derivative, we get\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{d^2\\ell}{d\\lambda^2}= -\\lambda^{-2} \\sum_{n=1}^N y_n.\n",
    "\\end{aligned}\n",
    "\n",
    "We see that the second derivative is negative for every value of $\\lambda$, hence $\\ell(\\lambda)$ is a concave function. Thus, $\\ell(\\lambda)$ has a global maximum at the stationary point $\\lambda = \\frac{1}{N}\\sum_n y_n$. That is\n",
    "$$\n",
    "\\lambda_{\\text{MLE}} = \\frac{1}{N}\\sum_n y_n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: (Univariate) Gaussian Distribution\n",
    "### Likelihood and log-likelihood\n",
    "\n",
    "Suppose that $Y_n \\overset{\\text{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2)$, where $\\sigma > 0$. Let $\\theta$ denote the set of parameters $(\\mu, \\sigma)$. The likelihood for $N$ observations $y_1, \\ldots, y_N$ is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\prod_{n=1}^N \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\mathrm{exp} \\left\\{ -\\frac{(y_n-\\mu)^2}{2\\sigma^2}\\right\\} =  \\frac{1}{(2\\pi \\sigma^2)^{N/2}} \\mathrm{exp} \\left\\{ -\\frac{\\sum_{n=1}^N(y_n-\\mu)^2}{2\\sigma^2}\\right\\}.\n",
    "$$\n",
    "The log likelihood is \n",
    "$$\n",
    "\\ell(\\theta) = -\\frac{N}{2}\\log 2\\pi - N\\log\\sigma - \\frac{(y_n-\\mu)^2}{2\\sigma^2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: (Univariate) Gaussian Distribution\n",
    "### Gradient of log-likelihood\n",
    "\n",
    "The gradient of $\\ell$ with respect to $\\theta$ is the vector $\\nabla_\\theta \\ell(\\theta) = \\left[\\frac{\\partial\\ell}{\\partial \\mu}, \\frac{\\partial \\ell}{\\partial \\sigma} \\right]$, where the partial derivatives are given by:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial\\ell}{\\partial \\mu} &= \\frac{1}{\\sigma^2} \\sum_{n=1}^N(y_n - \\mu)\\\\\n",
    "\\frac{\\partial\\ell}{\\partial \\sigma} &= -\\frac{N}{\\sigma} + \\sigma^{-3}\\sum_{n=1}^N (y_n - \\mu)^2\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: (Univariate) Gaussian Distribution\n",
    "### Stationary points of the gradient\n",
    "The stationary points of the gradients are solutions to the following system of equations:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{\\partial\\ell}{\\partial \\mu} = \\frac{1}{\\sigma^2} \\sum_{n=1}^N(y_n - \\mu) = 0 &\\\\\n",
    "\\frac{\\partial\\ell}{\\partial \\sigma} = -\\frac{N}{\\sigma} + \\sigma^{-3}\\sum_{n=1}^N (y_n - \\mu)^2 = 0&\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Solving this system, we get a *unique* solution at:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\mu = \\frac{1}{N} \\sum_{n=1}^Ny_n &\\\\\n",
    "\\sigma = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - \\mu)^2}&\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: (Univariate) Gaussian Distribution\n",
    "### Characterize local and global optima\n",
    "\n",
    "The log-likelihood in this case is concave -- the Hessian will be negative semi-definite for $\\mu$ and $\\sigma>0$. Thus, the log-likelihood is globally maximized at:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\mu_{\\text{MLE}} = \\frac{1}{N} \\sum_{n=1}^Ny_n &\\\\\n",
    "\\sigma_{\\text{MLE}} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - \\mu)^2}&\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "*Check for yourself:* write out the matrix of second order parial derivatives of the log-likelihood and check that all the upper-left submatrices have negative determinants.\n",
    "\n",
    "***Note:*** If the objective is not concave, then there is no guarantee that the stationary points will be global maxima!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Example: (Multivariate) Gaussian Distribution\n",
    "\n",
    "Suppose that $Y_n \\overset{\\text{iid}}{\\sim} \\mathcal{N}(\\mu, \\Sigma)$, where $\\Sigma \\in \\mathbb{R}^{D\\times D}$ is a permissible covariance matrix. Let $\\theta$ denote the set of parameters $(\\mu, \\Sigma)$. The likelihood for $N$ observations is\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\prod_{n=1}^N \\frac{1}{(2\\pi)^{D/2}|\\Sigma|^{1/2}} \\mathrm{exp}\\left\\{ -\\frac{1}{2} (y_n - \\mu)^\\top \\Sigma^{-1} (y_n - \\mu)\\right\\} = \\frac{1}{(2\\pi)^{ND/2}|\\Sigma|^{N/2}} \\mathrm{exp}\\left\\{ -\\frac{1}{2} \\sum_{n=1}^N(y_n - \\mu)^\\top \\Sigma^{-1} (y_n - \\mu)\\right\\}.\n",
    "$$\n",
    "\n",
    "The log likelihood is \n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = -\\frac{DN}{2}\\log 2\\pi + \\frac{N}{2}\\log|\\Sigma^{-1}| - \\frac{1}{2} \\sum_{n=1}^N(y_n - \\mu)^\\top \\Sigma^{-1} (y_n - \\mu).\n",
    "$$\n",
    "\n",
    "In deriving the expression for the log likelihood, we used the fact that $\\frac{1}{|\\Sigma|} = |\\Sigma^{-1}|$.\n",
    "\n",
    "The partial derivatives of the log-likelihood are given by:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial\\ell}{\\partial \\mu} &= \\sum_{n=1}^N \\Sigma^{-1}(y_n - \\mu)\\\\\n",
    "\\frac{\\partial\\ell}{\\partial \\Sigma^{-1}} &= \\frac{1}{2}\\sum_{n=1}^N(y_n-\\mu)(y_n-\\mu)^\\top + \\frac{N}{2} \\Sigma\n",
    "\\end{aligned}\n",
    "\n",
    "Note that we took the derivative with respect to the precision matrix $\\Sigma^{-1}$ rather than $\\Sigma$ in the above. Doing simplifies the algebra we need to do to solve for the stationary points. Note also that in order to compute $\\frac{\\partial\\ell}{\\partial \\Sigma^{-1}}$ we made use of two matrix derivative facts:\n",
    "1. $\\frac{\\partial}{\\partial X} a^\\top Xa = aa^\\top$\n",
    "2. $\\frac{\\partial}{\\partial X} \\log|X = X^{-T}$\n",
    "\n",
    "and the fact that $\\Sigma^T = \\Sigma$.\n",
    "\n",
    "Solving for where both partial derivative are zero gives us the unique stationary point\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\mu = \\overline{y}&\\\\\n",
    "\\Sigma = \\frac{1}{N}\\sum_{n=1}^N(y_n - \\overline{y})(y_n - \\overline{y})^\\top&\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "You can again check that the Hessian of the log-likelihood is negative semi-definite for all $\\mu$ and all allowable $\\Sigma$ (this is much more involved than in the case of the univariate Gaussian). Hence we can conclude that the log-likelihood is maximized at the stationary point, i.e.\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\mu_{\\text{MLE}} = \\overline{y}&\\\\\n",
    "\\Sigma_{\\text{MLE}} = \\frac{1}{N}\\sum_{n=1}^N(y_n - \\overline{y})(y_n - \\overline{y})^\\top&\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constrained Optimization\n",
    "\n",
    "Many times, we are constrained by the application to only consider certain types of values of the input $x$. Suppose that the ***constraints*** on $x$ are given by the equation $g(x) = 0$. The set of values of $x$ that satisfy the equation are called ***feasible***.\n",
    "\n",
    "We recall a useful theorem from calculus:\n",
    "\n",
    "**Theorem:**  For a differentiable function $f: \\mathbb{R}^D \\to \\mathbb{R}$, the local optima of $f$ constrained by $g(x)=0$ occur at points where the following hold for some $\\lambda \\in\\mathbb{R}$, \n",
    "$$g(x) = 0, \\quad \\nabla_xf(x) = \\lambda \\nabla_x g(x).$$\n",
    "\n",
    "The theorem says that the local optima of $f$ satisfying $g(x)=0$ are where the gradients of $f$ and $g$ are parallel.\n",
    "<img src=\"fig/lagrange.jpg\" style=\"height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Constrained Optimization via Lagrange Multipliers\n",
    "Unpacking the theorem, we get that solving an optimization problem within the ***feasible region*** of the function, i.e. \n",
    "$$\n",
    "\\underset{x}{\\mathrm{max}}\\; f(x),\\quad g(x) = 0\n",
    "$$\n",
    "involves:\n",
    "1. finding the stationary points of the augmented objective $J(x) = f(x) - \\lambda g(x)$, with respect to $x$ and $\\lambda$.\n",
    "2. determine global optima. Determine if any of the stationary points maximizes $f$.\n",
    "\n",
    "The augmented objective $J$ is called the ***Lagrangian*** of the constrained optimization problem and $\\lambda$ is called the ***Lagrange multiplier***.\n",
    "\n",
    "**Note:** Constrained optimization with inequality constraints can similarly be formulated in terms of finding stationary points of an augmented objective like the Lagrangian; this follows from the ***Karush–Kuhn–Tucker theorem***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Binomial Distribution\n",
    "### Likelihood and Log-Likelihood\n",
    "\n",
    "Suppose that $Y \\sim Bin(N, \\theta)$. To make the connection with constrained optimization (and to motivate the multinomial case), let's write $\\theta$ as a vector $[\\theta_0, \\theta_1]$, where $\\theta_1$ is the probability of a head and $\\theta_0 + \\theta_1 = 1$.\n",
    "\n",
    "The likelihood for a single observations is\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\frac{N!}{y!(N-y)!} \\theta_1^{y}\\theta_0^{N-y}.\n",
    "$$\n",
    "The log likelihood is \n",
    "\n",
    "$$\n",
    "\\ell(\\theta) = \\log (N!) - \\log(y!) - \\log(N-y)! + y\\log \\theta_1 + (N-y) \\log \\theta_0.\n",
    "$$\n",
    "\n",
    "We are interested in solving the following constrained optimization problem:\n",
    "\n",
    "$$\n",
    "\\mathrm{max}\\;\\ell(\\theta),\\quad\\theta_0 + \\theta_1 = 1\n",
    "$$\n",
    "\n",
    "whose Lagrangian is give by:\n",
    "\n",
    "$$\n",
    "J(\\theta, \\lambda) = \\ell(\\theta) - \\lambda(\\theta_0 + \\theta_1 - 1).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Binomial Distribution\n",
    "### Gradient of log-likelihood\n",
    "\n",
    "The gradient of the Lagrangian $J$ with respect to $(\\theta, \\lambda)$ is the vector $\\nabla_{(\\theta, \\lambda)} J(\\theta, \\lambda) = \\left[\\frac{\\partial\\ell}{\\partial \\theta_0}, \\frac{\\partial \\ell}{\\partial \\theta_1}, \\frac{\\partial \\ell}{\\partial \\lambda} \\right]$, where the partial derivatives are given by:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial J}{\\partial \\theta_0} &= \\frac{(N-y)}{\\theta_0} - \\lambda\\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_1} &= \\frac{y}{\\theta_1} - \\lambda\\\\\n",
    "\\frac{\\partial J}{\\partial \\lambda} &= \\theta_0 + \\theta_1 - 1\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Binomial Distribution\n",
    "### Stationary points of the Lagrangian\n",
    "\n",
    "The stationary points of the Lagrangian are solutions to the following system of equations:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\frac{\\partial J}{\\partial \\theta_0} = \\frac{(N-y)}{\\theta_0} - \\lambda=0\\\\\n",
    "\\frac{\\partial J}{\\partial \\theta_1} = \\frac{y}{\\theta_1} - \\lambda=0\\\\\n",
    "\\frac{\\partial J}{\\partial \\lambda} = \\theta_0 + \\theta_1 - 1=0\n",
    "\\end{cases}\n",
    "$$\n",
    "Solving this system, we get a *unique* solution at:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\theta_0 = \\frac{N-y}{\\lambda}&\\\\\n",
    "\\theta_1 = \\frac{y}{\\lambda}&\\\\\n",
    "\\theta_0 + \\theta_1 = 1&\n",
    "\\end{cases}\n",
    "$$\n",
    "In other words, $\\lambda=N$ and $\\theta_1 = \\frac{y}{N}$ and $\\theta_0 = \\frac{N-y}{N}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Binomial Distribution\n",
    "### Characterize global optima\n",
    "\n",
    "Since the log-likelihood $\\ell(\\theta)$ is concave and the constraint $\\theta_0 + \\theta_1 = 1$ is affine (linear up to a constant), we know then that $\\ell(\\theta)$ is maximized on the line at the stationary point. Hence,\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\theta_0^{\\text{MLE}} = \\frac{N-y}{N}&\\\\\n",
    "\\theta_1^{\\text{MLE}} = \\frac{y}{N}&\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "***Note:*** If the objective function we are maximizing is not concave and the equality constraint is not affine, then we have no guarantee that the stationary points of the langrangian either locally or globally optimizes our objective!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Is a Good Estimator?\n",
    "\n",
    "We see that if we assume a binomial model, $Bin(N, \\theta)$, for the number of heads in $N$ trials, then the fraction $\\frac{H}{N}$ is the maximum likelihood estimate of $\\theta$.\n",
    "\n",
    "**Question 1:** Is the MLE a good estimator of $\\theta$?\n",
    "\n",
    "**Question 2:** Is this the \"best\" way to estimate the $\\theta$? For example, is the quantity\n",
    "\n",
    "$$\n",
    "\\frac{\\text{Number of Heads} + 1}{\\text{Total Number of Tosses} + 2} = \\frac{H + 1}{N + 2}\n",
    "$$\n",
    "\n",
    "an equally valid or better estimate of $\\theta$?\n",
    "\n",
    "These questions depend on our list of desiderata for our estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Properties of MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Desiderata of Estimators\n",
    "\n",
    "Let $\\widehat{\\theta}$ be an estimator of the parameter $\\theta$ of a statistical model. We ideally want:\n",
    "- **(Consistency)** when the sample size $N$ increases, in the limit, $\\widehat{\\theta}$ approaches the true value of $\\theta$. \n",
    "\n",
    "  More formally, let $\\{p_\\theta; \\theta\\in \\Theta \\}$ be a family of candidate distributions and $X^\\theta$ be an infinite sample from $p_\\theta$. Define $\\widehat{g}_N(X^\\theta)$ to be an estimator for some parameter $g(\\theta)$ that is based on the first $N$ samples. Then we say that the sequence of estimators $\\{ \\widehat{g}_N(X^\\theta)\\}$ is (weakly) consistent if $\\lim_{N\\to \\infty} \\widehat{g}_N(X^\\theta) = g(\\theta)$ in probability for all $\\theta\\in \\Theta$.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Desiderata of Estimators\n",
    "\n",
    "- **(Unbiasedness)** on average, over all possible sets of observations from the distribution, the estimator nails the true value of $\\theta$.\n",
    "\n",
    "  More formally, we want $\\mathbb{E}_{X^\\theta} \\widehat{\\theta}(X^\\theta) = \\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Desiderata of Estimators\n",
    "\n",
    "- **(Minimum Variance)** Note that since our estimator $\\widehat{\\theta}$ depends on the random sample $X^\\theta$, it follows that $\\widehat{\\theta}$ also a random variable. The distribution of $\\widehat{\\theta}$ is called the ***sampling distribution***. Given that our estimator is unbiased, we want it to have minimum variance with respect to the sampling distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of MLE\n",
    "\n",
    "**Assumptions:** In order for nice properties of the MLE to hold, we need to make some assumptions, including \n",
    "\n",
    "(A) the model is **well-specified** -- the observed data is drawn from the same model class as the model being fitted; \n",
    "\n",
    "(B) the estimation problem is **well-posed** -- there are not two different set of parameters that generate the same data.\n",
    "\n",
    "With these assumptions, we have that:\n",
    "\n",
    "1. **(Consistency)** The MLE of *iid* observations is consistent. The asymptotic sampling distribution of the MLE is a Gaussian.\n",
    "2. **(Unbiasedness)** The MLE can be biased.\n",
    "3. **(Minimum Variance)** The MLE is not the estimator with the lowest variance. \n",
    "\n",
    "*Asympotically*, however, the MLE is unbiased and has the lowest variance (for unbiased estimators). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Example: The MLE Can Be Biased\n",
    "Suppose that $Y_n \\overset{\\text{iid}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2)$, where $\\sigma > 0$. Recall that the MLE of $\\sigma$ is\n",
    "$$\n",
    "\\sigma_{\\text{MLE}} = \\sqrt{\\frac{1}{N}(y_n - \\overline{y})^2}.\n",
    "$$\n",
    "We compute the bias of $\\sigma_{\\text{MLE}}$, for a random sample $Y^\\theta = \\{Y_1, \\ldots, Y_N\\}$ from $\\mathcal{N}(\\mu, \\sigma^2)$:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_{Y^\\theta}[\\sigma^2_{\\text{MLE}}]&= \\mathbb{E}_{Y^\\theta}\\left[ \\frac{1}{N}\\sum_{n=1}^N(Y_n - \\overline{Y})^2\\right]\\\\\n",
    "&= \\mathbb{E}_{Y^\\theta}\\left[ \\frac{1}{N}\\sum_{n=1}^N(Y^2_n  - 2 Y_n \\overline{Y} + \\overline{Y}^2)\\right]\\\\\n",
    "&= \\mathbb{E}_{Y^\\theta}\\left[ \\frac{1}{N}\\sum_{n=1}^NY^2_n  - 2\\frac{1}{N}\\sum_{n=1}^N Y_n \\overline{Y} + \\frac{1}{N}\\sum_{n=1}^N\\overline{Y}^2\\right]\\\\\n",
    "&= \\mathbb{E}_{Y^\\theta}\\left[ \\frac{1}{N}\\sum_{n=1}^NY^2_n  - 2\\overline{Y}^2 + \\overline{U}^2\\right]\\\\\n",
    "&= \\frac{1}{N}\\sum_{n=1}^N\\mathbb{E}_{Y^\\theta}\\left[ Y^2_n\\right] - \\mathbb{E}_{Y^\\theta}\\left[ \\overline{Y}^2\\right]\n",
    "\\end{aligned}\n",
    "Note that $\\sigma^2 = \\mathbb{E}[Y^2] - \\mathbb{E}[Y]^2$, and assume the fact that the MLE of $\\mu$ is unbiased (i.e. $\\mathbb{E}_{Y^\\theta}[\\overline{Y}^2] = \\mu$) with variance $\\sigma_{\\overline{Y}}^2 = \\mathbb{E}[\\overline{Y}^2] - \\mathbb{E}[\\overline{Y}]^2$. We can then rewrite the above as\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_{Y^\\theta}[\\sigma^2_{\\text{MLE}}] &= \\frac{1}{N}\\sum_{n=1}^N\\mathbb{E}_{Y^\\theta}\\left[ Y^2_n\\right] - \\mathbb{E}_{Y^\\theta}\\left[ \\overline{Y}^2\\right]\\\\\n",
    "&= (\\sigma^2 + \\mu^2) - (\\sigma_{\\overline{Y}}^2 + \\mu^2)\\\\\n",
    "&= (\\sigma^2 + \\mu^2) - \\left(\\mathrm{Var}\\left[\\frac{1}{N} \\sum_{n=1}^N Y_n\\right] + \\mu^2\\right)\\\\\n",
    "&= (\\sigma^2 + \\mu^2) - \\left(\\frac{1}{N^2} \\sum_{n=1}^N \\mathrm{Var}\\left[Y_n\\right] + \\mu^2\\right)\\\\\n",
    "&= (\\sigma^2 + \\mu^2) - \\left(\\frac{1}{N}\\sigma + \\mu^2\\right)\\\\\n",
    "&= \\frac{N-1}{N}\\sigma^2\n",
    "\\end{aligned}\n",
    "Hence, $\\mathbb{E}_{Y^\\theta}[\\sigma^2_{\\text{MLE}}] \\neq \\sigma^2$ and so $\\mathbb{E}_{Y^\\theta}[\\sigma^2_{\\text{MLE}}]$ is biased!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Confidence Intervals\n",
    "\n",
    "Since the MLE depends on the sample, it is important to quantify how certain we are about the estimate. \n",
    "\n",
    "***Confidence intervals*** of estimates $\\theta_{\\text{MLE}}$ are ways of summarizing the sampling distribution by describing it's coverage. Specifically, a 95% confidence interval for $\\theta$ is a ***random interval*** $\\left(L_{\\theta_{\\text{MLE}}}, U_{\\theta_{\\text{MLE}}}\\right)$, where $L$ and $U$ are bounds constructed from the estimate $\\theta_{\\text{MLE}}$, that contains the fixed true parameter $\\theta$ with 95% probability.\n",
    "\n",
    "Let $\\delta = \\theta_{\\text{MLE}} - \\theta$ be the distribution of the error of the estimator $\\theta_{\\text{MLE}}$, then the following is a confidence interval for $\\theta$:\n",
    "\n",
    "$$\n",
    "\\left[\\widehat{\\theta} - \\delta_{0.25}, \\widehat{\\theta} + \\delta_{0.975}\\right]\n",
    "$$\n",
    "\n",
    "where $\\delta_{0.25}, \\delta_{0.975}$ are the 2.5% and 97.5% thresholds of $\\delta$ respectively.\n",
    "\n",
    "We can take advantage of the asymptotic normality of the MLE and approximate the distribution of $\\delta$ as a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretation of Confidence Intervals\n",
    "\n",
    "It is very easy to misinterpret confidence intervals! \n",
    "\n",
    "**A Simplified Rule:** When in doubt, treat the confidence interval just as an **indication of the precision of the measurement.** \n",
    "\n",
    "If you estimated some quantity in a study with a confidence interval of $[17 - 6, 17 + 6]$ and someone else estimated it with a confidence interval of $[23 - 5, 23 + 5]$, then there is little reason to think that the two studies are inconsistent. \n",
    "\n",
    "On the other hand, if your estimate gives $[17 - 2, 17 + 2]$ and the other estimate is $[23 - 1, 23 + 1]$, then there is evidence that these studies differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bootstrap Confidence Intervals\n",
    "\n",
    "In practice, we may not know how to approximate the sampling distribution of $\\theta_{\\text{MLE}}$. We can approximate the sampling distribution by ***bootstraping***, i.e. we simulate samples $X^{\\theta}$ with size $N$ from $p_{\\theta}$ by sampling observations with size $N$ from the observed data (also with size $N$). \n",
    "\n",
    "We denote MLE obtained on a bootstrap sample by $\\theta^{\\text{bootstrap}}_{\\text{MLE}}$. When $N$ is sufficiently large, $\\theta^{\\text{bootstrap}}_{\\text{MLE}}$ approximates the distribution of $\\theta_{\\text{MLE}}$.\n",
    "\n",
    "Thus, we can approximate the 95% confidence interval of $\\theta$ using $\\theta^{\\text{bootstrap}}_{\\text{MLE}}$."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
