{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #7: Markov Chain Monte Carlo\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Gibbs Sampler for a Discrete Distribution\n",
    "2. Definition and Properties of Markov Chains\n",
    "3. Markov Chain Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "Recall that the Gibbs sampler was a sampling technique that we introduced along with rejection sampling and inverse CDF sampling. \n",
    "\n",
    "We applied this sampler to sample from the posterior of a semi-conjugate Bayesian model (normal likelihood and Inverse-Gamma prior on the mean parameter). Using rejection sampling on this posterior would have been quite difficult (recall your experiments tuning rejection sampling for the non-conjugate Bayesain model for birth weights in *In-Class Exercise 09.17*).\n",
    "\n",
    "But unlike in the case of rejection sampling and inverse CDF sampling, we never proved the correctness of this sampler!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gibbs Sampler for a Discrete Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gibbs Sampler for a Bivariate Discrete Distribution\n",
    "\n",
    "Suppose we have two independent random variables $X\\sim Ber(0.2)$ and $Y\\sim Ber(0.6)$. Their joint distribution is a categorical distribution:\n",
    "\n",
    "$$\n",
    "p(X, Y) = [0.12\\;\\; 0.48\\;\\; 0.08\\;\\; 0.32]\n",
    "$$ \n",
    "\n",
    "over the set of possible outcomes $(X=1, Y=1), (X=0, Y=1), (X=1, Y=0), (X=1, Y=1)$.\n",
    "\n",
    "A Gibbs sampler for $p(X, Y)$ will start with a sample $(X=X_n, Y=Y_n)$ and then generate a sample $(X=X_{n+1}, Y=Y_{n+1})$ by\n",
    "1. sampling $X_{n+1}$ from $$p(X|Y = Y_{n})$$\n",
    "2. sampling $Y_{n+1}$ from $$p(Y|X = X_{n+1})$$\n",
    "\n",
    "We want to compute what is the distribution of the generated sample $(X=X_{n+1}, Y=Y_{n+1})$ given $(X=X_n, Y=Y_n)$, i.e. we want $p(X=X_{n+1}, Y=Y_{n+1}| X=X_n, Y=Y_n)$, but there are $4^2$ number of these probabilities! How do we succinctly represent them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gibbs Sampler as Transition Matrix and State Diagram\n",
    "\n",
    "We can represent the $p(X=X_{n+1}, Y=Y_{n+1}| X=X_n, Y=Y_n)$ as a $4\\times 4$ matrix, $T$, where the $i, j$-th entry is the probability of starting with sample $i$ and generating sample $j$. \n",
    "\n",
    "Alternatively, we can visualize how the Gibbs sampler moves around in the sample space $(X, Y)$ with a diagram.\n",
    "\n",
    "<img src=\"fig/gibbs.jpg\" style=\"height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limiting Distribution\n",
    "\n",
    "We see that computing the probability of the next sample given the current sample $(X=1, Y=1)$\n",
    "\n",
    "$$\n",
    "p(X_{n+1}, Y_{n+1} | X_n=1, Y_n=1)\n",
    "$$\n",
    "\n",
    "is equivalent to multiplying the vector $[1\\;\\; 0\\;\\; 0\\;\\; 0]$ with the matrix $T$. **Can you see why?**\n",
    "\n",
    "When we do, we get the distribution $[0.12\\;\\; 0.48\\;\\; 0.08\\;\\; 0.32]$ over the next sample. But this distribution looks just like the joint distribution $p(X, Y)$!\n",
    "\n",
    "This means that if we start at $(X=1, Y=1)$, the next sample the Gibbs sampler returns will be from the joint distribution. \n",
    "\n",
    "In fact, you can start with any point in the samples space $(X, Y)$ or any distribution over the sample space, the next sample the Gibbs sampler returns will be from the joint distribution. I.e. any vector times $T$ will return $[0.12\\;\\; 0.48\\;\\; 0.08\\;\\; 0.32]$.\n",
    "\n",
    "This proves the correctness of the Gibbs sampler for $p(X, Y)$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Definition and Properties of Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov Chains in Discrete and Continuous Spaces\n",
    "\n",
    "A ***discrete-time stochastic process*** is set of random variables $\\{X_0, X_1, \\ldots \\}$, where each random variable takes value in $\\mathcal{S}$. The set $\\mathcal{S}$ is called the ***state space*** and can be continuous or finite. \n",
    "\n",
    "A stochastic process satisfies the ***Markov property*** if $X_n$ depends only on $X_{n-1}$ (i.e. $X_n$ is independent of $X_1, \\ldots, X_{n-2}$). A stochastic process that satisfies the Markov property is called a ***Markov chain***. \n",
    "\n",
    "We will assume that $p(X_n | X_{n-1})$ is the same for all $n$.\n",
    "\n",
    "**Exercise:** Give an example of a stochastic process that is not a Markov chain. Given an example of a stochastic process that is a Markov chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transition Matrices and Kernels\n",
    "The Markov property ensure that we can describe the dynamics of the entire chain by describing how the chain ***transitions*** from state $i$ to state $j$. **Why?**\n",
    "\n",
    "If the state space is finite, then we can represent the transition from $X_{n-1}$ to $X_{n}$ as a ***transition matrix*** $T$, where $T_{ij}$ is the probability of the chain transitioning from state $i$ to $j$:\n",
    "$$\n",
    "T_{ij} = \\mathbb{P}[X_n = j | X_{n-1} = i].\n",
    "$$\n",
    "\n",
    "The transition matrix can be represented visually as a ***finite state diagram***.\n",
    "\n",
    "If the state space is continuous, then we can represent the transition from $X_{n-1}$ to $X_{n}$ as ***transition kernel pdf***,\n",
    "$\n",
    "T(x, x')\n",
    "$,\n",
    "representing the likelihood of transitioning from state $X_{n-1}=x$ to state $X_{n} = x'$. The probability of transitioning into a region $A \\subset S$ from state $x$ is given by\n",
    "\n",
    "$$\n",
    "\\mathbb{P}[X_n \\in A | X_{n-1} = x] = \\int_{A} T(x, y) dy,\n",
    "$$\n",
    "\n",
    "such that $\\int_{\\mathcal{S}} T(x, y) dy = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chapman-Kolmogorov Equations: Dynamics as Matrix Multiplication\n",
    "If the state space is finite, the probability of the $n=2$ state, given the initial $n=0$ state. can be computed by the ***Chapman-Kolmogorov equation***:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}[X_2 = j | X_{0} = i] = \\sum_{k\\in \\mathcal{S}} \\mathbb{P}[X_1 = k | X_{0} = i]\\mathbb{P}[X_{2} = j|X_{1}=k] = \\sum_{k\\in \\mathcal{S}}T_{ik}T_{kj}\n",
    "$$\n",
    "\n",
    "We recognize $\\sum_{k\\in \\mathcal{S}}T_{ik}T_{kj}$ as the $ij$-the entry in the matrix $TT$. Thus, the Chapman-Kolmogorov equation gives us that the matrix $T^{(n)}$ for a $n$-step transition is \n",
    "$$\n",
    "T^{(n)} = \\underbrace{T\\ldots T}_{\\text{$n$ times}}\n",
    "$$\n",
    "\n",
    "In particular, when we have the initial distribution $\\pi^{(0)}$ over states, then the unconditional distribution $\\pi^{(1)}$ over the next state is given by:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}[S_1 = i] = \\sum_{k\\in \\mathcal{S}} \\mathbb{P}[X_1 = i | X_0 = k]\\mathbb{P}[X_0 = k]\n",
    "$$\n",
    "\n",
    "That is, $\\pi^{(1)} = \\pi^{(0)} T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Smart Phone Market Model\n",
    "\n",
    "Consider a simple model of the dynamics of the smart phone market, where we model the customer loyalty as follows:\n",
    "\n",
    "<img src=\"fig/apple.jpg\" style=\"height:200px;\">\n",
    "\n",
    "The transition matrix for the Markov chain is:\n",
    "$$\n",
    "T = \\left(\\begin{array}{cc}0.8 & 0.2\\\\ 0.4 & 0.6 \\end{array} \\right)\n",
    "$$\n",
    "\n",
    "Say that the market is initially $\\pi^{(0)} = [0.7\\; 0.3]$, i.e. 70% Apple. What is the market distribution in the long term?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Smart Phone Market Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.33333333]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transition matrix\n",
    "T = np.array([[0.8, 0.2],\n",
    "              [0.4, 0.6]])\n",
    "#initial distribution\n",
    "pi_0 = np.array([0.7, 0.3]).reshape((1, -1))\n",
    "#time\n",
    "N = 500\n",
    "\n",
    "pi_0.dot(np.linalg.matrix_power(T, N))\n",
    "#try different values of N and different inital distributions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chapman-Kolmogorov Equations: Continuous State Space\n",
    "\n",
    "If the state space is continuous, the likelihood of the $n=2$ state, given the initial $n=0$ state, can be computed by the ***Chapman-Kolmogorov equation***:\n",
    "\n",
    "$$\n",
    "T^{(2)}(x, x') = \\int_{\\mathcal{S}}T(x, y)\\, T(y, x') dy.\n",
    "$$\n",
    "\n",
    "In particular, when we have the initial distribution $\\pi^{(0)}(x)$ over states, then the unconditional distribution $\\pi^{(1)}(x)$ over the next state is given by:\n",
    "\n",
    "$$\n",
    "\\pi^{(1)}(x) = \\int_{\\mathcal{S}} T(y, x)\\,\\pi^{(0)}(y)dy.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of Markov Chains: Irreducibility\n",
    "\n",
    "A Markov chain is called ***irreducible*** if every state can be reached from every other state in finite time.\n",
    "\n",
    "<img src=\"fig/irred.jpg\" style=\"height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of Markov Chains: Aperiodicity\n",
    "\n",
    "A state $s\\in \\mathcal{S}$ is has period $t$ if one can only return to $s$ in multiples of $t$ steps.\n",
    "\n",
    "A Markov chain is called ***aperiodic*** if the period of each state is 1.\n",
    "\n",
    "<img src=\"fig/periodicity.jpg\" style=\"height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of Markov Chains: Stationary Distributions\n",
    "\n",
    "A distribution $\\pi$ over the finite state space $\\mathcal{S}$ is a ***stationary distribution*** of the Markov Chain with transition matrix $T$ if\n",
    "$$\n",
    "\\pi = \\pi T,\n",
    "$$\n",
    "i.e. performing the transition matrix doesn't change the distribution.\n",
    "\n",
    "The equivalent condition for continuous state space $\\mathcal{S}$ is:\n",
    "$$\n",
    "\\pi(x) = \\int_{\\mathcal{S}} T(y, x)\\,\\pi(y)dy.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of Markov Chains: Limiting Distributions\n",
    "\n",
    "We are often interested in what happens to a distribution after many transitions,\n",
    "\n",
    "$$\n",
    "\\pi^{(n)} = \\pi^{(0)}T^{(n)},\\; \\text{ or }\\; \\pi^{(n)}(x) = \\int_{\\mathcal{S}} T^{(n)}(y, x)\\,\\pi^{(0)}(y)dy\n",
    "$$\n",
    "\n",
    "If $\\pi^{(\\infty)} = \\underset{n\\to \\infty}{\\lim}\\pi^{(n)}$ exists (with some caveats in the continuous state case), we call it the ***limiting distribution*** of the Markov chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fundamental Theorm of Markov Chains\n",
    "Now we are ready to relate all these properties of Markov chains in a single theorem:\n",
    "\n",
    "**Fundamental Theorem of Markov Chains:** if a Markov chain is irreducible and aperiodic, then it has a *unique* stationary distribution $\\pi$ and $\\pi^{\\infty} = \\underset{n\\to \\infty}{\\lim}\\pi^{(n)} = \\pi$.\n",
    "\n",
    "In practice, the theorem says you can start with any initial distribution over the state space $\\mathcal{S}$, asymptotically, you will always obtain the distribution $\\pi$.\n",
    "\n",
    "While we unfortunately can't prove the theorem, we can indicate why both conditions are helpful.\n",
    "\n",
    "<img src=\"fig/counter.jpg\" style=\"height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of Markov Chains: Reversibility\n",
    "A Markov chain is called ***reversible*** with respect to a distribution $\\pi$ over a finite state space $\\mathcal{S}$ if the following holds:\n",
    "\n",
    "$$\n",
    "\\pi_i T_{i, j} = \\pi_j T_{j , i}\n",
    "$$\n",
    "\n",
    "\n",
    "For a continuous state space, the condition is:\n",
    "\n",
    "$$\n",
    "\\pi(x)T(x, y) = T(y, x) \\pi(y).\n",
    "$$\n",
    "\n",
    "The condition for reversibility is often called the ***detailed balance*** condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reversibility and Stationary Distributions\n",
    "Using reversibility, we have another way to characterize a stationary distribution.\n",
    "\n",
    "**Theorem:** If a Markov chain, with transition matrix or kernel pdf $T$, is reversible with respect to $\\pi$. Then $\\pi$ is a stationary distribution of the chain.\n",
    "\n",
    "***Proof:*** We will give the proof for the case of a continuous state space $\\mathcal{S}$. \n",
    "Supoose that $\\pi(x)T(x, y) = T(y, x) \\pi(y)$, then\n",
    "\n",
    "$$\n",
    "\\int_{\\mathcal{S}}\\pi(y) T(y, x)dy = \\int_{\\mathcal{S}} \\pi(x) T(x, y)dy  = \\pi(x) \\int_{\\mathcal{S}}T(y, x)dx = \\pi(x) \\cdot 1 = \\pi(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Markov Chain Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov Chain Monte Carlo Samplers\n",
    "\n",
    "Every sampler for a distribution $p(\\theta)$ over the domain $\\Theta$ defines a stochastic process $\\{X_0, X_1, \\ldots, \\}$, where the state space is $\\Theta$.\n",
    "\n",
    "If the sampler defines a Markov chain whose unique stationary and limiting distribution is $p$, we call it a ***Markov Chain Monte Carlo (MCMC)*** sampler.\n",
    "\n",
    "That is, for every MCMC sampler, we have that \n",
    "1. **Stationary:** $pT = p$<br><br>\n",
    "\n",
    "2. **Limiting:** $\\underset{n\\to \\infty}{\\lim} \\pi^{(n)} = p$, for any $\\pi^{(0)}$\n",
    "\n",
    "where $T$ is the transition matrix or kernel pdf defined by the sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Do We Need to Prove to get $pT=p$ and $\\underset{n\\to \\infty}{\\lim} \\pi^{(n)} = p$?\n",
    "\n",
    "1. Prove that the sampler is ***irreducible*** and ***aperiodic***. Then, there is a unique stationary distribution $\\pi$ such that \n",
    "$$\\pi T = \\pi.$$\n",
    "\n",
    "2. Prove that the sampler is ***reversible*** or ***detailed balanced*** with respect to $p$. Then, \n",
    "$$\\pi = p.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gibbs as MCMC\n",
    "\n",
    "We've seen an example where the Gibbs sampler for a discrete target distribution defines a MCMC sampler. \n",
    "\n",
    "But what about Gibbs samplers for a continuous target distribution $p$? Certainly, the samples $X_n$ obtained by the sampler defines a Markov Chain: the distribution over the next sample depends only on the current sample.\n",
    "\n",
    "But, in order to be a MCMC sampler, we need to prove that $p$ is the stationary and limiting distribution of the sampler?\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
