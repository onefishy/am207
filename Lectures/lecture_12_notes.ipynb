{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture #12: Logistic Regression and Gradient Descent\n",
    "## AM 207: Advanced Scientific Computing\n",
    "### Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "### Fall, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\" style=\"height:150px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "### Import basic libraries\n",
    "from autograd import numpy as np\n",
    "from autograd import grad\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "1. Logistic Regression\n",
    "2. Gradient Descent\n",
    "3. Convex Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coin-Toss Revisited: Modeling a Bernoulli Variable with Covariates\n",
    "\n",
    "Let's revisit our model for coin-toss: we'd assumed that the outcomes $Y^{(n)}$ were independently and identically distributed as Bernoulli's, $Y^{(n)} \\sim Ber(\\theta)$. Today, we will re-examine the ***identical*** part of the modeling assumptions. \n",
    "\n",
    "Realistically, the probability of $Y^{(n)} = 1$ depends on variables like force, angle, spin etc. \n",
    "\n",
    "Let $\\mathbf{X}^{(n)} \\in \\mathbb{R}^D$ be $D$ number of such measurements of the $n$-th toss. We model the probability of $Y^{(n)} = 1$ as a function of these ***covariates*** $\\mathbf{X}^{(n)}$:\n",
    "\n",
    "$$\n",
    "Y^{(n)} \\sim Ber\\left(\\mathrm{sigm}\\left(f\\left(\\mathbf{X}^{(n)}; \\mathbf{w}\\right)\\right)\\right)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w}$ are the parameters of $f$ and $\\mathrm{sigm}$ is the sigmoid function $\\mathrm{sigm}(z) = \\frac{1}{1 + e^{-z}}$. \n",
    "\n",
    "**Note:** we need the sigmoid function to transform an arbitrary real number $f\\left(\\mathbf{X}^{(n)}; \\mathbf{w}\\right)$ into a probability (i.e. a number in $[0, 1]$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Logistic Regression Model\n",
    "Given a set of $N$ observations $(\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(N)}, y^{(N)})$. We assume the following model for the data.  \n",
    "\n",
    "$$\n",
    "Y^{(n)} \\sim Ber\\left(\\mathrm{sigm}\\left(f\\left(\\mathbf{X}^{(n)}; \\mathbf{w}\\right)\\right)\\right).\n",
    "$$\n",
    "\n",
    "This is called the ***logistic regression*** model.\n",
    "\n",
    "Fitting this model on the data means ***inferring*** the parameters $\\mathbf{w}$ that best aligns with the observations. \n",
    "\n",
    "Once we have inferred the parameters $\\mathbf{w}$, given a new set of covariates $\\mathbf{x}^{\\text{new}}$, we can **predict** the probability of $\\mathbf{Y}^{\\text{new}} = 1$ by computing \n",
    "\n",
    "$$\\mathrm{sigm}\\left(f\\left(\\mathbf{X}^{(n)}; \\mathbf{w}\\right)\\right).$$\n",
    "\n",
    "For now, we will assume that $f$ is a linear function:\n",
    "\n",
    "$$\n",
    "f\\left(\\mathbf{X}^{(n)}; \\mathbf{w}\\right) = \\mathbf{w}^\\top \\mathbf{X}^{(n)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## The Relationship Between Logistic Regression and Classification\n",
    "\n",
    "### What Is Classification?\n",
    "A regression problem is where we predict a **quantitative** outcome $y$ based on some covariates $x$.\n",
    "\n",
    "   - **Example:** Given the number of room, the number of allowed guests and the neighborhood, predict price of an Airbnb listing.\n",
    "\n",
    "A ***classification*** problem is a **categorical** outcome $y$ based on some covariates $x$.\n",
    "\n",
    "   - **Example:** Given the age, education and income level of a loan applicant, predict whether or not the loan application will be approved.\n",
    "   \n",
    "### How Do We Classify?\n",
    "\n",
    "The data for classification consists of a set of tuples $(\\mathbf{x}^{(n)}, y^{(n)})$, where $\\mathbf{x}^{(n)}\\in \\mathbb{R}^D$ is a set of predictors/covariates (e.g. age, income etc) for the $n$-th observation and $y^{(n)}$ is the binary label (e.g. did the person receive the loan). The set of observations that are labeled $y=0$ are called class 0 and the set labeled $y=1$ are called class 1.\n",
    "\n",
    "The goal is learn a function $f(\\mathbf{x})$ so that given a new set of covariates $\\mathbf{x}$ we can predict the correpsonding label $y$.\n",
    "\n",
    "Suppose our data has a small number of predictors, by visualizing the data we can intuitively check how easy it is to separate the classes. In the following example, we have two covariates. We scatter plot the covariates (pictured as crosses and bars), we then indicate data belonging to class 1 by visualizing them as blue crosses and those belonging to class 0 as red bars.\n",
    "\n",
    "<img src=\"fig/fig0.png\" alt=\"\" style=\"height: 200px;\"/>\n",
    "\n",
    "Ideally, the classes are easily separated by a curve (or surface) in the input space, this curve (or surface) is called the ***decision boundary***.\n",
    "\n",
    "When the decision boundary is linear (i.e. a line or a plane), it is defined by the equation\n",
    "\n",
    "$$\n",
    "\\mathbf{w}^\\top \\mathbf{x} = w_0x_0 + w_1x_1 + \\ldots + w_D x_D = 0\n",
    "$$\n",
    "\n",
    "where $x_0 = 1$. Often we write $b = w_0x_0$ and we call $b$ the ***bias*** term.\n",
    "\n",
    "The vector $\\mathbf{w}$ allow us to gauge the 'distance' of a point from the decision boundary. That is, the magnitude of the inner product $|\\mathbf{w}^\\top \\mathbf{x}|$ gives us the distance between the point $\\mathbf{x}$ and the decision boundary, whereas the sign of $\\mathbf{w}^\\top \\mathbf{x}$ gives us the side of the decision boundary on which the point $\\mathbf{x}$ lies. The proof of this is short and geometric -- you should try to work it out!\n",
    "\n",
    "*(In the picture below, they use $\\mathbf{w}^\\top \\mathbf{x}$ to express $w_1x_1 + \\ldots + w_D x_D + b$, whereas we pretend that $x_0 = 1$ and $b = w_0x_0$. This is just a difference in notation)*\n",
    "<img src=\"fig/fig1.png\" alt=\"\" style=\"height: 200px;\"/>\n",
    "\n",
    "\n",
    "Once we have a decision boundary should we always classify points on one side of the boundary as 1 and points on the other side as 0?\n",
    "\n",
    "<img src=\"fig/fig2.png\" alt=\"\" style=\"height: 200px;\"/>\n",
    "\n",
    "For classes that are not linearly separable (the classes don't fall cleanly along two sides of a linear decision boundary), we want to make \"soft classifications\". That is, rather than declaring $y=1$ when $\\mathbf{w}^\\top \\mathbf{x} > 0$ and $y=0$ when $\\mathbf{w}^\\top \\mathbf{x} <0$, we want to encode uncertainty in our prediction of the label $y$. We want to say the following:\n",
    "\n",
    "> When $\\mathbf{x}$ is very far from the decision boundary, i.e. when $|\\mathbf{w}^\\top \\mathbf{x}|$ is very large, and when $\\mathbf{w}^\\top \\mathbf{x}>0$ then we want to say $y=1$ with **high probability** (analogously, when $\\mathbf{x}$ is very far from the decision boundary and $\\mathbf{w}^\\top \\mathbf{x}<0$, we want to say $y=0$ with high probability). When $\\mathbf{x}$ is very close from the decision boundary, i.e. when $|\\mathbf{w}^\\top \\mathbf{x}|\\approx 0$, we want to be maximally uncertain about how predict $y$, i.e. we predict $y$ randomly by a 50-50 chance.\n",
    "\n",
    "Formalizing this: to model the **probability of labeling a point a certain class**, we have to convert distance, $\\mathbf{w}^\\top\\mathbf{x}$ (which is unbounded) into a number (probability) between 0 and 1, using the ***sigmoid function***:\n",
    "$$\n",
    "\\text{Prob}(y = 1 | \\mathbf{x}) = \\text{sigm}(\\underbrace{\\mathbf{w}^\\top\\mathbf{x}}_{\\text{distance}})\n",
    "$$\n",
    "where $\\text{sigm}(z) = \\frac{1}{1 + e^{-z}}$. This way, when $\\mathbf{w}^\\top\\mathbf{x}$ is large and positive then $\\text{sigm}(\\underbrace{\\mathbf{w}^\\top\\mathbf{x}}_{\\text{distance}})$ is close to 1, when $\\mathbf{w}^\\top\\mathbf{x}$ is large and negative then $\\text{sigm}(\\underbrace{\\mathbf{w}^\\top\\mathbf{x}}_{\\text{distance}})$ is close to 0; when $\\mathbf{w}^\\top\\mathbf{x}$ is near 0 then $\\text{sigm}(\\underbrace{\\mathbf{w}^\\top\\mathbf{x}}_{\\text{distance}})$ is close to 0.5, i.e. $\\text{Prob}(y = 1 | \\mathbf{x}) = 0.5$. This is exactly what we wanted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpreting a Logistic Regression Model\n",
    "\n",
    "Suppose that you fit a logistic regression model to predict whether a loan application should be approved. Suppose that you have three covariates: \n",
    "\n",
    "1. `x_1` representing gender: 0 for male, 1 for female\n",
    "2. `x_2` for the income\n",
    "3. `x_3` for the loan amount\n",
    "\n",
    "Suppose that the parameters you found are:\n",
    "$$\n",
    "p(y=1 | x_1, x_2, x_3) = \\mathrm{sigm}(-1 + 3 x_1 + 1.5 x_2 + 1.75 x_3).\n",
    "$$\n",
    "\n",
    "What are the parameters telling us about the most influential attribute for predicting loan approval? What does this say about our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## With Great Explanatory Power Comes Great Responsibility!\n",
    "\n",
    "So in the previous predictve model for loan application model we'd use the follwing covariates:\n",
    "\n",
    "1. `x_1` representing gender: 0 for male, 1 for female\n",
    "2. `x_2` for the income\n",
    "3. `x_3` for the loan amount\n",
    "\n",
    "We were pleased that using covariates we can hypothesize **why** the outcome $y$ is a certain way - why is the loan approved or denied. But just because we can collect the data for the covariates does it mean that we should use them? It particular, in our logistic model on the previous slide, the most impactful factor on a loan decision was gender, does this seem reasonable? Should we use this model in real-life loan decisions?\n",
    "\n",
    "In models that are built on human data, covariates containing information that infringes on the rights of the subject to remain anonymous or to keep potentially non-relevant and biasing information out of the decision making process are called **sensitive** or **protected attributes**. \n",
    "\n",
    "### When not to use sensitive/protected attributes\n",
    "\n",
    "Gender, in the case of loan decisions, is a protected attribute, since under the [Equal Credit Opportunity Act](https://www.consumerfinance.gov/fair-lending/) makes it illegal for a creditor to discriminate in any aspect of credit transaction based on certain characteristics including gender. Thus, any decision process using gender to inform a loan decision can be potentially considered discriminatory. \n",
    "\n",
    "In fact, most credit models used in industry are trained on data that are stripped of protected attributes - meaning that these models never see covariates like gender during training or deployment!\n",
    "\n",
    "If we had deployed the logistic model to make real life loan decisions, not only would our decisions be potentially unfair and possibly questionable in terms of financial soundness (since our decisions are heavily influenced by a covariate not directly related to the financial qualifications of a loan applicant), using such a model exposes our company to regulatory sanctions and law-suits. \n",
    "\n",
    "\n",
    "### When you might want to use sensitive/protected attributes\n",
    " \n",
    "So should we never collect data on protected attributes? Well, unfortunately, just because we are blind to protected attributes it does not mean that our decisions are fair with respect to these attributes! That is, just because we don't see gender when making decisions, it does not mean that our decisions impact men and women equally. In fact, the Equal Credit Opportunity Act contains explicit language that protects consumers from the [disparate impact](https://www.americanbanker.com/opinion/dont-ditch-disparate-impact) of credit decision systems. Disparate impact is defined as the unequal impact of a credit policy on a legally protected class:\n",
    "\n",
    "> \"A lender's policies, even when applied equally to all its credit applicants, may have a negative effect on certain applicants. For example, a lender may have a policy of not making single family home loans for less than \\$60,000. This policy might exclude a high number of applicants who have lower income levels or lower home values than the rest of the applicant pool. That uneven effect of the policy is called disparate impact.\"\n",
    "\n",
    "How do modelers and engineers prevent their models from creating disparate impact for protected classes of people? One of the key ways to check for disparate impact is to compare model decisions on protected classes against model decisions against the population (for example, we can compare the percentages of loans the model approves for men and for women). But in order perform these checks, we need access to protected attributes! \n",
    "\n",
    "Often times, the models we are auditing for disparate impact are \"black-boxes\", that is, the inner workings of the model (as well as details like the training procedure) are all proprietary information and we only have access to the models inputs and outputs, $(x_2, x_3, y)$ (note that the black-box model is not using the protected attribute gender $x_1$). In these cases, we can train a proxy model on covariates including protected attributes as well as the predictions of the black-box model, and then analyze our proxy model as an approximation to the black-box model. If we suppose that our logistic regression model in the previous slide was a proxy model that is trained to approximate a black-box model, then our proxy model is telling us that the black-box model's decisions are highly correlated with gender and hence it's decisions may cause disparate impact! This is a case where we needed to look at protected attributes in order to check for regulartory compliance! \n",
    "\n",
    "So why is this happening - why is a black-box model that is trained on data stripped of gender information making decisions that are highly correlated with gender? There are many possible explanations for this, but one common reason for this is that gender is a **confounding variable**, that is, some combination of income and loan amount is secretly encoding for gender (e.g. let's say that in your data set, all the men earn 50k-60k and apply for exactly \\$10,000 of loans) and the black-box model is ***implicitly*** relying on gender to make loan decisions.\n",
    "\n",
    "\n",
    "\n",
    "### Appropriate usage of sensitive/protected attributes\n",
    "\n",
    "So now we see that protected attributes can be extremely useful when we are auditing models for regulatory compliance, should we collect sensitive or protected attributes all the time and as many of them as possible? Unfortunately, it's not so simple: some industries are required by law to collect sensitive attribute data, while others are prohibited from doing so, still others infer sensitive attributes from collected data for compliance checking purposes (e.g. using income and other covariates to infer gender or zip codes to infer race). While useful for antidiscrimnatory purposes, collecting or infering sensitive protected attribute is nonetheless full of challenges and potential pitfalls (for example, protected attributes you infered can violate the subject's right to non-disclosure!) - references in this section addresses the issues of appropriate usage of sensitive attributes. \n",
    "\n",
    "Furthermore, **how** you collect and ecnode ata can deeply impact your fairness/compliance analysis. For example, application forms that include two options for gender and three boxes for race may cause us to incorrectly aggregate subjects who do not fall neatly into these boxes under categories that are inappropriate (and by the way, we've not been carefully distinguishing gender and sex in this discussion, but they maybe treated differently under different bodies of laws. Also, are they equivalent concepts to our human subjects from whom we are collecting the data? What are we really trying to measure by collecting gender or sex data?). Such misleading categorization can cause us to miss cases of disparate impact in our analysis, since we have simply missed out on important subgroups of the population. \n",
    "\n",
    "If you feel like we're now treading in the unfamiliar territories of social science or law, you are not mistaken! When we work with human data, the interpretation of the data and hence our analysis hinges on our ability to meaningfully understand the data in social contexts and in human terms. Just as we need to consult physicist domain experts when we are building a statistical model for physical data, medical experts when we are building models for medical data, when we build models that have social impact we have to proceed carefully under the advisement of experts on our social/legal systems.\n",
    "\n",
    "\n",
    "**References for Issues Involving Proper Use of Protected Attributes**\n",
    "0. <a href=\"https://journals.sagepub.com/doi/10.1177/0891243215584758\">New Categories Are Not Enough: Rethinking the Measurement of Sex and Gender in Social Surveys</a>\n",
    "1. <a href=\"https://www.pewsocialtrends.org/2015/11/06/chapter-1-estimates-of-multiracial-adults-and-other-racial-and-ethnic-groups-across-various-question-formats/\"> Estimates of Multiracial Adults and Other Racial and Ethnic Groups Across Various Question Formats</a>\n",
    "2. <a href=\"https://arxiv.org/pdf/1806.03281.pdf\">Blind Justice: Fairness with Encrypted Sensitive Attributes</a>\n",
    "3. <a href=\"https://arxiv.org/pdf/1912.06171.pdf\">Awareness in Practice: Tensions in Access to Sensitive Attribute Data for Antidiscrimination</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Interpreting a Logistic Regression Model: Log-Odds\n",
    "\n",
    "A more formal way of interpreting the parameters of the logistic regression model is through the ***log-odds***. That is, we solve for $\\mathbf{w}^\\top\\mathbf{x}$ in terms of $\\text{Prob}(y = 1 | \\mathbf{x})$.\n",
    "\n",
    "\\begin{aligned}\n",
    "\\text{Prob}(y = 1 | \\mathbf{x}) &= \\text{sigm}(\\mathbf{w}^\\top\\mathbf{x})\\\\\n",
    "\\text{sigm}^{-1}(\\text{Prob}(y = 1 | \\mathbf{x})) &= \\mathbf{w}^\\top\\mathbf{x}\\\\\n",
    "\\log \\left( \\frac{\\text{Prob}(y = 1 | \\mathbf{x})}{1 - \\text{Prob}(y = 1 | \\mathbf{x})}\\right)&= \\mathbf{w}^\\top\\mathbf{x}\\\\\n",
    "\\log \\left( \\frac{\\text{Prob}(y = 1 | \\mathbf{x})}{\\text{Prob}(y = 0 | \\mathbf{x})}\\right)&= \\mathbf{w}^\\top\\mathbf{x}\n",
    "\\end{aligned}\n",
    "where we used the fact that $\\text{sigm}^{-1}(z) = \\frac{z}{1 - z}$.\n",
    "\n",
    "The term $\\log \\left( \\frac{\\text{Prob}(y = 1 | \\mathbf{x})}{\\text{Prob}(y = 0 | \\mathbf{x})}\\right)$ is essentially a ratio of the probability of $y=1$ and the probability of $y=0$, we can interpret this quantity as the (log) odds of you winning if you'd bet that $y=1$. Thus, we can imagine the parameter $w_d$ for the covariate $x_d$ as telling us if the odd of winning a bet on $y=1$ is good.\n",
    "\n",
    "1. if $w_d < 0$, then by increasing $x_d$ (while holding all other covariates constant) we make $\\mathbf{w}^\\top\\mathbf{x}$ more negative, and hence the ratio $\\frac{\\text{Prob}(y = 1 | \\mathbf{x})}{\\text{Prob}(y = 0 | \\mathbf{x})}$ closer to 0. That is, when $w_d < 0$, increasing $x_d$ decreases our odds.<br><br>\n",
    "\n",
    "2. if $w_d > 0$, then by increasing $x_d$ (while holding all other covariates constant) we make $\\mathbf{w}^\\top\\mathbf{x}$ more positive, and hence the ratio $\\frac{\\text{Prob}(y = 1 | \\mathbf{x})}{\\text{Prob}(y = 0 | \\mathbf{x})}$ larger. That is, when $w_d > 0$, increasing $x_d$ decreases our odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximizing the Logistic Regression Log-likelihood\n",
    "\n",
    "Given a set of $N$ observations $(\\mathbf{x}^{(1)}, y^{(1)}), \\ldots, (\\mathbf{x}^{(N)}, y^{(N)})$. We want to find $\\mathbf{w}_{\\text{MLE}}$ that maximizes the log (joint) likelihood:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\small\n",
    "\\mathbf{w}_{\\text{MLE}} =& \\underset{\\mathbf{w}}{\\mathrm{argmax}}\\;\\ell(\\mathbf{w}) \\equiv \\underset{\\mathbf{w}}{\\mathrm{argmin}}\\;-\\ell(\\mathbf{w}) =\\underset{\\mathbf{w}}{\\mathrm{argmin}}-\\log \\prod_{n=1}^N p(y^{(n)} | \\mathbf{x}^{(n)}) \\\\\n",
    "=& \\underset{\\mathbf{w}}{\\mathrm{argmin}}\\sum_{n=1}^N -\\log\\left( \\mathrm{sigm}(\\mathbf{w}^\\top \\mathbf{x}^{(n)})^{y^{(n)}}(1 -\\mathrm{sigm}(\\mathbf{w}^\\top \\mathbf{x}^{(n)}))^{1 - y^{(n)}}   \\right) \\\\\n",
    "=& \\underset{\\mathbf{w}}{\\mathrm{argmin}}\\sum_{n=1}^N -y^{(n)}\\,\\log\\,\\mathrm{sigm}(\\mathbf{w}^\\top \\mathbf{x}^{(n)}) \\\\\n",
    "&- (1 - y^{(n)}) \\log (1 -\\mathrm{sigm}(\\mathbf{w}^\\top \\mathbf{x}^{(n)}))\n",
    "\\end{aligned}\n",
    "\n",
    "Optimizing the likelihood requires us to find the stationary points of the gradient of $\\ell(\\mathbf{w})$:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\ell(\\mathbf{w}) = -\\sum_{n=1}^N \\left(y^{(n)} - \\frac{1}{1 + e^{-\\mathbf{w}^\\top\\mathbf{x}^{(n)}}} \\right) \\mathbf{x}^{(n)} =\\mathbf{0}\n",
    "$$\n",
    "\n",
    "Can we solve for $\\mathbf{w}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient as Directional Information\n",
    "The gradient is orthogonal to the level curve of $f$ at $x^*$ and hence, *when it is not zero*, points in the direction of the greatest instantaneous increase in $f$.\n",
    "<img src=\"fig/levelcurves.jpg\" style=\"height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An Intuition for Gradient Descent\n",
    "The intuition behind various flavours of gradient descent  is as follows:\n",
    "\n",
    "<img src=\"./fig/fig9.pdf\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent: the Algorithm\n",
    "1. start at random place: $\\mathbf{w}^{(0)}\\leftarrow \\textbf{random}$\n",
    "\n",
    "2. until (stopping condition satisfied):\n",
    "\n",
    "  a. compute gradient at $\\mathbf{w}^{(t)}$: \n",
    "     gradient ($\\mathbf{w}^{(t)}$) = $\\nabla_{\\mathbf{w}}$ loss\\_function ($\\mathbf{w}^{(t)}$)\n",
    "\n",
    "  b. take a step in the negative gradient direction: \n",
    "     $\\mathbf{w}^{(t+1)} \\leftarrow \\mathbf{w}^{(t)}$ - $\\eta$ * gradient ($\\mathbf{w}^{(t)}$)\n",
    "\n",
    "Here $\\eta$ is called the ***learning rate***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnosing Design Choices with the Trajectory\n",
    "If this is your objective function during training, what can you conclude about your step-size?\n",
    "<img src=\"./fig/fig13.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnosing Issues with the Trajectory\n",
    "If this is your objective function during training, what can you conclude about your step-size?\n",
    "<img src=\"./fig/fig14.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnosing Issues with the Trajectory\n",
    "If this is your objective function during training, what can you conclude about your step-size?\n",
    "<img src=\"./fig/fig15.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent: Step Size Matters\n",
    "<img src=\"./fig/fig10.jpg\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent for Logistic Regression\n",
    "\n",
    "When diagnosing our gradient descent learning, we can:\n",
    "\n",
    "1. Visualize the log-likelihood. **What does this tell us?**\n",
    "2. Visualize the norm of the gradients. **What does this tell us?**\n",
    "\n",
    "What else should we visualize to check that our learned model aligns with the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# generate 2d classification dataset\n",
    "centers = [[1, 1], [-1, -1]]\n",
    "X, y = make_blobs(n_samples=400, n_features=2, centers=centers)\n",
    "X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "\n",
    "#define the sigmoid function\n",
    "sigmoid = lambda z: 1. / (1. + np.exp(-z))\n",
    "\n",
    "#define the negative log-likelihood\n",
    "nll = lambda X, y, w:  -(y * np.log(sigmoid(X.dot(w))) + (1 - y) * np.log(1 - sigmoid(X.dot(w)))).sum()\n",
    "\n",
    "#define logistic regression MLE inference by gradient descent\n",
    "def logistic_regression_fit(X, y, init=None, threshold=1e-6, total_iterations=1000, eta=1e-4):\n",
    "    #implement the gradient\n",
    "    gradient = lambda w: -((y - sigmoid(X.dot(w)))[:, np.newaxis] * X).sum(axis=0)\n",
    "    \n",
    "    #initialize parameters w\n",
    "    if init is None:\n",
    "        init = np.random.normal(0, 1, size=X.shape[1])\n",
    "    w_current = init\n",
    "    #store the negative log-likelihood\n",
    "    nlls = []\n",
    "    #store the norm of the gradients \n",
    "    grad_norms = []\n",
    "    #initialize the difference between current and new parameters\n",
    "    delta = 1\n",
    "    i = 0\n",
    "    #gradient descent \n",
    "    while i < total_iterations and delta > threshold:\n",
    "        #compute the gradient\n",
    "        grad = gradient(w_current)\n",
    "        #update parameter by taking gradient step\n",
    "        w_next = w_current - eta * grad\n",
    "        #compute the difference between current parameters and new parameters\n",
    "        delta = np.linalg.norm(w_next - w_current)\n",
    "        #compute the norm of the gradient\n",
    "        grad_norms.append(np.linalg.norm(grad))\n",
    "        #compute the negative log-likelihood\n",
    "        nlls.append(nll(X, y, w_current))\n",
    "        \n",
    "        w_current = w_next\n",
    "        i += 1\n",
    "        \n",
    "    return w_current, nlls, grad_norms\n",
    "\n",
    "def plot_diagnostics(ax, nlls, grad_norms):\n",
    "    ax[0].plot(range(len(nlls)), nlls)\n",
    "    ax[0].set_title('negative log-likelihood over iterations of gradient descent')\n",
    "    ax[0].set_ylabel('negative log-likelihood')\n",
    "    ax[0].set_xlabel('iterations of gradient descent')\n",
    "\n",
    "    ax[1].plot(range(len(grad_norms)), grad_norms, color='red')\n",
    "    ax[1].set_title('norms of gradients over iterations of gradient descent')\n",
    "    ax[1].set_ylabel('norm of gradient')\n",
    "    ax[1].set_xlabel('iterations of gradient descent')\n",
    "    return ax\n",
    "\n",
    "w_MLE, nlls, grad_norms = logistic_regression_fit(X, y, init=None, threshold=1e-6, total_iterations=2000, eta=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "ax = plot_diagnostics(ax, nlls, grad_norms)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But Did We Optimize It?\n",
    "\n",
    "<img src=\"fig/optima.jpg\" style=\"height:450px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convex Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex Sets\n",
    "\n",
    "A ***convex set*** $S\\subset \\mathbb{R}^D$ is a set that contains the line segment between any two points in $S$. Formally, if $x, y \\in S$ then $S$ contains all convex combinations of $x$ and $y$:\n",
    "\n",
    "$$\n",
    "tx + (1-t) y \\in S,\\quad t\\in [0, 1].\n",
    "$$\n",
    "\n",
    "<img src=\"./fig/convexset.jpg\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex Functions\n",
    "A function $f$ is a ***convex function*** if domain of $f$ is a convex set, and the line segment between the points $(x, f(x))$ and $(y, f(y))$ lie above the graph of $f$. Formally, for any $x, y\\in \\mathrm{dom}(f)$, we have \n",
    "\n",
    "$$\n",
    "\\underbrace{f(tx + (1-t)y)}_{\\text{height of graph of $f$ at a point between $x$ and $y$}} \\quad \\leq \\underbrace{tf(x) + (1-t)f(y)}_{\\text{height of point on line segment between $(x, f(x))$ and $(y, f(y))$}},\\quad t\\in [0, 1]\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"./fig/convex.jpg\" style='height:300px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex Function: First Order Condition\n",
    "\n",
    "How do we check that a function $f$ is convex? If $f$ is differentiable then $f$ is convex if the graph of $f$ lies above every tangent plane.\n",
    "\n",
    "**Theorem:** If $f$ is differentiable then $f$ is convex if and only if for every $x \\in \\mathrm{dom}(f)$, we have\n",
    "\n",
    "$$\n",
    "\\underbrace{f(y)}_{\\text{height of graph of $f$ over $y$}} \\geq \\underbrace{f(x) + \\nabla f(x)^\\top (y - x)}_{\\text{height of plane tangent to $f$ at $x$, evaluated over $y$}},\\quad \\forall y\\in \\mathrm{dom}(f)\n",
    "$$\n",
    "\n",
    "<img src=\"./fig/convex_first_order.jpg\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex Function: Second Order Condition\n",
    "\n",
    "If $f$ is twice-differentiable then $f$ is convex if the \"second derivative is positive\".\n",
    "\n",
    "**Theorem:** If $f$ is twice-differentiable then $f$ is convex if and only if the Hessian $\\nabla^2 f(x)$ is positive semi-definite for every $x\\in \\mathrm{dom}(f)$. \n",
    "\n",
    "<img src=\"./fig/convex_nonconvex.jpg\" style='height:300px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of Convex Functions\n",
    "\n",
    "How to build complex convex functions from simple convex functions:\n",
    "\n",
    "1. if $w_1, w_2 \\geq 0$ and $f_1, f_2$ are convex, then $h = w_1 f_1 + w_2 f_2$ is convex<br><br>\n",
    "\n",
    "2. if $f$ and $g$ are convex, and $g$ is univariate and non-decreasing then $h = g \\circ f$ is convex<br><br>\n",
    "\n",
    "3. Log-sum-exp functions are convex: $f(x) = \\log \\sum_{k=1}^K e^{x}$\n",
    "\n",
    "**Note:** there are many other convexity preserving operations on functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convex Optimization\n",
    "\n",
    "A ***convex optimization problem*** is an optimization of the following form:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathrm{min}\\; &f(x) & (\\text{convex objective function})\\\\\n",
    "\\text{subject to}\\; & h_i(x) \\leq 0, i=1, \\ldots, i & (\\text{convex inequality constraints}) \\\\\n",
    "& a_j^\\top x - b_j = 0, j=1, \\ldots, J & (\\text{affine equality constraints}) \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "The set of points that satisfy the constraints is called the ***feasible set***.\n",
    "\n",
    "You can prove that the a convex optimization problem optimizes a convex objective function over a convex feasible set. But why should we care about convex optimization problems?\n",
    "\n",
    "**Theorem:** Let $f$ be a convex function defined over a convex feasible set $\\Omega$. Then if $f$ has a local minimum at $x\\in \\Omega$ -- $f(y) \\geq f(x)$ for $y$ in a small neighbourhood of $x$ -- then $f$ has a global minimum at $x$.\n",
    "\n",
    "**Corollary:** Let $f$ be a differentiable convex function:\n",
    "1. if $f$ is unconstrained, then $f$ has a **local minimum** and hence **global minimum** at $x$ if $\\nabla f(x) = 0$.\n",
    "2. if $f$ is constrained by equalities, then $f$ has a global minimum at $x$ if $\\nabla J(x, \\lambda) = 0$, where $J(x, \\lambda)$ is the Lagrangian of the constrained optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Note:** we can also characterize the global minimum of inequalities constrained convex optimization problems using the Lagrangian, but the formulation is more complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convexity of the Logistic Regression Negative Log-Likelihood\n",
    "\n",
    "But why do we care about convex optimization problems? Let's connect the theory of convex optimization to MLE inference for logistic regression. Recall that the negative log-likelihood of the logistic regression model is\n",
    "\n",
    "\\begin{aligned}\n",
    "-\\ell(\\mathbf{w}) &= -\\sum_{n=1}^N y^{(n)}\\,\\log\\,\\mathrm{sigm}(\\mathbf{w}^\\top \\mathbf{x}^{(n)}) + (1 - y^{(n)}) \\log (1 -\\mathrm{sigm}(\\mathbf{w}^\\top \\mathbf{x}^{(n)}))\\\\\n",
    "&=\\sum_{n=1}^N y^{(n)} \\log(e^0 + e^{\\mathbf{w}^\\top \\mathbf{x}^{(n)}}) + (1 - y^{(n)})(- \\mathbf{w}^\\top \\mathbf{x}^{(n)})\n",
    "\\end{aligned}\n",
    "\n",
    "**Proposition:** The negative log-likelihood of logistic regression $-\\ell(\\mathbf{w})$ is convex.\n",
    "\n",
    "**What does this mean for gradient descent?** If gradient descent finds that $\\mathbf{w}^*$ is a stationary point of $-\\nabla_{\\mathbf{w}}\\ell(\\mathbf{w})$ then $-\\ell(\\mathbf{w})$ has a global minimum at $\\mathbf{w}^*$. Hence, $\\ell(\\mathbf{w})$ is maximized at $\\mathbf{w}^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***Proof of the Proposition:*** Note that\n",
    "1. $- \\mathbf{w}^\\top \\mathbf{x}^{(n)}$ and $(1 - y^{(n)})(- \\mathbf{w}^\\top \\mathbf{x}^{(n)})$ are convex, since they are linear\n",
    "2. $\\log(e^0 + e^{\\mathbf{w}^\\top \\mathbf{x}^{(n)}})$ is convex since it is the composition of a log-sum-exp function (which is convex) and a convex function $\\mathbf{w}^\\top \\mathbf{x}^{(n)}$\n",
    "3. $\\sum_{n=1}^N y^{(n)} \\log(e^0 + e^{\\mathbf{w}^\\top \\mathbf{x}^{(n)}})$ is convex since it is a nonnegative linear combination of convex functions\n",
    "4. $-\\ell(\\mathbf{w})$ is convex since it is the sum of two convex functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## But Does It Always Converge?\n",
    "We've seen that if we choose the learning rate to be too large (say for example `1e10` then gradient descent can fail to converge even if the function $f$ is convex. But how large is \"too large\". There are two cases to consider\n",
    "\n",
    "1. You have some prior knowledge about how smooth the function $f$ is -- i.e. how quickly $f$ can increase or decrease. Then using this you can choose a learning rate that will provably guaratee convergence<br><br>\n",
    "2. In most cases, the objective function (like the log-likelihood) may be too complex to reason about. In which case, \n",
    "  1. we do a scientific \"guess-and-check\" to determine the learning rate:\n",
    "    1. we find a learning rate that is large enough to cause gradient descent to diverge\n",
    "    2. we find a leanring rate that is small enough to cause gradient descent to converge too slowly\n",
    "    3. we choose a range of values between the large rate and the small rate and try them all to determine the optimal rate<br><br>\n",
    "  2. alternatively, we can choose the step-size $\\eta$ adaptively (e.g. when the gradient is large we can set $\\eta$ to be moderate to small and when the gradient is small we can set $\\eta$ to be larger). There are a number of adaptive step-size regimes that you may want to look up and implement for your specific problem.\n",
    "  \n",
    "The prior knowledge required to choose $\\eta$ for provable convergence is called Lipschitz continuity. If we knew that $f$ is convex, differentiable and that there is a constant $L>0$ such that $\\|\\nabla f(x) - \\nabla f(y)\\|_2 \\leq L\\|x -y\\|_2$, then if we choose a fixed step size to be $\\eta \\leq \\frac{1}{L}$ then gradient descent **provably** converges to the global minimum of $f$ as the number of iterations $N$ goes to infinity. The constant $L$ is called the ***Lipschitz constant***.\n",
    "\n",
    "## But How Quickly Can We Get There?\n",
    "\n",
    "Just because we know gradient descent will converge it doesn't mean that it will give us a good enough approximation of the global minimum within our time limit. This is why studying the ***rate of convergence*** of gradient descent is extremely important. Again there are two cases to consider\n",
    "\n",
    "1. You have prior knowledge that $f$ is convex, differentiable and its Lipschitz constant is $L$ and suppose that $f$ has a global minimum at $x^*$, then for gradient descent to get within $\\epsilon$ of $f(x^*)$, we need $O(1/\\epsilon)$ number of iterations.<br><br>\n",
    "\n",
    "2. In most cases, the objective function will fail to be convex and its Lipschitz constant may be too difficult to compute. In this case, we simply stop the gradient descent when the gradient is sufficiently small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But Does It Scale?\n",
    "\n",
    "Gradient is such a simple algorithm that can be applied to **any optimization problem** for which you can compute the gradient of the objective function. \n",
    "\n",
    "**Question:** Does this mean that maximum likelihood inference for statistical models is now an easy task (i.e. just use gradient descent)?\n",
    "\n",
    "For every likelihood optimization problem, evaluating the gradient at a set of parameters $\\mathbf{w}$ requires evaluating the likelihood of the entire dataset using $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}} \\ell(\\mathbf{w}) = -\\sum_{n=1}^N \\left(y^{(n)} - \\frac{1}{1 + e^{-\\mathbf{w}^\\top\\mathbf{x}^{(n)}}} \\right) \\mathbf{x}^{(n)} =\\mathbf{0}\n",
    "$$\n",
    "\n",
    "Imagine if the size of your dataset $N$ is in the millions. Naively evaluating the gardient **just once** may take up to seconds or minutes, thus running gradient descent until convergence may be unachievable in practice!\n",
    "\n",
    "**Idea:** Maybe we don't need to use the entire data set to evaluate the gradient during each step of gradient descent. Maybe we can approximate the gradient at $\\mathbf{w}$ well enough with just a subset of the data."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
