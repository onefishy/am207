# Week 5

### Lecture
- [Lecture #8 Notes](./Lectures/lecture_8_notes.ipynb)
- [Lecture #8 Summary](./Lectures/lecture_8_summary.ipynb)
- [Lecture #9 Notes](./Lectures/lecture_9_notes.ipynb)

### Activities
- [In-Class Exercise #8: Metropolis-Hastings and Gibbs Sampling](https://deepnote.com/project/AM207Fall202108-MH-and-Gibbs-_-uidW8qQXy-mQ6w0LJ7-w)
- [In-Class Exercise #9: Expectation Maximization](https://deepnote.com/project/AM207Fall202109-expectation-maximization-l6HX_UP8RXCBD7K5DvSAvg)
- [Homework #4](./HW/AM207_HW4.ipynb)

### Reading
<p><strong>Applications and Broader Impact:</strong></p>
<ol>
    <li><a class="inline_disabled" href="https://medium.com/@lucabenedetto/advantages-in-using-item-response-theory-for-assessing-students-and-more-4a9665258863" target="_blank" rel="noopener">Item Response Theory for assessing students and questions (pt. 1)</a></li>
    <li><a class="inline_disabled" href="https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1004969" target="_blank" rel="noopener">Simultaneous Discovery, Estimation and Prediction Analysis of Complex Traits Using a Bayesian Mixture Model</a></li>
    <li><a class="inline_disabled" href="https://content.iospress.com/articles/journal-of-parkinsons-disease/jpd140523" target="_blank" rel="noopener">Parkinson&rsquo;s Disease Subtypes in the Oxford Parkinson Disease Centre (OPDC) Discovery Cohort</a></li>
    <li><a class="inline_disabled" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6757227/" target="_blank" rel="noopener">One Size Doesn&rsquo;t Fit All: Using Factor Analysis to Gather Validity Evidence When Using Surveys in Your Research</a></li>
</ol>
<p><strong>MCMC Diagnostics:</strong></p>
<ol>
    <li>(Introductory) <a href="https://www.statlect.com/fundamentals-of-statistics/Markov-Chain-Monte-Carlo-diagnostics">Markov Chain Monte Carlo (MCMC) Diagnostics</a></li>
    <li>(In-Depth)&nbsp;<a href="https://arxiv.org/pdf/1909.11827.pdf"><span style="font-family: inherit; font-size: 1rem;">Convergence diagnostics for Markov chain Monte Carlo</span></a></li>
    <li><span style="font-family: inherit; font-size: 1rem;">(Opinion) <a href="http://users.stat.umn.edu/~geyer/mcmc/diag.html">On the Bogosity of MCMC Diagnostics</a></span></li>
</ol>
<p><strong><span style="font-family: inherit; font-size: 1rem;">More on Thinning:</span></strong></p>
<ol>
    <li><span style="font-family: inherit; font-size: 1rem;">(Thinning is bad!) <a href="https://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210X.2011.00131.x">On Thinning of Chains in MCMC</a></span></li>
    <li><span style="font-family: inherit; font-size: 1rem;">(Thinning can be good!) <a href="https://arxiv.org/pdf/1510.07727.pdf">Statistically efficient thinning of a Markov chain sampler</a></span></li>
</ol>
<p><strong><span style="font-family: inherit; font-size: 1rem;">More on Chain Length:</span></strong></p>
<ol>
    <li><span style="font-family: inherit; font-size: 1rem;"><span style="font-family: inherit; font-size: 1rem;">(The longer the better? Not always) </span></span><a href="https://arxiv.org/pdf/1708.03625.pdf">Unbiased Markov chain Monte Carlo with couplings</a></li>
</ol>
<p><strong>Latent Variable Models:</strong></p>
<div class="page" title="Page 1">
    <div class="section">
        <div class="layoutArea">
            <div class="column">
                <ol>
                    <li><span style="font-family: sans-serif; font-size: 1rem;">(Introductory) </span><a href="http://www.cs.columbia.edu/~blei/papers/Blei2014b.pdf"><span style="font-family: sans-serif; font-size: 1rem;">Build, Compute, Critique, Repeat: Data Analysis with Latent Variable Models</span></a></li>
                    <li><span style="font-family: sans-serif; font-size: 1rem;">(Introductory) </span><a href="http://mayagupta.org/publications/EMbookGuptaChen2010.pdf"><span style="font-family: sans-serif; font-size: 1rem;">Theory and Use of the EM Algorithm</span></a></li>
                </ol>
                <p>&nbsp;</p>
            </div>
        </div>
    </div>
</div>
